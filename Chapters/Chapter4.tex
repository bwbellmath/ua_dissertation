\chapter{Kernel Neural Equialence} % Main chapter title
\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1} 

% %%%%%%%% ICML 2023 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

% \documentclass{article}

% % Recommended, but optional, packages for figures and better typesetting:
% \usepackage{microtype}
% \usepackage{graphicx}
% %\usepackage{subfigure}
% \usepackage{booktabs} % for professional tables

% % hyperref makes hyperlinks inÆ’ the resulting PDF.
% % If your build breaks (sometimes temporarily if a hyperlink spans a page)
% % please comment out the following usepackage line and replace
% % \usepackage{icml2023} with \usepackage[nohyperref]{icml2023} above.
% \usepackage{hyperref}


 % Attempt to make hyperref and algorithmic work together better:
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% % Use the following line for the initial blind version submitted for review:
% %\usepackage{icml2023}

% % If accepted, instead use the following line for the camera-ready submission:
% \usepackage[accepted]{icml2023}

% % For theorems and such
% \usepackage{amsmath}
% \usepackage{amssymb}
% \usepackage{mathtools}
% \usepackage{amsthm}
% \usepackage{graphicx}

% \usepackage[utf8]{inputenc} % allow utf-8 input
% \usepackage[T1]{fontenc}    % use 8-bit T1 fonts
% \usepackage{hyperref}       % hyperlinks
% \usepackage{calc}
% \usepackage{url}            % simple URL typesetting
% \usepackage{booktabs}       % professional-quality tables
% \usepackage{amsfonts}       % blackboard math symbols
% \usepackage{nicefrac}       % compact symbols for 1/2, etc.
% \usepackage{microtype}      % microtypography
% \usepackage{xcolor}         % colors
% % subcaption does not play nice with subfigure, we'll see if we support the other style
% \usepackage{subcaption}
% \newcommand{\RR}{I\!\!R} %real numbers
% \newcommand{\Nat}{I\!\!N} %natural numbers
% \newcommand{\CC}{\mathcal{C}} %complex numbers
% \usepackage{textcase}
% \usepackage{thmtools,thm-restate}


% \newlength{\depthofsumsign}
% \setlength{\depthofsumsign}{\depthof{$\sum$}}
% \newlength{\totalheightofsumsign}
% \newlength{\heightanddepthofargument}

% \newcommand{\nsum}[1][1.4]{% only for \displaystyle
%     \mathop{%
%         \raisebox
%             {-#1\depthofsumsign+1\depthofsumsign}
%             {\scalebox
%                 {#1}
%                 {$\displaystyle\sum$}%
%             }
%     }
% }
% \newcommand{\resum}[1]{%
%     \def\s{#1}
%     \mathop{
%         \mathpalette\resumaux{#1}
%     }
% }

% \newcommand{\resumaux}[2]{% internally
%     \sbox0{$#1#2$}
%     \sbox1{$#1\sum$}
%     \setlength{\heightanddepthofargument}{\wd0+\dp0}
%     \setlength{\totalheightofsumsign}{\wd1+\dp1}
%     \def\quot{\DivideLengths{\heightanddepthofargument}{\totalheightofsumsign}}
%     \nsum[\quot]%
% }

% % http://tex.stackexchange.com/a/6424/16595
% \makeatletter
% \newcommand*{\DivideLengths}[2]{%
%   \strip@pt\dimexpr\number\numexpr\number\dimexpr#1\relax*65536/\number\dimexpr#2\relax\relax sp\relax
% }
% \makeatother


% \usepackage{bbm}
% % \usepackage{algpseudocode}
% \usepackage{booktabs}
% \usepackage{pgfplots}
% \usepackage{tikz-3dplot}
% \usepackage{physics}
% \usepackage[outline]{contour} % glow around text
% \usetikzlibrary{angles,quotes} % for pic
% \contourlength{1.2pt}

% \tikzset{>=latex} % for LaTeX arrow head
% \usepackage{xcolor}
% \usetikzlibrary{fadings}
% \usetikzlibrary{arrows.meta}
% \tikzset{arl/.style={line width=4pt, {-Latex[left]}, #1}}
% \tikzset{arr/.style={line width=4pt, {-Latex[right]}, #1}}

% %
% \colorlet{veccol}{green!70!black}
% \colorlet{vcol}{green!70!black}
% \colorlet{xcol}{blue!85!black}
% \colorlet{projcol}{xcol!60}
% \colorlet{unitcol}{xcol!60!black!85}
% \colorlet{myblue}{blue!70!black}
% \colorlet{myred}{red!90!black}
% \colorlet{ntk}{red!90!white}
% \colorlet{dpk}{blue!70!white}
% \colorlet{epkt}{green!40!black}
% \colorlet{diffpk}{orange}
% \colorlet{epk}{black!70!gray}
% \colorlet{mypurple}{blue!50!red!80!black!80}
\tikzstyle{vector}=[->,line width=0.65mm, xcol]
\usetikzlibrary{intersections, pgfplots.fillbetween}
%\usepackage{dsfont}

% \tdplotsetmaincoords{60}{115}
% \pgfplotsset{compat=newest}

% % if you use cleveref..
% \usepackage[capitalize,noabbrev]{cleveref}

% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % THEOREMS
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \theoremstyle{plain}
% \newtheorem{theorem}{Theorem}[section]
% \newtheorem{proposition}[theorem]{Proposition}
% \newtheorem{lemma}[theorem]{Lemma}
% \newtheorem{corollary}[theorem]{Corollary}
% \theoremstyle{definition}
% \newtheorem{definition}[theorem]{Definition}
% \newtheorem{assumption}[theorem]{Assumption}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}
% %\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}
% \newenvironment{sproof}{%
%   \renewcommand{\proofname}{Proof Sketch}\proof}{\endproof}
% \newcommand{\e}{\varepsilon}
% \newcommand{\relu}{\text{ReLU}}
% % \newcommand{\grad}{\text{grad}}
% % \newcommand{\norm}[1]{\left\vert #1 \right\vert}
% \newcommand{\Norm}[1]{\left\Vert #1 \right\Vert}

\newcounter{remcounter}
%\setcounter{remcounter}{-1}
\newcommand*{\remlabel}[1]{\refstepcounter{remcounter}\theremcounter\label{#1}}
\newcommand*{\remref}[1]{\ref{#1}}

% % Todonotes is useful during development; simply uncomment the next line
% %    and comment out the line below the next line to turn off comments
% %\usepackage[disable,textsize=tiny]{todonotes}
% \usepackage[textsize=tiny]{todonotes}


% % The \icmltitle you define below is probably too long as a header.
% % Therefore, a short form for the running title is supplied here:
% \icmltitlerunning{An Exact Kernel Equivalence for Finite Classification Models }

% \begin{document}

% \twocolumn[
% \icmltitle{An Exact Kernel Equivalence for Finite Classification Models
% % (ICML 2023)
% }



% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2023
% package.

% List of affiliations: The first argument should be a (short)
% identifier you will use later to specify author affiliations
% Academic affiliations should list Department, University, City, Region, Country
% Industry affiliations should list Company, City, Region, Country

% You can specify symbols, otherwise they are numbered in order.
% Ideally, you should not use this facility. Affiliations will be numbered
% in order of appearance and this is the preferred way.
% \icmlsetsymbol{equal}{*}

% \begin{icmlauthorlist}
% \icmlauthor{Brian Bell}{equal,yyy}
% \icmlauthor{Michael Geyer}{equal,yyy,comp}
% \icmlauthor{Juston Moore}{yyy}
% \icmlauthor{David Glickenstein}{comp}
% \icmlauthor{Amanda Fernandez}{sch}
% % \icmlauthor{Firstname6 Lastname6}{sch,yyy,comp}
% % \icmlauthor{Firstname7 Lastname7}{comp}
% % %\icmlauthor{}{sch}
% % \icmlauthor{Firstname8 Lastname8}{sch}
% % \icmlauthor{Firstname8 Lastname8}{yyy,comp}
% %\icmlauthor{}{sch}
% %\icmlauthor{}{sch}
% \end{icmlauthorlist}

% \icmlaffiliation{yyy}{Los Alamos National Lab}
% \icmlaffiliation{comp}{University of Arizona}
% \icmlaffiliation{sch}{University of Texas Austin}

% \icmlcorrespondingauthor{Brian Bell}{bwbell@math.arizona.edu}
% \icmlcorrespondingauthor{Michael Geyer}{mgeyer@lanl.gov}

% % You may provide any keywords that you
% % find helpful for describing your paper; these are used to populate
% % the "keywords" metadata in the PDF but will not be shown in the document
% \icmlkeywords{Machine Learning, Kernel Machines, Mathematics}

% \vskip 0.3in
% ]

% % this must go after the closing bracket ] following \twocolumn[ ...

% % This command actually creates the footnote in the first column
% % listing the affiliations and the copyright notice.
% % The command takes one argument, which is text to display at the start of the footnote.
% % The \icmlEqualContribution command is standard text for equal contribution.
% % Remove it (just {}) if you do not need this facility.

% %\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

\begin{abstract}
We explore the equivalence between neural networks and kernel methods by deriving the first exact representation of any finite-size parametric classification model trained with gradient descent as a kernel machine. We compare our exact representation to the well-known Neural Tangent Kernel (NTK) and discuss approximation error relative to the NTK and other non-exact path kernel formulations. We experimentally demonstrate that the kernel can be computed for realistic networks up to machine precision. We use this exact kernel to show that our theoretical contribution can provide useful insights into the predictions made by neural networks, particularly the way in which they generalize.
\end{abstract}

\section{Introduction}

This study investigates the relationship between kernel methods and finite parametric models. To date, interpreting the predictions of complex models, like neural networks, has proven to be challenging. Prior work has shown that the inference-time predictions of a neural network can be exactly written as a sum of independent predictions computed with respect to each training point. We formally show that classification models trained with cross-entropy loss can be exactly formulated as a kernel machine. It is our hope that these new theoretical results will open new research directions in the interpretation of neural network behavior.




% \begin{tikzpicture}[scale=0.8]
% % main step angles
% \def\angA{72}
% \def\angB{35}
% \def\angC{15}
% \def\angD{-20}

% \def\nang{62}
% \def\dang{45}
% \def\sang{15}
% \def\pang{30}

% % \def\mang{29}
% % \def\sang{39}
% % \def\tang{50}
% % \def\bang{60}
% % \def\dang{34}

%   \def\xs{1.0}
%   \def\xa{4}
%   \def\xe{1.0}
%   \def\xd{0.75}
%   \def\xn{0.5}
%   \def\xi{1.4}
%   % incoming edge
%   \coordinate  (s1) at (0,0);
%   \coordinate  (si1) at ($ (s1) + (\angA:1) $);
%   \coordinate  (s2) at ($ (si1) + (\angA:\xs) $);
%   \coordinate  (s3) at ($ (s2) + (\angB:\xa) $);
%   \coordinate  (si3) at ($(s3) + (\angC:\xs) $);
%   \coordinate  (s4) at ($(si3) + (\angC:\xs) $);

%   %\node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
%   \path (si1) -- (s2) node [midway, sloped] (elip) {\ldots};
%   \path (s3) -- (si3) node [midway, sloped] (elip) {\ldots};
%   \node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
%   \node[fill=black,circle,inner sep=1.9] (s02) at (s2) {};
%   \node[fill=black,circle,inner sep=1.9] (s03) at (s3) {};
%   \node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};
%   %\node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};

% \draw[name path=pb, black, line width=0.4mm, draw opacity=0.7] (s2) -- (s3);
% \draw[black, line width=0.4mm, draw opacity=0.7] (s1) -- (si1);
% \path[name path=fb, black, line width=0.4mm, draw opacity=0.7] (s1) -- (s2);
% \draw[name path=gb, black, line width=0.4mm, draw opacity=0.7] (si3) -- (s4);

% \coordinate (p1) at (s2);%($ (s2)+(\ang:\xi) $);   % origin
% % \coordinate (p1n) at ($ (p1)+(\nang:\xe) $);       % ntk test

% % \coordinate (p1d) at ($ (p1)+(\angB+\sang:\xe) $); % dpk test

% % \coordinate (p1e) at ($ (p1)+(\angB+\sang:\xe) $); % epk test
% % \coordinate (p1t) at ($ (p1)+(\angD:\xe) $); % ntk train
% % \coordinate (p1b) at ($ (p1)+(\angB:\xe) $); % dpk epk train

% % \coordinate (p2) at  ($ (p1b)+(\angB:\xi) $);   % origin
% % \coordinate (p2n) at ($ (p2)+(\nang:\xe) $);       % ntk test
% % \coordinate (p2d) at ($ (p2)+(\angB+\sang:\xe) $); % dpk test
% % \coordinate (p2e) at ($ (p2)+(\angB+\pang:\xe) $); % epk test
% % \coordinate (p2t) at ($ (p2)+(\angD:\xe) $); % ntk train
% % \coordinate (p2b) at ($ (p2)+(\angB:\xe) $); % dpk epk train

% % \coordinate (p3) at  (s4); %($ (s4)+(\angB:\xi) $);   % origin
% %   \draw[line width=10pt, ->, red] (0,0) -- (3,0);
% %   \draw[line width=10pt, ->, blue] (0,0.5) -- (3,0.5);
  
% %     \begin{scope}
% %     \clip (-1,0) rectangle (3,1);
% %     \draw[line width=10pt, ->] (0,0) -- (3,0);
% %   \end{scope}
% % \coordinate (p3n) at ($ (p3)+(\nang:\xe) $);       % ntk test
% % \coordinate (p3d) at ($ (p3)+(\nang:\xe) $); % dpk test
% % \coordinate (p3e) at ($ (p3)+(\nang:\xe) $); % epk test
% % \coordinate (p3t) at ($ (p3)+(\angD:\xe) $); % ntk train
% % \coordinate (p3b) at ($ (p3)+(\angD:\xe) $); % dpk epk train

% % \coordinate (p2) at ($ (p1a)+(\angB:0.6) $);
% % \coordinate (p2a) at ($ (p2)+(\angB:\xe) $);
% % \coordinate (p2b) at ($ (p2)+(\sang:\xe) $); 
% % \coordinate (p3) at ($ (p2a)+(\angB:\xi) $);
% % \coordinate (p3a) at ($ (p3)+(\angB:\xe) $);
% % \coordinate (p3b) at ($ (p3)+(\tang:\xe) $);
% % \coordinate (n0) at ($ (s1)+(\nang:\xn) $);
% % \coordinate (n1) at ($ (p1)+(\nang:\xn) $);
% % \coordinate (n2) at ($ (p2)+(\nang:\xn) $);
% % \coordinate (n3) at ($ (p3)+(\nang:\xn) $);
% % \coordinate (d0) at ($ (s1)+(\dang:\xd) $);
% % \coordinate (d1) at ($ (p1)+(\dang:\xd) $);
% % \coordinate (d2) at ($ (p2)+(\dang:\xd) $);
% % \coordinate (d3) at ($ (p3)+(\dang:\xd) $);
    
% % \draw[vector, ->, ntk] (s1) -- (n0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % \draw[vector, ->, dpk] (s1) -- (d0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % \draw[vector, opacity=0.5, ->, ntk] (p1) -- (p1n) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk test};

% % \draw[vector, opacity=0.5,  ->, dpk] (p1) -- (p1d) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%dpk test};

% % \begin{scope}
% % \clip (p1) -- (p1d) -- ($(p1d) + (0,1) $) -- ($ (p1) + (-1,1) $);
% % \draw[vector, ->, epk] (p1) -- (p1e) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%epk test};
% % \end{scope}
% % \draw[vector, ->, ntk] (p1) -- (p1t) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk train};
% % \draw[vector, ->, epk] (p1) -- (p1b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%(de)pk test};

% % \draw[vector, ->, ntk] (p2) -- (p2n) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk test};
% % \draw[vector, ->, dpk] (p2) -- (p2d) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%dpk test};
% % \draw[vector, ->, epk] (p2) -- (p2e) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%epk test};
% % \draw[vector, ->, ntk] (p2) -- (p2t) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk train};
% % \draw[vector, ->, epk] (p2) -- (p2b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%(de)pk test};



% % \draw[vector, ->, ntk] (p3) -- (p3n) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk test};
% % \draw[vector, ->, dpk] (p3) -- (p3d) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%dpk test};
% % \draw[vector, ->, epk] (p3) -- (p3e) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%epk test};
% % \draw[vector, ->, ntk] (p3) -- (p3t) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk train};
% % \draw[vector, ->, epk] (p3) -- (p3b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%(de)pk test};

% \draw [name path=pa, thick , ->]
%   (p1) .. controls ($ (s2) + (37:2.1) $)  .. ($ (s3) + (125:0.6) $);
%   \draw [name path=fa, thick , ->]
%   (s1) .. controls ($ (s1) + (80:1.1) $)  .. ($ (s2) + (162:0.50) $);
%   \draw [name path=ga, thick , ->]
%   (s3) .. controls ($ (s3) + (20:1) $)  .. ($ (s4) + (105:0.4) $);
% \tikzfillbetween[of=pa and pb]{orange, opacity=0.2};
% \tikzfillbetween[of=fa and fb]{orange, opacity=0.2}; 
% \tikzfillbetween[of=ga and gb]{orange, opacity=0.2};

% % \draw[vector, ->, epk] (p1) -- (p1b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % \draw[vector, ->, epk] (p1) -- (p1a) node[scale=1,below left=1mm and 0mm, draw opacity=0.5] {$\vu{y}$};
% % \draw[vector, ->, epk] (p2) -- (p2b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % \draw[vector, ->, epk] (p2) -- (p2a) node[scale=1,below left=1mm and 0mm, draw opacity=0.5] {$\vu{y}$};
% % \draw[vector, ->, epk] (p3) -- (p3b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % \draw[vector, ->, epk] (p3) -- (p3a) node[scale=1,below left=1mm and 0mm, draw opacity=0.5] {$\vu{y}$};

% \end{tikzpicture}
\begin{figure}[!ht]
\begin{tikzpicture}[scale=0.72]
\def\bang{72}
\def\ang{35}
\def\lang{15}
\def\mang{57}
\def\sang{69}
\def\kang{90}
\def\tang{87}
\def\dang{34}
\def\xs{1.0}
\def\xa{4}
\def\xe{1.0}
\def\xd{0.75}
\def\xn{0.5}
\def\xi{0.4}
% incoming edge
\coordinate  (s1) at (0,0);
\coordinate  (si1) at ($ (s1) + (\bang:1) $);
\coordinate  (s2) at ($ (si1) + (\bang:\xs) $);
\coordinate  (s3) at ($ (s2) + (\ang:\xa) $);
\coordinate  (si3) at ($(s3) + (\lang:\xs) $);
\coordinate  (s4) at ($(si3) + (\lang:\xs) $);

%\node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
\path (si1) -- (s2) node [midway, sloped] (elip) {\ldots};
\path (s3) -- (si3) node [midway, sloped] (elip) {\ldots};
\node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
\node [below right=0mm and 1mm, rotate=\kang-90] (ss1) at (s01) {$w_1(t=0)$};
\node[fill=black,circle,inner sep=1.9] (s02) at (s2) {};
\node [below right=0mm and 1mm, rotate=\kang-90] (ss2) at (s02) {$w_s(t=0)$};
\node[fill=black,circle,inner sep=1.9] (s03) at (s3) {};
\node [below right=-1mm and 1mm, rotate=\kang-90] (ss3) at (s3) {$w_s(t=1)=w_{s+1}(t=0)$}; 
\node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};
%\node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};
\node [below right=-2mm and 1mm, rotate=\kang-90] (ss4) at (s04) {$w_S(t=0)$};
\draw[black, line width=0.4mm, draw opacity=0.3] (s2) -- (s3);
\draw[black, line width=0.4mm, draw opacity=0.3] (s1) -- (si1);
\draw[black, line width=0.4mm, draw opacity=0.3] (si3) -- (s4);

\coordinate (p1) at (s2);%($ (s2)+(\ang:\xi) $);
\coordinate (p1a) at ($ (p1)+(\ang:\xe) $);
\coordinate (p1b) at ($ (p1)+(\mang:\xe) $);
\coordinate (p2) at ($ (p1a)+(\ang:0.6) $);
\coordinate (p2a) at ($ (p2)+(\ang:\xe) $);
\coordinate (p2b) at ($ (p2)+(\sang:\xe) $); 
\coordinate (p2c) at ($ (p2)+(\mang:\xe) $); 
\coordinate (p3) at ($ (p2a)+(\ang:\xi) $);
\coordinate (p3a) at ($ (p3)+(\ang:\xe) $);
\coordinate (p3b) at ($ (p3)+(\tang:\xe) $);
\coordinate (p3c) at ($ (p3)+(\mang:\xe) $);
\coordinate (n0) at ($ (s1)+(\bang:\xn) $);
\coordinate (n1) at ($ (p1)+(\bang:\xn) $);
\coordinate (n2) at ($ (p2)+(\bang:\xn) $);
\coordinate (n3) at ($ (p3)+(\bang:\xn) $);
\coordinate (d0) at ($ (s1)+(\dang:\xd) $);
\coordinate (d1) at ($ (p1)+(\dang:\xd) $);
\coordinate (d2) at ($ (p2)+(\dang:\xd) $);
\coordinate (d3) at ($ (p3)+(\dang:\xd) $);

\draw[name path=pb, black, line width=0.4mm, draw opacity=0.7] (s2) -- (s3);
\draw[black, line width=0.4mm, draw opacity=0.7] (s1) -- (si1);
\path[name path=fb, black, line width=0.4mm, draw opacity=0.7] (s1) -- (s2);
\draw[name path=gb, black, line width=0.4mm, draw opacity=0.7] (si3) -- (s4);  
  \draw [name path=pa, thick , ->, opacity=0.0]
  (p1) .. controls ($ (s2) + (37:2.1) $)  .. ($ (s3) + (125:0.6) $);
  \draw [name path=fa, thick , ->, opacity=0.0]
  (s1) .. controls ($ (s1) + (80:1.1) $)  .. ($ (s2) + (162:0.50) $);
  \draw [name path=ga, thick , ->, opacity=0.0]
  (s3) .. controls ($ (s3) + (20:1) $)  .. ($ (s4) + (105:0.4) $);
\tikzfillbetween[of=pa and pb]{orange, opacity=0.2};
\tikzfillbetween[of=fa and fb]{orange, opacity=0.2}; 
\tikzfillbetween[of=ga and gb]{orange, opacity=0.2};
    
% \draw[vector, ->, ntk] (s1) -- (n0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% \draw[vector, ->, dpk] (s1) -- (d0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
\draw[vector, ->, dpk] (p1) -- (p1b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0)}(x)$};
\begin{scope}
\clip (p1) -- (p1b) -- ($(p1b) + (90+\mang:0.1) $) -- ($ (p1) + (90+\mang:0.1) $);

\draw[vector, ->, epkt] (p1) -- (p1b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0)}(x)$};

\end{scope}
\draw[vector, ->, epk] (p1) -- (p1a) node[scale=1,below right=1mm and -2mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[vector, ->, epkt] (p2) -- (p2b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0.4)}(x)$};
\draw[vector, ->, dpk] (p2) -- (p2c) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {};
\draw[vector, ->, epk] (p2) -- (p2a) node[scale=1,below right=3mm and -4mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[vector, ->, epkt] (p3) -- (p3b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0.7)}(x)$};
\draw[vector, ->, dpk] (p3) -- (p3c) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {};
\draw[vector, ->, epk] (p3) -- (p3a) node[scale=1,below right=4mm and -6mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[->,line width=0.2mm, xcol, diffpk] (p2c) -- (p2b) node[scale=1,below right=1mm and -2mm, draw opacity=0.5] {};
\draw[->,line width=0.2mm, xcol, diffpk] (p3c) -- (p3b) node[scale=1,below right=1mm and -2mm, draw opacity=0.5] {};
  

  %\draw[vector,<->,unitcol]
  %  (v3) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v1) --
  %  (v2) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};
    
    %   \draw[vector,<->,unitcol]
    % (v6) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v4) --
    % (v5) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};
    %       \draw[vector,<->,unitcol]
    % (v9) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v7) --
    % (v8) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};

% active step
% % outgoing edge
%   \def\ul{0.52}
%   \def\R{2.6}
%   \def\ang{28}
%   \coordinate (O) at (0,0);
%   \coordinate (R) at (\ang:\R);
%   \coordinate (X) at ({\R*cos(\ang)},0);
%   \coordinate (Y) at (0,{\R*sin(\ang)});
%   \node[fill=black,circle,inner sep=1.9] (O') at (O) {};
%   \node[fill=black,circle,inner sep=1.9] (R') at (R) {};
%   \node[above right=-2] at (R') {$(x,y)$};
%   \draw[<->,line width=0.9] %very thick
%     ({1.2*\R*cos(\ang)},0) -- (O) -- (0,{1.3*\R*sin(\ang)});
%   \draw[projcol,dashed] (X) -- (R);
%   \draw[black, thick] (O') -- (R');
%   \draw[projcol,dashed] (Y) -- (R);
%   %\draw[vector] (O) -- (R') node[midway,left=5,above right=0] {$\vb{r}$};
%   \draw[vector,<->,unitcol]
%     (\ul,0) node[scale=1,left=2,below left=0] {$\vu{x}$} -- (O) --
%     (0,\ul) node[scale=1,below=2,below left=0] {$\vu{y}$};
%   \draw pic[->,thick,"$\theta$",draw=black,angle radius=26,angle eccentricity=1.3]
%     {angle = X--O--R};
%   \draw[thick] (X)++(0,0.1) --++ (0,-0.2) node[scale=0.9,below=-1] {$x = r\cos\theta$};
%   \draw[thick] (Y)++(0.1,0) --++ (-0.2,0) node[scale=0.9,left] {$y = r\sin\theta$};
\end{tikzpicture}
\caption{Comparison of test gradients used by Discrete Path Kernel (DPK) from prior work (Blue) and the Exact Path Kernel (EPK) proposed in this work (green) versus total training vectors (black) used for both kernel formulations along a discrete training path with $S$ steps. Orange shading indicates cosine error of DPK test gradients versus EPK test gradients shown in practice in Fig.~\ref{fig:error}. }
\label{fig:vecs}
\end{figure}

% \begin{figure}[!ht]
% % \begin{tikzpicture}
% % \node
% % \end{tikzpicture}
% \end{figure}
\begin{figure}[!ht]
        \centering
        % \begin{minipage}{0.5\textwidth}
        \includegraphics[width=1.01\linewidth]{c4_figures/estimation_comparison.pdf}
        % \end{minipage}

        \caption{Measurement of gradient alignment on test points across the training path. The EPK is used as a frame of reference. The y-axis is exactly the difference between the EPK and other representations. For example $EPK-DPK = \langle \phi_{s,t}(X), \phi_{s,t}(x) - \phi_{s,0}(x) \rangle$ (See Definition 3.4). Shaded regions indicate total accumulated error. Note: this is measuring an angle of error in weight space; therefore, equivalent positive and negative error will not result in zero error.}
        \label{fig:error}
\end{figure}

There has recently been a surge of interest in the connection between neural networks and kernel methods~\cite{bietti2019bias, du2019graphntk, tancik2020fourierfeatures, abdar2021uq, geifman2020similarity, chen2020generalized, alemohammad2021recurrent}. Much of this work has been motivated by the the neural tangent kernel (NTK), which describes the training dynamics of neural networks in the infinite limit of network width~\cite{jacot2018neural}.
We argue that many intriguing behaviors arise in the \emph{finite} parameter regime~\cite{DBLP:conf/nips/BubeckS21}. 
All prior works, to the best of our knowledge, appeal to discrete approximations of the kernel corresponding to a neural network. 
Specifically, prior approaches are derived under the assumption that training step size is small enough to guarantee close approximation of a gradient flow
%. These approximations have been speculated to open up parametric models trained with gradient descent, including artificial neural networks (ANNs), to many theoretical tools available to kernel methods
~\cite{ghojogh2021, shawe2004kernel, zhao2005extracting}.

In this work, we show that the simplifying assumptions used in prior works (i.e. infinite network width and infinitesimal gradient descent steps) are not necessary. Our \textbf{Exact Path Kernel (EPK)} provides the first, exact method to study the behavior of finite-sized neural networks used for classification.
Previous results are limited in application ~\cite{incudini2022quantum} due to dependence of the kernel on test data unless strong conditions are imposed on the training process as by ~\cite{chen2021equivalence}. We show, however, that the training step sizes used in practice do not closely follow this gradient flow, introducing significant error into all prior approaches (Figure~\ref{fig:error}).
%use of such tools is highly dependent on the assumption that ANN training is a faithful discrete approximation of the smooth path kernel at practical step sizes. The accuracy of this assumption depends on difficult measurements of convergence and error.

Our experimental results build on prior studies attempting to evaluate empirical properties of the kernels corresponding to finite neural networks ~\cite{DBLP:conf/iclr/LeeBNSPS18, chen2021equivalence}. While the properties of infinite neural networks are fairly well understood~\cite{neal1996priors}, we find that the kernels learned by finite neural networks have non-intuitive properties that may explain the failures of modern neural networks on important tasks such as robust classification and calibration on out-of-distribution data.

This paper makes the following significant theoretical and experimental contributions:
\begin{enumerate}
    \item We prove that finite-sized neural networks trained with finite-sized gradient descent steps and cross-entropy loss can be exactly represented as kernel machines using the EPK. Our derivation incorporates a previously-proposed path kernel, but extends this method to account for practical training procedures~\cite{domingos2020every, chen2021equivalence}.
  
    \item We demonstrate that it is computationally tractable to estimate the kernel underlying a neural network classifier, including for small convolutional computer vision models.
    % \item We estimate a kernel corresponding to both a toy-sized neural network and a convolutional classifier on MNIST up to machine precision.
    \item We compute Gram matrices using the EPK and use them to illuminate prior theory of neural networks and their understanding of uncertainty. 
    \item We employ Gaussian processes to compute the covariance of a neural network's logits and show that this reiterates previously observed shortcomings of neural network generalization.
\end{enumerate}

\section{Related Work}
% Interpreting and understanding 

% The neural tangent kernel (NTK) ~\cite{he2020bayesian} has received significant attention recently in light of a growing body of work relating parametric gradient models in infinite width with Gaussian processes (GPs) and also the NTK. 
Fundamentally, the neural tangent kernel (NTK) is rooted in the concept that all information necessary to represent a parametric model is stored in the Hilbert space occupied by the model's weight gradients up to a constant factor. 
This is very well supported in infinite width ~\cite{jacot2018neural}. 
In this setting, it has been shown that neural networks are equivalent to support vector machines, drawing a connection to maximum margin classifiers ~\cite{chen2021equivalence, chizat2020maxmargin}.
Similarly, Shah et al. demonstrate that this maximum margin classifier exists in Wasserstien space; however, they also show that model gradients may not contain the required information to represent this ~\cite{shah2021input}.
% However, discrete approximation of an infinite-width kernel is very limiting.

% This formulation derives a kernel for finite-parametric models however it relies on a continuous integration over a gradient flow, while real-world models are trained by discrete Forward Euler steps through a gradient field.
% The exploration of discrete approximations has been a key focus in addressing the challenges associated with neural networks. 
The correspondence between kernel machines and parametric models trained by gradient descent has been previously developed in the case of a continuous training path (i.e. the limit as gradient descent step size $\varepsilon \to 0$)
% One notable approach is the formulation of the continuous path kernel, which aims to derive a kernel for finite-parametric models
~\cite{domingos2020}. We will refer to the previous finite approximation of this kernel as the Discrete Path Kernel (DPK).
However, a limitation of this formulation is its reliance on a continuous integration over a gradient flow, which differs from the discrete forward Euler steps employed in real-world model training. 
This discrepancy raises concerns regarding the applicability of the continuous path kernel to practical scenarios ~\cite{incudini2022quantum}.
Moreover, the formulation of the sample weights and bias term in the DPK depends on its test points. Chen et al. propose that this can be addressed, in part, by imposing restrictions on the loss function used for training, but did not entirely disentangle the kernel formulation from sample importance weights on training points ~\cite{chen2021equivalence}.

We address the limitations of \citet{domingos2020} and \citet{chen2021equivalence} in Subsection %~\remref{rem5}
~\ref{subsec:disc}. By default, their approach produces a system which can be viewed as an ensemble of kernel machines, but without a single aggregated kernel which can be analyzed directly. ~\citet{chen2021equivalence} propose that the resulting sum over kernel machines can be formulated as a kernel machine so long as the sign of the gradient of the loss stays constant through training; however, we show that this is not necessarily a sufficient restriction. Instead, their formulation leads to one of several non-symmetric functions which can serve as a surrogate to replicate a given models behavior, but without retaining properties of a kernel.

% I don't think we need this paragraph
% Previous studies have explored the use of various discrete kernel approximations. Tancik et al. employed Fourier features to approximate the kernel function, resulting in improved performance on coordinate-based MLP networks ~\cite{tancik2020fourierfeatures}.
% The broader justification for this thread of research surrounds the potential value of having a single kernel representation for a neural network.
% Various discrete kernel approximations have been used with interesting affects in order to adapt neural networks to achieve desirable kernel properties including stationarity and uncertainty quantification ~\cite{wang2022pinns, tancik2020fourierfeatures}. 


% Similarly, Wang et al. (2022) utilized discrete kernel approximations to adapt neural networks, specifically focusing on achieving desirable properties such as stationarity and uncertainty quantification~\cite{wang2022pinns}. These approaches highlight the versatility and potential of discrete kernel approximations in enhancing the performance and interpretability of neural networks.

% ~\cite{shah2021input}

% ~\cite{incudini2022quantum} % CERN (unpublished?) paper discussing discrete path kernel and quantum context. They address limitations of domingos and chen, bring up non-uniqueness, but do not provide any real examples or proofs. Limitations : no discrete qualifier, "effective path kernel" is unfounded and a little odd. 

% Something sets us up for the limitations and justifies our discrete path kernel exact representation to dig into these. Also set up studying spatial properties with neural tangent kernels?

% ~\cite{domingos2020} % poses path kernel, limitation: depends on X, approximate. 
% ~\cite{chen2021equivalence} % refines path kernel and does a lot of discussion, mentions discrete path kernel, limitations : constant signed loss function, does not figure out precise discrete formulation
% ~\cite{chizat2020maxmargin} % shows NNs form max margin classifiers wrt wasserstein
% ~\cite{shah2021input} % talks about limitations of gradients to explain what's going on due to chizat max margin equivalence [4]

\section{Theoretical Results}

Our goal is to show an equivalence between any given finite parametric model trained with gradient descent $f_w(x)$  (e.g. neural networks) and a kernel based prediction that we construct. We define this equivalence in terms of the output of the parametric model $f_w(x)$ and our kernel method in the sense that they form identical maps from input to output. In the specific case of neural network classification models, we consider the mapping $f_w(x)$ to include all layers of the neural network up to and including the log-softmax activation function. Formally:
\begin{definition}
A {kernel} is a function of two variables which is symmetric and positive semi-definite. 
\end{definition}

\begin{definition}
Given a Hilbert space $X$, a test point $x \in X$, and a training set $X_T = \{x_1,x_2,...x_n\} \subset X$ indexed by $I$, a \emph{Kernel Machine} is a model characterized by 
\begin{align}
    \text{K}(x) = b + \sum_{i\in I} a_i k(x,x_i)
\end{align}
where the $a_i \in \mathbb{R}$ do not depend on $x$, $b \in \mathbb{R}$ is a constant, and $k$ is a kernel. ~\cite{rasmussen2006gaussian}

By Mercer's theorem ~\cite{ghojogh2021} a kernel can be produced by composing an inner product on a Hilbert space with a mapping $\phi$ from the space of data into the chosen Hilbert space.
We use this property to construct a kernel machine of the following form.
\begin{align}
    \text{K}(x) = b + \sum_{i\in I} a_i \langle \phi(x), \phi(x_i) \rangle
\end{align}
\end{definition}
Where $\phi$ is a function mapping input data into the weight space via gradients. Our $\phi$ will additionally differentiate between test and training points to resolve a discontinuity that arises under discrete training. 

\subsection{Exact Path Kernels}

 We first derive a kernel which is an exact representation of the change in model output over one training step, and then compose our final representation by summing along the finitely many steps.
Models trained by gradient descent can be characterized by a discrete set of intermediate states in the space of their parameters.
These discrete states are often considered to be an estimation of the gradient flow, however in practical settings where $\epsilon \not \rightarrow 0$ these discrete states differ from the true gradient flow.
Our primary theoretical contribution is an algorithm which accounts for this difference by observing the true path the model followed during training.
Here we consider the training dynamics of practical gradient descent steps by integrating a discrete path for weights whose states differ from the gradient flow induced by the training set.
% As such, the model's training dynamics do not exactly follow the the gradient flow defined by the model's loss function (i.e. the $\epsilon \rightarrow 0$ limit in forward Euler). 
%These states are commonly computed by gradient descent.

\textbf{Gradient Along Training Path vs Gradient Field:}
In order to compute the EPK, gradients on training data must serve two purposes. 
First, they are the reference points for comparison (via inner product) with test points. 
Second, they determine the path of the model in weight space. 
% % For a continuous path kernel which follows a gradient flow, gradients on the training data exactly match (determine) the path of the parameters through the gradient field.
% This would allow us to simply evaluate the gradient of the training data directly.
% This means that that for every point, we can simply evaluate the gradient of the training data directly.
% Unfortunately, the path followed in practice is not the gradient flow.
In practice, the path followed during gradient descent does not match the gradient field exactly. 
Instead, the gradient used to move the state of the model forward during training is only computed for finitely many discrete weight states of the model.
In order to produce a path kernel, we must \textit{continuously} compare the model's gradient at test points with \textit{fixed} training gradients along each discrete training step $s$ whose weights we we interpolate linearly by $w_s(t) = w_s - t(w_s - w_{s+1})$. We will do this by integrating across the gradient field induced by test points, but holding each training gradient fixed along the entire discrete step taken. This creates an asymmetry, where test gradients are being measured continuously but the training gradients are being measured discretely (see Figure~\ref{fig:vecs}).
% One problem encountered when constructing a path kernel for discrete steps is the which requires addressing is the different ways which this learned kernel treats its training points compared to all other inputs.
% The path taken during training is dependent on the combination of model, training data, learning algorithm and loss function.
% Because the final path includes dependence on training data, the final kernel that represents this path will also depend on the training data.

To account for this asymmetry in representation, we will redefine our data using an indicator to separate training points from all other points in the input space.
\begin{definition}
\label{fpm}
Let $X$ be two copies of a Hilbert space $H$ with indices $0$ and $1$ so that $X = H \times \{0,1\}$. We will write $x \in H \times \{0,1\}$ so that $x = (x_H, x_I)$ (For brevity, we will omit writing $_H$ and assume each of the following functions defined on $H$ will use $x_H$ and $x_I$ will be a hidden indicator).
Let $ f_{w}$ be a differentiable function on $H$ parameterized by $w \in \mathbb{R}^d$. Let $X_T = \{(x_i, 1)\}_{i=1}^M$ be a finite subset of $X$ of size $M$ with corresponding observations $Y_T = \{y_{x_i}\}_{i=1}^M$ with initial parameters $w_0$ so that there is a constant $b \in \mathbb{R}$ such that for all $x$, $ f_{w_0}(x) = b$. Let $L$ be a differentiable loss function of two values which maps $(f(x), y_x)$ into the positive real numbers. Starting with $f_{w_0}$, let $\{w_s\}$ be the sequence of points attained by $N$ forward Euler steps of fixed size $\varepsilon$ so that $w_{s+1} = w_{s} - \varepsilon \nabla L(f(X_T), Y_T)$. Let $x \in H \times \{0\}$ be arbitrary and within the domain of $f_w$ for every $w$. Then $f_{w_s(t)}$ is a \emph{finite parametric gradient model (FPGM)}. 
\end{definition}

\begin{definition}
\label{epk}

Let $f_{w_s(t)}$ be an FPGM with all corresponding assumptions. Then, for a given training step $s$, the \emph{exact path kernel} (EPK) can be written  
\begin{equation}
 K_{\text{EPK}}(x, x', s) = \int_0^1\langle \phi_{s,t}(x), \phi_{s,t}(x')\rangle dt
 \label{eq2}
\end{equation}
where
\begin{align}
% a_{i, s} &= -\varepsilon  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial f_i} \in \mathbb{R} \\
\phi_{s, t}(x) &=  \nabla_w f_{w_s(t,x)} (x)\\
w_s(t) &= w_s - t(w_s - w_{s+1})\\
w_s(t,x) &= \begin{cases} w_s(0), & \text{if } x_I = 1\\ w_s(t), & \text{if } x_I = 0 \end{cases}
% b &= f_{w_0}(x) 
\end{align}
\textbf{Note:} $\phi$ is deciding whether to select a continuously or discrete gradient based on whether the data is from the training or testing copy of the Hilbert space $H$. This is due to the inherent asymmetry that is apparent from the derivation of this kernel (see Appendix section~\ref{proof:eker}). This choice avoids potential discontinuity in the kernel output when a test set happens to contain training points. 
\end{definition}
\begin{restatable}{lemma}{ker}
The exact path kernel (EPK) is a kernel.
\end{restatable}
% \begin{proof}
% We must show that the associated kernel matrix $K_{\text{DPK}} \in \mathbb{R}^{n\times n}$ defined for an arbitrary subset of data $\{x_i\}_{i=1}^M \subset X$ as $K_{\text{DPK},i,j} = \int_0^1\langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt$ is both symmetric and positive semi-definite.

% Since the inner product on a Hilbert space $\langle \cdot, \cdot \rangle$ is symmetric and since the same mapping $\varphi$ is used on the left and right, $K_{\text{DPK}}$ is \textbf{symmetric}. 

% To see that $K_{\text{DPK}}$ is \textbf{Positive Semi-Definite}, let $f = (f_1, f_2, \dots, f_n)^\top \in \mathbb{R}^n$ be any vector. We need to show that $f^\top K_{\text{DPK}} f \geq 0$. We have

% \begin{align*}
% f^\top K_{\text{DPK}} f &= \sum_{i=1}^n \sum_{j=1}^n f_i f_j \int_0^1 \langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt \\
% &= \sum_{i=1}^n \sum_{j=1}^n f_i f_j \int_0^1 \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n f_i f_j \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n  \langle f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, f_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1    \langle \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, \sum_{j=1}^n f_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% & \text{Re-ordering the sums so that their indices match, we have}\\
% &= \int_0^1 \left\lVert \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}\right\rVert^2 dt \\
% &\geq 0,
% \end{align*}

% We note that this reordering does not depend on the continuity of our mapping function $\phi_{s,t}(x_i)$. 
% \end{proof}
\begin{restatable}[Exact Kernel Ensemble Representation]{theorem}{eker}
\label{thm:eker}
A model $f_{w_N}$ trained using discrete steps matching the conditions of the exact path kernel has the following exact representation as an ensemble of $N$ kernel machines:
\begin{equation}
f_{w_N} = \text{KE}(x) :=  \sum_{s = 1}^N \sum_{i = 1}^{M} a_{i,s} K_{\text{EPK}}(x, x', s) + b
\label{ensemble}
\end{equation}
where
\begin{align}
a_{i, s} &= -\varepsilon  \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)} \\
% \phi_{s, t}(x) &=  \nabla_w f_{w_s(t,x)} (x)\\
% w_s(t,x) &= \begin{cases} w_s, & \text{if } x_I = 1\\ w_s(t), & \text{if } x_I = 0 \end{cases}
b &= f_{w_0}(x)
\end{align}
\end{restatable}

\begin{sproof}
Assuming the theorem hypothesis, we'll measure the change in model output as we interpolate across each training step $s$ by measuring the change in model state along a linear parametrization $w_s(t) = w_s - t(w_s - w_{s+1})$. We will let $d$ denote the number of parameters of $f_w$. For brevity, we define $L(x_i, y_i)= l(f_{w_s(0)}(x_i),  y_i)$ where $l$ is the loss function used to train the model.
\begin{align}
    \dfrac{d \hat y}{dt} &= \sum_{j = 1}^{d} \dfrac{d \hat y}{\partial w_j} \dfrac{d w_j}{dt}\\
&= \sum_{j = 1}^{d} \dfrac{d f_{w_s(t)}(x)}{\partial w_j} \left(-\varepsilon \sum_{i = 1}^{M}\dfrac{\partial L(x_i, y_i)}{\partial f_{w_s(0)}(x_i)}\dfrac{\partial f_{w_s(0)}(x_i)}{\partial w_j}\right) \label{eq11}
\end{align}
We use fundamental theorem of calculus to integrate this equation from step $s$ to  step $s+1$ and then add up across all steps. See Appendix~\ref{proof:eker} for the full proof.
\end{sproof}

\textbf{Remark ~\remlabel{rem:init}} Note that in this formulation, $b$ depends on the test point $x$.
In order to ensure information is not being leaked from the kernel into this bias term the model $f$ must have constant output for all input. 
When relaxing this property, to allow for models that have a non-constant starting output, but still requiring $b$ to remain constant, we note that this representation ceases to be exact for all $x$.
The resulting approximate representation has logit error bounded by its initial bias which can be chosen as $b = \text{mean}(f_{w_0(0)}(X_T))$.
Starting bias can be minimized by starting with small parameter values which will be out-weighed by contributions from training.
In practice, we sidestep this issue by initializing all weights in the final layer to $0$, resulting in $b=\text{log}(\text{softmax}(0))$, thus removing $b$'s dependence on $x$.

\textbf{Remark ~\remlabel{rem:exact}} 
The exactness of this proof hinges on the \emph{separate} measurement of how the model's parameters change.
The gradients on training data, which are fixed from one step to the next, measure how the parameters are changing.
This is opposed to the gradients on test data, which are \textit{not} fixed and vary with time.
These measure a continuous gradient field for a given point.
We are using interpolation as a way to measure the difference between the step-wise linear training path and the continuous loss gradient field. 

\newcommand{\pluseq}{\mathrel{+}=}
\begin{algorithm*}[h]
    \caption{Exact Path Kernel: Given a training set $(X, Y)$ with $M$ data points, a testing point $x$ and $N$ weight states $\{w_0, w_1 ... w_N\}$, the kernel machine corresponding to the exact path kernel can be calculated for a model with $W$ weights and $K$ outputs. We estimate the integral across test points by calculating the Riemann sum with sufficient steps ($T$) to achieve machine precision. For loss functions that do not have constant gradient values throughout training, this algorithm produces an ensemble of kernel machines.}
    \label{alg:exact}
\begin{algorithmic}
    % \STATE {\bfseries Input:} $(X_T, Y_T)$, $x$, $w_s \in S$
    \STATE $b = f(w_0, x)$
    \FOR{$s=0$ \textbf{to} $N$}
    \STATE $J^{X} = \nabla_{w} f_{w_s(0)}(X)$ \hfill \COMMENT{Jacobian of training point outputs w.r.t model weights $[M \times K \times W]$ }
    \FOR{t \textbf{from} 0 \textbf{to} 1 \textbf{with step} 1/T}
    \STATE $w_s(t) = w_s + t(w_{s+1} - w_s)$
    \STATE $J^{x} \pluseq \dfrac{1}{T} \nabla_{w} f_{w_s(t)}(x)$ \hfill \COMMENT{Jacobian of testing point output w.r.t model weights averaged across $T$ steps $ [K \times W]$}
    \ENDFOR
    \STATE $G_{ijk} =  \sum_w J_{ijw}^{X} J_{kw}^x$ \hfill \COMMENT{Inner product on the weight space, this is the kernel value $[M \times K \times K]$}
    \STATE $L' = \nabla_{f}L(f_{w_s(0)}(X), Y)$ \hfill \COMMENT{Jacobian of loss w.r.t model output of training points $[M \times K]$}
    \STATE $P^s_{ik} = \sum_j L'_{ij} G_{ijk}$ \hfill \COMMENT{Inner product of kernel value scaled by loss gradients $[M \times K$]}
    \ENDFOR
    \STATE $\mathcal{P}_{sik} = \{P^0, P^1, ..., P^N\}$ \hfill \COMMENT{Stack values across all training steps $[N \times M \times K]$}
    \STATE $\hat p = -\varepsilon \dfrac{1}{M} \sum_s \sum_i \mathcal{P}_{sik} + b$ \hfill \COMMENT{Sum across training steps and average across training points for final prediction $[K]$}
\end{algorithmic}
% \caption{caption}
\end{algorithm*}

\begin{restatable}[Exact Kernel Machine Reduction]{theorem}{ekr}
\label{thm:ekr}
Let $\nabla L(f(w_{s}(x), y)$ be constant  across steps $s$, $(a_{i,s}) = (a_{i,0})$. Let the kernel across all $N$ steps be defined as $K_{\text{NEPK}}(x,x') = \sum_{s = 1}^N a_{i,0} K_{\text{EPK}}(x, x', s)$ Then the exact kernel ensemble representation for $f_{w_N}$ can be reduced exactly to the kernel machine representation:
\begin{equation}
f_{w_N}(x) = \text{KM}(x) := b + \sum_{i = 1}^{M} a_{i,0} K_{\text{NEPK}}(x,x')
\label{exact}
\end{equation}
\end{restatable}
See Appendix~\ref{proof:ekmr} for full proof. By combining theorems ~\ref{thm:eker} and ~\ref{thm:ekr}, we can construct an exact kernel machine representation for any arbitrary parameterized model trained by gradient descent which satisfies the additional property of having constant loss across training steps (e.g. any ANN using catagorical cross-entropy loss (CCE) for classification). This representation will produce exactly identical output to the model across  the model's entire domain. This establishes exact kernel-neural equivalence for classification ANNs. Furthermore, Theorem ~\ref{thm:eker} establishes an exact kernel ensemble representation without limitation to models using loss functions with constant derivatives across steps. It remains an open problem to determine other conditions under which this ensemble may be reduced to a single kernel representation.  

% \begin{tikzpicture}
% \node[\text{[insert graphic of forward euler with vectors]}
% \end{tikzpicture}


% \begin{theorem}
% Let $ y_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed size $\varepsilon$ on a finite subset $X_T = \{x_i\}_{i=1}^M$ of a Hilbert space $X$ of size $M$ with labels $Y_T = \{y_i\}_{i=1}^M$, with initial parameters $w_0$ so that there is a constant $b \in \mathbb{R}$ such that $\forall x$, $ y_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x$ be an arbitrary point in the domain of $\hat y_w$ for every $w$. Then $\hat y_{w_N}$ (the final trained state of the model) has the following exact representation: 
% \begin{equation}
% \hat y_{w_N}(x) = b + \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i,s} \int_0^1\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\rangle dt
% \end{equation}
% where
% \begin{align}
% a_{i, s} &= -\varepsilon  \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i} \in \mathbb{R} \\
% \phi_{s,t}(x) &=  \nabla_w \hat y_{w_s(t,x)} (x)\\
% w_s(t,x) &= \begin{cases} w_s, x \in X_T\\ w_s(t), x \notin X_T \end{cases}
% \end{align}
% Which is to say that all such models are Kernel Methods. 
% \end{theorem}

% \begin{proof}
% Let $\hat y_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed step size $\varepsilon$ on a training dataset $X$ with labels $ Y$, with initial parameters $w_0$ so that there is a constant $b$ such that $\forall x$, $\hat y_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x$ be an arbitrary point in the domain of $\hat y_w$ for every $w$. For the final trained state of this model $\hat y_{w_N}$, let $y = \hat y_{w_N}(x)$. 

% For one step of training, we consider $y_s  = \hat y_{w_s(0)}(x)$ and $y_{s+1} = \hat y_{w_{s+1}}(x)$. We wish to account for the change $y_{s+1} - y_s$ in terms of a gradient flow, so we must compute $\dfrac{\partial \hat y}{dt}$ for a continuously varying parameter $t$. Since $f$ is trained using forward Euler with a step size of $\varepsilon > 0$, this derivative is determined by a step of fixed size of the weights $w_s$ to $w_{s+1}$. We will parameterize this step in terms of the weights:

% \begin{align}
%     \dfrac{\partial w_s(t)}{dt} &= (w_{s+1} - w_s)\\   
%     \int \dfrac{\partial w_s(t)}{dt} dt &= \int (w_{s+1} - w_s)dt\\
%     w_s(t) &= w_s + t(w_{s+1} - w_s)\\
% \end{align}
% Since $f$ is being trained using forward Euler, we can write:
% \begin{align}
%     \dfrac{\partial w_s(t)}{dt} &= -\varepsilon \nabla_w L(\hat y_{w_s(0)}(x_i), y_i) = -\varepsilon \sum_{j = 1}^{d} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial w_j} \label{eq10}
% \end{align}
% Applying chain rule and the above substitution, we can write
% \begin{align}
%     \dfrac{\partial \hat y}{dt} = \dfrac{d \hat y_{w_s(t)}}{dt} &= \sum_{j = 1}^{d} \dfrac{\partial \hat y}{\partial w_j} \dfrac{\partial w_j}{dt}\\
% &= \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j} \left(-\varepsilon \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial w_j}\right)\\
% &= \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j} \left(-\varepsilon \sum_{i = 1}^{M}\dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}\dfrac{\partial \hat y_{w_s(0)}(x_i)}{\partial w_j}\right)\\
% &= -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i} \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j}  \dfrac{\partial \hat y_{w_s(0)}(x_i)}{\partial w_j}\\
% &= -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(_i),  y_i)}{\partial \hat y_i}  \nabla_w \hat y_{w_s(t)}(x) \cdot \nabla_w \hat y_{w_s(0)}(x_i)\\
% \end{align}
% Using the fundamental theorem of calculus, we can compute the change in the model's output over step $s$
% \begin{align}
%     y_{s+1} - y_s &= \int_0^1 -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \nabla_w \hat y_{w_s(t)}(x) \cdot \nabla_w \hat y_{w_s(0)}(x_i)dt\\
%  &=  -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s(0)}(x_i)\\
% \end{align}
% For all $N$ training steps, we have
% \begin{align}
% y_N &= b + \sum_{s=1}^N y_{s+1} - y_s\\
% y_N &= \sum_{s = 1}^N -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s(0)}(x_i)\\
% % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s(0)}(x_i)\\
% % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t)}(x), \nabla_w \hat y_{w_s(0)}(x_i) \right\rangle dt\\ 
% &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t,x)}(x), \nabla_w \hat y_{w_s(t,x_i)}(x_i) \right\rangle dt\\ 
% &= \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i, s}  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt
% \end{align}
% Since an integral of a symmetric positive semi-definite function is still symmetric and positive-definie and likewise for discrete sums, this represention is a kernel method. 

% \end{proof}
\subsection{Discussion}
%\textbf{Remark \remlabel{rem0}} 


%\textbf{Remark \remlabel{rem1}} 
$\phi_{s,t}(x)$ depends on both $s$ and $t$, which is non-standard but valid, however an important consequence of this mapping is that the output of this representation is not guaranteed to be continuous. This discontinuity is exactly measuring the error between the model along the exact path compared with the gradient flow for each step. 

We can write another function $k'$ which is continuous but not symmetric, yet still produces an exact representation:
\begin{align}
k'(x, x') = \langle \nabla_w f_{w_s(t)}(x), \nabla_w f_{w_s(0)}(x')\rangle
\end{align}
The resulting function is a valid kernel if and only if for every $s$ and every $x$, 
\begin{align}
\label{eq:cond}
    \int_0^1 \nabla_w f_{w_s(t)}(x)dt = \nabla_w f_{w_s(0)}(x)
\end{align}

%\textbf{Remark \remlabel{rem3}} 
We note that since $f$ is being trained using forward Euler, we can write:
\begin{align}
    \dfrac{\partial w_s(t)}{dt} &= -\varepsilon \nabla_w L(f_{w_s(0)}(x_i), y_i) \label{dstep}% = -\varepsilon \sum_{j = 1}^{d} \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial w_j} \label{rem3}
\end{align}
In other words, our parameterization of this step depends on the step size $\varepsilon$ and as $\varepsilon \to 0$, we have 
\begin{align}
    \int_0^1 \nabla_w f_{w_{s}(t)}(x)dt \approx \nabla_w f_{w_s(0)}(x)
\end{align}
In particular, given a model $f$ that admits a Lipshitz constant $K$ this approximation has error bounded by $\varepsilon K$ and a proof of this convergence is direct. 
This demonstrates that the asymmetry of this function is exactly measuring the disagreement between the discrete steps taken during training with the gradient field. 
This function is one of several subjects for further study, particularly in the context of Gaussian processes whereby the asymmetric Gram matrix corresponding with this function can stand in for a covariance matrix. It may be that the not-symmetric analogue of the covariance in this case has physical meaning relative to uncertainty.

\subsection{Independence from Optimization Scheme}
We can see that by changing equation ~\ref{dstep} we can produce an exact representation for any first order discrete optimization scheme that can be written in terms of model gradients aggregated across subsets of training data. This could include backward Euler, leapfrog, and any variation of adaptive step sizes. This includes stochastic gradient descent, and other forms of subsampling (for which the training sums need only be taken over each sample). One caveat is adversarial training, whereby the $a_i$ are now sampling a measure over the continuum of adversarial images. We can write this exactly, however computation will require approximation across the measure. Modification of this kernel for higher order optimization schemes remains an open problem.

%\textbf{Remark \remlabel{rem2}} 

\begin{figure*}[!ht]
    \centering
        \includegraphics[width=0.3\textwidth]{c4_figures/svm1.png}\includegraphics[width=0.3\textwidth]{c4_figures/svm2.png}\includegraphics[width=0.3\textwidth]{c4_figures/svm3.png}
    \caption{Updated predictions with kernel $a_i$ updated via gradient descent with training data overlaid for classes 1 (left), 2 (middle), and 3 (right). The high prediction confidence in regions far from training points demonstrates that the learned kernel is non-stationary.}
    \label{fig:svm}
\end{figure*}
\subsection{Ensemble Reduction}
%\textbf{Remark \remlabel{rem4}} 
In order to reduce the ensemble representation of Equation ~\eqref{ensemble} to the kernel representation of Equation ~\eqref{exact}, we require that the sum over steps still retain the properties of the kernel (symmetry and positive semi-definiteness). In particular we require that for every subset of the training data ${x_i}$ and arbitrary ${\alpha_i}$ and ${\alpha_j}$, we have
\begin{align}
    \sum_{i=1}^n\sum_{j=1}^n \sum_{l=1}^M \sum_{s=1}^N \alpha_i \alpha_j a_{l, s}\int_{0}^1 K_{\text{EPK}}(x_i,x_j) dt \geq 0
\end{align}
A sufficient condition for this reduction is that the gradient of the loss function does not change throughout training. This is the case for categorical cross-entropy where labels are in $\{0,1\}$. In fact, in this specific context the gradient of the loss function does not depend on $f(x)$, and are fully determined by the ground truth label, making the gradient of the cross-entropy loss a constant value throughout training (See Appendix section ~\ref{proof:ekmr}). Showing the positive-definiteness of more general loss functions (e.g. mean squared error loss) will likely require additional regularity conditions on the training path, and is left as future work.
% There are other conditions which may be imposed in order to guarantee this reduction, and it remains to be studied whether this is unconditionally true for certain training paths.
%  We demonstrate that constant sign of the loss gradient is a sufficient but not necessary condition for positive-semi-definiteness of the kernel.

\subsection{Prior Work}
\label{subsec:disc}
%\textbf{Remark \remlabel{rem5}} 
Constant sign loss functions have been previously studied by Chen et al. ~\cite{chen2021equivalence}, however the kernel that they derive for a finite-width case is of the form
\begin{align}
    K(x,x_i) =  \int_0^T |\nabla_f L(f_t(x_i), y_i)| \langle \nabla_w f_t(x), \nabla_w f_t(x_i) \rangle dt
\end{align}
The summation across these terms satisfies the positive semi-definite requirement of a kernel, however the weight $|\nabla L(f_t(x_i), y_i)|$ depends on $x_i$ which is one of the two inputs. This makes the resulting function $K(x,x_i)$ asymmetric and therefore not a kernel.
% In this formulation, weight $|\nabla L(f_t(x_i), y_i)|$ depends on $x_i$ which is one of the two inputs. This makes the resulting function $k$ asymmetric and therefore not a kernel.

%\textbf{Remark \remlabel{rem6}} 
\subsection{Uniqueness}
Uniqueness of this kernel is not guaranteed. 
The mapping from paths in gradient space to kernels is in fact a function, meaning that each finite continuous path has a unique exact kernel representation of the form described above. 
However, this function is not necessarily onto the set of all possible kernels. 
This is evident from the existence of kernels for which representation by a finite parametric function is impossible.
Nor is this function necessarily one-to-one since there is a continuous manifold of equivalent parameter configurations for neural networks.
For a given training path, we can pick another path of equivalent configurations whose gradients will be separated by some constant $\delta > 0$.
The resulting kernel evaluation along this alternate path will be exactly equivalent to the first, despite being a unique path. 
We also note that the linear path $l_2$ interpolation is not the only valid path between two discrete points in weight space.
Following the changes in model weights along a path defined by Manhattan Distance is equally valid and will produce a kernel machine with equivalent outputs.
It remains an open problem to compute paths from two different starting points which both satisfy the constant bias condition from Definition~\eqref{epk} which both converge to the same final parameter configuration and define different kernels.

\section{Experimental Results}
    % \begin{figure}
    %     \centering
    %     \begin{minipage}{0.45\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/sample_dataset.pdf}
    %     \end{minipage}
    %     \begin{minipage}{0.45\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/model_kernel_predictions.pdf}
    %     \end{minipage}
    %     \caption{On the left is a 2d dataset of points sampled from Gaussians with different means. Specifically, class A is normally distributed with $\mu = \left[1, 4\right]$ and $\sigma^2 = 1$ while class B is $\mu = \left[4, 1\right]$ and $\sigma^2 = 1$. 2000 data points were sampled for each class. These values were chosen arbitrarily to provide separation with a limited amount of overlap. On the right is the prediction similarity between the kernel and the original model. This demonstrates that our kernel formulation accurately represents the trained network.}
    %     \label{fig:sample_data}
    % \end{figure}
    Our first experiments test the kernel formulation on a dataset 
    which can be visualized in 2d. These experiments serve as a sanity check
    and provide an interpretable representation of what the kernel is learning.
    \begin{figure}[!ht]
        \centering

        \includegraphics[width=0.40\textwidth]{c4_figures/image.png}
        \caption{Class 1 EPK Kernel Prediction (Y) versus neural network prediction (X) for 100 test points, demonstrating extremely close agreement.}
        \label{fig:toymatch}
    \end{figure}

\subsection{Evaluating The Kernel} \label{subsec:evaluate}
A small test data set within 100 dimensions is created by generating 1000 random samples with means $(1,4,0,...)$, $(4,1,0,...)$ and $(5,5,0,...)$ and standard deviation $1.0$. These points are labeled according to the mean of the Gaussian used to generate them, providing 1000 points each from 3 classes. A fully connected ReLU network with 1 hidden layer is trained using categorical cross-entropy (CCE) and gradient descent with gradients aggregated across the entire training set for each step. We then compute the EPK for this network, approximating the integral from Equation~\ref{eq2} with 100 steps which replicates the output from the ReLU network within machine precision. The EPK (Kernel) outputs are compared with neural network predictions in Fig.~\ref{fig:toymatch} for class 1. Having established this kernel, and its corresponding kernel machine, one natural extension is to allow the kernel weights $a_i$ to be retrained. We perform this updating of the krenel weights using a SVM and present its predictions for each of three classes in Fig.~\ref{fig:svm}.
    


    % \begin{figure}[h]
    %     \centering
    %     \begin{minipage}{0.45\textwidth}
    %         \centering
    %         % \begin{wrapfigure}{l}
    %         \includegraphics[width=0.95\linewidth]{figures/in_distribution_uncertan.pdf}
    %         % \caption{In Distribution}
    %         % \end{wrapfigure}
    %         % \captionof{figure}{Figure 1 is a figure}
    %     \end{minipage}
    %         \begin{minipage}{0.45\textwidth}
    %             \centering
    %             \includegraphics[width=0.95\linewidth]{figures/ood_positive_2.pdf}
    %             % \captionof{figure}{Figure 1 is a figure}
    %         \end{minipage}
    % %   \caption{Another figure caption.}
    % % \end{figure}

    % % \begin{figure}
    %     \centering
    %     \begin{minipage}{0.45\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/in_distribution.pdf}
    %         \captionsetup{labelformat=empty}
    %         \captionof{figure}{In-Distribution}
    %         \addtocounter{figure}{-1}
    %         \end{minipage}
    %         \begin{minipage}{0.45\textwidth}
    %             \centering
    %             \includegraphics[width=0.95\linewidth]{figures/ood.pdf}
    %         \captionsetup{labelformat=empty}
    %         \captionof{figure}{Out-Of-Distribution}
    %         \addtocounter{figure}{-1}
    %         \end{minipage}
    %     \caption{Example of the kernel values on in-distribution and out-of-distribution (OOD) data. Left column shows samples which are in-distribution for our dataset. Right column row shows OOD samples.}
    %     \label{fig:kernel}
    % \end{figure}

    % We are able to see the agreement between the neural network and its kernel representation in  figure ~\ref{fig:agree}. In figure ~\ref{fig:near} we see that the function learned by the kernel does not directly mimic euclidean distance in the image space. Samples which are nearby in kernel space are not necessarily nearby in pixel space. The similarity metric learned is a direct explanation of how the neural network is making decisions.

    %     \begin{figure}
    %     \centering
    %     \begin{minipage}{0.2\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/samples/original_0.png}
    %         % \captionof{figure}{Figure 1 is a figure}
    %     \end{minipage}
    %     \begin{minipage}{0.2\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/samples/8409_0.png}
    %         % \captionof{figure}{Figure 1 is a figure}
    %     \end{minipage}
    %     \begin{minipage}{0.2\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/samples/euclid_12516_0.png}
    %         % \captionof{figure}{Figure 1 is a figure}
    %     \end{minipage} \\
    %     \vspace{.2cm}
    % %   \caption{Another figure caption.}
    %     \centering
    %     \begin{minipage}{0.2\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/samples/original_1.png}
    %         \captionsetup{labelformat=empty}
    %         \captionof{figure}{Original Image}            % \captionof{figure}{Figure 1 is a figure}
    %         \addtocounter{figure}{-1}
    %     \end{minipage}
    %     \begin{minipage}{0.2\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/samples/322_0.png}
    %         \captionsetup{labelformat=empty}
    %         \captionof{figure}{Kernel Distance}            % \captionof{figure}{Figure 1 is a figure}
    %         \addtocounter{figure}{-1}
    %     \end{minipage}
    %     \begin{minipage}{0.2\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/samples/euclid_3233_1.png}
    %         % Pixel Distance
    %         \captionsetup{labelformat=empty}
    %         \captionof{figure}{Pixel Distance}
    %         \addtocounter{figure}{-1}
    %     \end{minipage}
    %   \caption{Comparison between the nearest samples in kernel space and pixel space. From left to right in each column: Test set point, nearest sample in kernel space, nearest sample in pixel space using euclidean distance.}
    %   \label{fig:near}
    % \end{figure}
    
    % \begin{figure}
    % \end{figure}

% \begin{figure*}[h]
% \centering
% \includegraphics[width=8cm]{c4_figures/image.png}
% \caption{This plot shows output of the ANN versus output of the corresponding kernel representation for a set of test images from the MNIST dataset. We note the very strong agreement between the two outputs.}  
% \label{fig:agree}
% \end{figure*}


\subsection{Kernel Analysis}
Having established the efficacy of this kernel for model representation, the next step is to analyze this kernel to understand how it may inform us about the properties of the corresponding model. In practice, it becomes immediately apparent that this kernel lacks typical properties preferred when humans select kernels. Fig.~\ref{fig:svm} show that the weights of this kernel are non-stationary on our toy problem, with very stable model predictions far away from training data. Next, we use this kernel to estimate uncertainty. Consistent with many other research works on Gaussian processes for classification ~\cite{rasmussen2006gaussian} we use a GP to regress to logits. We then use Monte-Carlo to estimate posteriors with respect to probabilities (post-soft-max) for each prediction across a grid spanning the training points of our toy problem. The result is shown on the right-hand column of Fig.~\ref{fig:cov}. We can see that the kernel values are more confident (lower standard deviation) and more stable (higher kernel values) the farther they get from the training data in most directions. 

% show kernel values or mean prediction with variances from GP
    \begin{figure}[h]
        \centering
        \includegraphics[width=0.24\textwidth]{c4_figures/kers_square.png}\includegraphics[width=0.24\textwidth]{c4_figures/vars_square.png}
        \caption{(left) Kernel values measured on a grid around the training set for our 2D problem. Bright yellow means high kernel value (right) Monte-Carlo estimated standard deviation based on gram matrices generated using our kernel for the same grid as the kernel values. Yellow means high standard deviation, blue means low standard deviation.}
        \label{fig:cov}
    \end{figure}

In order to further understand how these strange kernel properties come about, we exercise another advantage of a kernel by analyzing the points that are contributing to the kernel value for a variety of test points. 
In Fig.~\ref{fig:points} we examine the kernel values for each of the training points during evaluation of three points chosen as the mean of the generating distribution for each class. 
The most striking property of these kernel point values is the fact that they are not proportional to the euclidean distance from the test point.
This appears to indicate a set of basis vectors relative to each test point learned by the model based on the training data which are used to spatially transform the data in preparation for classification. This may relate to the correspondence between neural networks and maximum margin classifiers discussed in related work (~\cite{chizat2020maxmargin} ~\cite{shah2021input}). 
% In aggregate, the data are imposing a spatial transform on the test point and this transform is represented in the kernel weights by a smooth variation in the weights orthogonal to the basis function of this transform. 
% Our toy problem primarily varies in only 2 dimensions so these basis functions correspond with only normal vectors in 2 dimensions. 
Another more subtle property is that some individual data points, mostly close to decision boundaries are slightly over-weighted compared to the other points in their class. 
This latter property points to the fact that during the latter period of training, once the network has already achieved high accuracy, only the few points which continue to receive incorrect predictions, i.e. caught on the wrong side of a decision boundary, will continue contributing to the training gradient and therefore to the kernel value.

    \begin{figure*}[ht]
        \centering
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_00_sample_00.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_01_sample_00.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_02_sample_00.pdf}\\
        
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_00_sample_01.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_01_sample_01.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_02_sample_01.pdf}\\

        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_00_sample_02.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_01_sample_02.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_02_sample_02.pdf}
        % \includegraphics[width=0.40\textwidth]{c4_figures/test_ntk_example_4.pdf}
        % \includegraphics[width=0.40\textwidth]{c4_figures/test_epk_example_3.pdf}
        % \includegraphics[width=0.40\textwidth]{c4_figures/test_ntk_example_3.pdf}
        \caption{Plots showing kernel values for each training point relative to a test point. Because our kernel is replicating the output of a network, there are three kernel values per sample on a three class problem. This plot shows kernel values for all three classes across three different test points selected as the mean of the generating distribution. Figures on the diagonal show kernel values of the predicted class. Background shading is the neural network decision boundary.}
        \label{fig:points}
    \end{figure*}


% some more words

% \newpage

\subsection{Extending To Image Data}
    We perform experiments on MNIST to demonstrate the applicability to image data. 
    This kernel representation was generated for convolutional ReLU Network with the categorical cross-entropy loss function, using Pytorch ~\cite{pytorch}. 
    The model was trained using forward Euler (gradient descent) using gradients generated as a sum over all training data for each step. 
    The state of the model was saved for every training step. In order to compute the per-training-point gradients needed for the kernel representation, the per-input jacobians are computed at execution time in the representation by loading the model for each training step $i$, computing the jacobians for each training input to compute $\nabla_w f_{w_s(0)}(x_i)$, and then repeating this procedure for 200 $t$ values between 0 and 1 in order to approximate $\int_0^1 f_{w_s(t)}(x)$. For MNIST, the resulting prediction is very sensitive to the accuracy of this integral approximation, as shown in Fig.~\ref{fig:mnist}. The top plot shows approximation of the above integral with only one step, which corresponds to the DPK from previous work (~\cite{chen2021equivalence}, ~\cite{domingos2020}, ~\cite{incudini2022quantum}) and as we can see, careful approximation of this integral is necessary to achieve an accurate match between the model and kernel. 

\begin{figure}[!h]
    \centering
    % \vspace{-5mm}
    \includegraphics[width=0.95\linewidth]{c4_figures/mnist_model_kernel_compare_1_step.pdf}
    % \vspace{-18mm}

    \vspace{-9mm}
    
    \includegraphics[width=0.95\linewidth]{c4_figures/mnist_model_kernel_compare_10_steps.pdf}
    
    \vspace{-9mm}
    
    \includegraphics[width=0.95\linewidth]{c4_figures/mnist_model_kernel_compare_200_steps.pdf}
    \caption{Experiment demonstrating the relationship between model predictions and kernel predictions for varying precision of the integrated path kernel. The top figure shows the integral estimated using only a single step. This is equivalent to the discrete path kernel (DPK) of previous work ~\cite{domingos2020every, chen2021equivalence}. The middle figure shows the kernel evaluated using 10 integral steps. The final figure shows the path kernel evaluated using 200 integral steps.}
    \label{fig:mnist}
\end{figure}
%I will leave it to michael to add any out-of-sample plots and plots related to $a_{i,s}$ weights and such. 
\section{Conclusion and Outlook} % confusion and onset
The implications of a practical and finite kernel representation for the study of neural networks are profound and yet importantly limited by the networks that they are built from. For most gradient trained models, there is a disconnect between the input space (e.g. images) and the parameter space of a network. Parameters are intrinsically difficult to interpret and much work has been spent building approximate mappings that convert model understanding back into the input space in order to interpret features, sample importance, and other details ~\cite{simonyan2013deep, lundberg2017unified, Selvaraju_2019}. The EPK is composed of a direct mapping from the input space into parameter space. This mapping allows for a much deeper understanding of gradient trained models because the internal state of the method has an exact representation mapped from the input space. As we have shown in Fig.~\ref{fig:points}, kernel values derived from gradient methods tell an odd story. We have observed a kernel that picks inputs near decision boundaries to emphasize and derives a spatial transform whose basis vectors depend neither uniformly nor continuously on training points. Although kernel values are linked to sample importance, we have shown that most contributions to the kernel's prediction for a given point are measuring an overall change in the network's internal representation. This supports the notion that most of what a network is doing is fitting a spatial transform based on a wide aggregation of data, and only doing a trivial calculation to the data once this spatial transform has been determined ~\cite{chizat2020maxmargin}. 
As stated in previous work ~\cite{domingos2020}, this representation has strong implications about the structure of gradient trained models and how they can understand the problems that they solve. Since the kernel weights in this representation are fixed derivatives with respect to the loss function $L$, $a_{i, s} = -\varepsilon  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial f_i}$, nearly all of the information used by the network is represented by the kernel mapping function and inner product. Inner products are not just measures of distance, they also measure angle. In fact, figure \ref{fig:grad} shows that for a typical training example, the $L_2$ norm of the weights changes monotonically by only 20-30\% during training. This means that the "learning" of a gradient trained model is dominated by change in angle, which is predicted for kernel methods in high dimensions ~\cite{hardle2004nonparametric}.

\begin{figure}[h]
\centering
\includegraphics[width=.45\textwidth]{c4_figures/stab-n-201mnist-C32-100-100-10-0.001-0.0001-eval-mod_int_acc-trn.png}
\caption{This plot shows a linear interpolation $w(t) = w_0 + t(w_{1} - w_0)$ of model parameters $w$ for a convolutional neural network $f_w$ from their starting random state $w_0$ to their ending trained state $w_1$. The hatched purple line shows the dot product of the sum of the  gradient over the training data $X$, $\langle \nabla_w f_{w(t)}(X), (w_1 - w_0)/|w_1 - w_0| \rangle$. The other lines indicate accuracy (blue), total loss (red decreasing), and L2 Regularization (green increasing)}  
\label{fig:grad}
\end{figure}

% Perhaps the most significant advantage for gradient trained models of an exact kernel representation is that the combination of kernel and kernel weights provides a spatial representation of the model's understanding relative to the training data. In previous work (~\cite{gillette2022data} ~\cite{yousefzadeh2021deep} it has been shown that image classification can be represented by projection onto the convex hull of training data. This projection is computationally infeasible, but it provides a geometric gold-standard classifier in the native image space. Recent work ~\cite{chizat2020maxmargin} indicates that neural networks are in fact max margin classifiers in a metric space defined by their approximation of the wasserstein metric. Since kernel methods provide a spatial representation of their prediction which can be directly compared to the spatial classifier in image space, it can be used to analyze properties of the spatial transform that converts the computationally intractable convex hull in image space, to the computationally tractable approximated Wasserstein metric space. We can see the consequence of this in Fig.~\ref{fig:points}. 

For kernel methods, our result also represents a new direction. Despite their firm mathematical foundations, kernel methods have lost ground since the early 2000s because the features implicitly learned by deep neural networks yield better accuracy than any known hand-crafted kernels for complex high-dimensional problems ~\cite{NIPS2005_663772ea}. 
We're hopeful about the scalability of learned kernels based on recent results in scaling kernel methods  ~\cite{snelson2005sparse}. 
Exact kernel equivalence could allow the use of neural networks to implicitly construct a kernel. 
This could allow kernel based classifiers to approach the performance of neural networks on complex data. 
Kernels built in this way may be used with Gaussian processes to allow meaningful direct uncertainty measurement. 
This would allow for much more significant analysis for out-of-distribution samples including adversarial attacks ~\cite{szegedy2013intriguing, ilyas2019adversarial}. 
There is significant work to be done in improving the properties of the kernels learned by neural networks for these tools to be used in practice.
We are confident that this direct connection between practical neural networks and kernels is a strong first step towards achieving this goal.
% Although this does not provide a "free lunch" on its own, it may open the door for composite kernels based on neural network representation which can retain performance while gaining desirable properties like stationarity ~\cite{tancik2020fourierfeatures}. 

% <<< I would recommend ending here. The next paragraph is way too technical and seems more like ending in a wimper than a bang >>>

% Another implication from this representation is the increased importance of models following their gradient flow during training. Since this derived kernel is either discontinuous or asymmetric depending on the neural network's training trajectory, developing training restrictions which satisfy equation~\ref{eq:cond} may produce more useful kernels and have implications about the accuracy and generality of neural network models. This will provide a new motivation for such research separate from just the question of efficiency of training. Approaches in this direction may be found in control theory (~\cite{lin2020gradient}) and the neural ODE approach (~\cite{bilovs2021neural}, ~\cite{neuralode2018}).
%Also in this vein is the precise formulation of the divergence error from the discrete training path to the smooth gradient flow. Such a formulation would shed light on the dynamics of how such representations converge in performance under various step refinements.

% TODO final conclusion paragraph

% \section{Acknowledgements}

% This research was funded by Los Alamos National Lab LDRD-DR XX9C UQ4ML (help with how to acknowledge this LDRD funding juston?) Thanks to Yen Ting Lin, Philip Hoskins, Keenan Eikenberry, and Craig Thompson for feedback on early iterations of this paper. 

%\bibliography{bibfile}
%\bibliographystyle{icml2023}

\appendix

\onecolumn
\section{Appendix}

\subsection{The EPK is a Kernel}

\ker*
\begin{proof}
We must show that the associated kernel matrix $K_{\text{EPK}} \in \mathbb{R}^{n\times n}$ defined for an arbitrary subset of data $\{x_i\}_{i=1}^M \subset X$ as $K_{\text{EPK},i,j} = \int_0^1\langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt$ is both symmetric and positive semi-definite.

Since the inner product on a Hilbert space $\langle \cdot, \cdot \rangle$ is symmetric and since the same mapping $\varphi$ is used on the left and right, $K_{\text{EPK}}$ is \textbf{symmetric}. 

To see that $K_{\text{EPK}}$ is \textbf{Positive Semi-Definite}, let $\alpha = (\alpha_1, \alpha_2, \dots, \alpha_n)^\top \in \mathbb{R}^n$ be any vector. We need to show that $\alpha^\top K_{\text{EPK}} \alpha \geq 0$. We have

\begin{align}
\alpha^\top K_{\text{EPK}} \alpha &= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \int_0^1 \langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt \\
&= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \int_0^1 \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
&= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
&= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n  \langle \alpha_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, \alpha_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
&= \int_0^1    \langle \sum_{i=1}^n \alpha_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, \sum_{j=1}^n \alpha_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
& \text{Re-ordering the sums so that their indices match, we have}\\
&= \int_0^1 \left\lVert \sum_{i=1}^n \alpha_i \nabla_{w}\hat{y}_{w_s(t,x_i)}\right\rVert^2 dt \\
&\geq 0,
\end{align}

Note that this reordering does not depend on the continuity of our mapping function $\phi_{s,t}(x_i)$.

\end{proof}

\textbf{Remark \remlabel{rem2}} In the case that our mapping function $\varphi$ is not symmetric, after re-ordering, we still yield something of the form:
\begin{align}
&= \int_0^1 \left\lVert \sum_{i=1}^n \alpha_i \nabla_{w}\hat{y}_{w_s(t,x_i)}\right\rVert^2 dt \\
\end{align}
The natural asymmetric $\phi$ is symmetric for every non-training point, so we can partition this sum. For the non-training points, we have symmetry, so for those points we yield exactly the $L^2$ metric. For the remaining points, if we can pick a Lipschitz constant $E$ along the entire gradient field, then if training steps are enough, then the integral and the discrete step side of the asymmetric kernel will necessarily have positive inner product. In practice, this Lipschitz constant will change during training and for appropriately chosen step size (smaller early in training, larger later in training) we can guarantee positive-definiteness. In particular this only needs to be checked for training points. 

\subsection{The EPK gives an Exact Representation}
\label{proof:eker}
\eker*
\begin{proof}


Let $f_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed step size $\varepsilon$ on a training dataset $X$ with labels $ Y$, with initial parameters $w_0$ so that there is a constant $b$ such that for every $x$, $f_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x \in X$ be arbitrary and within the domain of $f_w$ for every $w$. For the final trained state of this model $f_{w_N}$, let $y = f_{w_N}(x)$. 

For one step of training, we consider $y_s  = f_{w_s(0)}(x)$ and $y_{s+1} = f_{w_{s+1}}(x)$. We wish to account for the change $y_{s+1} - y_s$ in terms of a gradient flow, so we must compute $\dfrac{\partial y}{dt}$ for a continuously varying parameter $t$. Since $f$ is trained using forward Euler with a step size of $\varepsilon > 0$, this derivative is determined by a step of fixed size of the weights $w_s$ to $w_{s+1}$. We parameterize this step in terms of the weights:

\begin{align}
    \dfrac{d w_s(t)}{dt} &= (w_{s+1} - w_s)\\   
    \int \dfrac{d w_s(t)}{dt} dt &= \int (w_{s+1} - w_s)dt\\
    w_s(t) &= w_s + t(w_{s+1} - w_s)\\
\end{align}
Since $f$ is being trained using forward Euler, across the entire training set $X$ we can write:
\begin{align}
    \dfrac{d w_s(t)}{dt} &= -\varepsilon \nabla_w L(f_{w_s(0)}(X), y_i) = -\varepsilon \sum_{j = 1}^{d} \sum_{i=1}^M  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial w_j} \label{eq10}
\end{align}
Applying chain rule and the above substitution, we can write
\begin{align}
    \dfrac{d \hat y}{dt} = \dfrac{d f_{w_s(t)}}{dt} &= \sum_{j = 1}^{d} \dfrac{d f}{\partial w_j} \dfrac{\partial w_j}{dt}\\
&= \sum_{j = 1}^{d} \dfrac{d f_{w_s(t)}(x)}{\partial w_j} \left(-\varepsilon \dfrac{\partial L(f_{w_s(0)}(X_T),  Y_T)}{\partial w_j}\right)\\
&= \sum_{j = 1}^{d} \dfrac{d f_{w_s(t)}(x)}{\partial w_j} \left(-\varepsilon \sum_{i = 1}^{M}\dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}\dfrac{\partial  f_{w_s(0)}(x_i)}{\partial w_j}\right)\\
&= -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)} \sum_{j = 1}^{d} \dfrac{d f_{w_s(t)}(x)}{\partial w_j}  \dfrac{d f_{w_s(0)}(x_i)}{\partial w_j}\\
&= -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)} \nabla_w f_{w_s(t)}(x) \cdot \nabla_w f_{w_s(0)}(x_i)\\
\end{align}
Using the fundamental theorem of calculus, we can compute the change in the model's output over step $s$
\begin{align}
    y_{s+1} - y_s &= \int_0^1 -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}  \nabla_w f_{w_s(t)}(x) \cdot \nabla_w f_{w_s(0)}(x_i)dt\\
 &=  -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}  \left(\int_0^1\nabla_w f_{w_s(t)}(x)dt\right) \cdot \nabla_w f_{w_s(0)}(x_i)\\
\end{align}
For all $N$ training steps, we have
\begin{align*}
y_N &= b + \sum_{s=1}^N y_{s+1} - y_s\\
y_N &= b + \sum_{s = 1}^N -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}  \left(\int_0^1\nabla_w f_{w_s(t)}(x)dt\right) \cdot \nabla_w f_{w_s(0)}(x_i)\\
% &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial f_i}  \left(\int_0^1\nabla_w f_{w_s(t)}(x)dt\right) \cdot \nabla_w f_{w_s(0)}(x_i)\\
% &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial f_i}  \int_0^1\left\langle \nabla_w f_{w_s(t)}(x), \nabla_w f_{w_s(0)}(x_i) \right\rangle dt\\ 
&= b + \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}  \int_0^1\left\langle \nabla_w f_{w_s(t,x)}(x), \nabla_w f_{w_s(t,x_i)}(x_i) \right\rangle dt\\ 
&= b + \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i, s}  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt
\end{align*}
Since an integral of a symmetric positive semi-definite function is still symmetric and positive-definite, each step is thus represented by a kernel machine. 

\end{proof}
\subsection{When is an Ensemble of Kernel Machines itself a Kernel Machine?}
\label{proof:ekmr}
Here we investigate when our derived ensemble of kernel machines composes to a single kernel machine.
In order to show that a linear combination of kernels also equates to a kernel it is sufficient to show that $a_{i,s} = a_{i,0}$ for all $a_{i,s}$.
The $a_{i}$ terms in our kernel machine are determined by the gradient the training loss function.
This statement then implies that the gradient of the loss term must be constant throughout training in order to form a kernel.
Here we show that this is the case when we consider a log softmax activation on the final layer and a negative log likelihood loss function.
% In this case it is possible to let the sample weights of our final kernel machine equal $a_{i,0}$.
% In order to show this, we impose some structure on the loss function and network.
% Here we show this is the case for binary crossentropy on a network with sigmoid activations on the logits.
% (TODO: More argument here using mercer's theorem. All positive linear combinations of kernels are kernels. There are cases where some negative coefficients are allowed but that's going to take a lot more thought. How do we extend this to say $aKa > 0$ for all $a$?)

\begin{proof}
Assume a two class problem. In the case of a function with multiple outputs, we consider each output to be a kernel. We define our network output $\hat y_i$ as all layers up to and including the log softmax and $y_i$ is a one-hot encoded vector. 

\begin{align}
    L(\hat y_i,  y_i)
    &=  \sum_{k=1}^K -y_i^k(\hat y_i^k) \\
    % \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= -y_i - (1-y_i)\\
    % &= -1
\end{align}
For a given output indexed by $k$, if $y_i^k = 1$ then we have
\begin{align}
    L(\hat y_i,  y_i)
    &=  -1(\hat y_i^k) \\
    \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= -1\\
\end{align}
If $y_i^k = 0$ then we have
\begin{align}
    L(\hat y_i,  y_i)
    &=  0(\hat y_i^k) \\
    \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= 0\\
\end{align}
% Here we can see that the gradient of the loss value is fixed for all weight states, therefore 

% For a binary classification problem it is standard to have $y_i \in \{0, 1\}$ and using a sigmoid activation on the final layer we have $f_i \in (0, 1)$. \\

% \begin{center}
    
% \begin{minipage}{0.45\textwidth}
% Assume $y_i = 0$.
% \begin{align}
%     \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= \dfrac{0 - \hat y_i}{(\hat y_i - 1) \hat y_i}\\
%     &= \dfrac{-1}{\hat y_i - 1}\\
%     &= \dfrac{1}{|\hat y_i - 1|}
% \end{align}
% The last equality relies on the fact that $\hat y_i < 1$.
% \begin{equation}
%     y_i = 0 \implies \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} > 0
% \end{equation}
% \end{minipage}
% \hspace{0.04\textwidth}
% \begin{minipage}{0.45\textwidth}
% Assume $y_i = 1$.
% \begin{align}
%     \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= \dfrac{1 - \hat y_i}{(\hat y_i - 1) \hat y_i}\\
%     &= \dfrac{1 - \hat y_i}{-(1-\hat y_i) \hat y_i}\\
%     &= -\dfrac{1}{\hat y_i}
% \end{align}
% Because $\hat y_i > 0$.
% \begin{equation}
%     y_i = 1 \implies \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} < 0
% \end{equation}
% \end{minipage}
% \end{center}

In this case, since the loss is scaled directly by the output, and the only other term is an indicator function deciding which class label to take, we get a constant gradient.
This shows that the gradient of the loss function does not depend on $\hat y_i$. 
Therefore:
\begin{align}
    y &= b - \varepsilon \sum_{i = 1}^{N}\sum_{s = 1}^S a_{i, s}  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt\\
     &= b - \varepsilon \sum_{i = 1}^{N} a_{i, 0} \sum_{s = 1}^S  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt
\end{align}
This formulates a kernel machine where
\begin{align}
a_{i, 0} &= \dfrac{\partial L(f_{w_0}(x_i),  y_i)}{\partial f_i} \\
K(x, x_i) &= \sum_{s = 1}^S \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt \\
\phi_{s,t}(x) &=  \nabla_w f_{w_s(t,x)} (x)\\
w_s(t,x) &= \begin{cases} w_s, x \in X_T\\ w_s(t), x \notin X_T \end{cases}\\
b &= 0
\end{align}
\end{proof}

It is important to note that this does not hold up if we consider the log softmax function to be part of the loss instead of the network.
In addition, there are loss structures which can not be rearranged to allow this property.
In the simple case of linear regression, we can not disentangle the loss gradients from the kernel formulation, preventing the construction of a valid kernel. 
For example assume our loss is instead squared error. Our labels are continuous on $\mathds{R}$ and our activation is the identity function.
\begin{align}
    L(f_i,  y_i) 
    &= (y_i - f_{i, s})^2 \\
    \dfrac{\partial L(f_i,  y_i)}{\partial f_i} &= 2(y_i- f_{i, s})
\end{align}

This quantity is dependent on $f_i$ and its value is changing throughout training. %(TODO: Make this more formal and rigorous)

In order for 
\begin{align}
    \sum_{s=1}^S a_{i,s} \int_0^1 \langle \phi_{s,t}(x), \phi_{s,t}(x_i)\rangle dt
\end{align}
to be a kernel on its own, we need it to be a positive (or negative) definite operator and symetric. In the specific case of our practical path kernel, i.e. that in $K(x,x')$ if $x'$ happens to be equal to $x_i$, then positive semi-definiteness can be accounted for:
\begin{align}
    &= \sum_{s=1}^S 2(y_i- f_{i, s}) \int_0^1 \langle \phi_{s,t}(x), \phi_{s,t}(x_i)\rangle dt\\
    &= \sum_{s=1}^S 2(y_i- f_{i, s}) \int_0^1 \langle \nabla_w f_{w_s(t))} (x), \nabla_w f_{w_s(0)} (x_i)\rangle dt\\
    &= \sum_{s=1}^S 2 \left(y_i \cdot \int_0^1 \langle \nabla_w f_{w_s(t))} (x), \nabla_w f_{w_s(0)} (x_i)\rangle dt - f_{i, s} \int_0^1 \langle \nabla_w f_{w_s(t))} (x), \nabla_w f_{w_s(0)} (x_i)\rangle dt \right)\\
    &= \sum_{s=1}^S 2 \left(y_i \cdot \int_0^1 \langle \nabla_w f_{w_s(t))} (x), \nabla_w f_{w_s(0)} (x_i)\rangle dt -  \int_0^1 \langle \nabla_w f_{w_s(t))} (x), f_{i, s} \nabla_w f_{w_s(0)} (x_i)\rangle dt \right)\\
    &= \sum_{s=1}^S 2 \left(y_i \cdot \int_0^1 \langle \nabla_w f_{w_s(t))} (x), \nabla_w f_{w_s(0)} (x_i)\rangle dt -  \int_0^1 \langle \nabla_w f_{w_s(t))} (x), \dfrac{1}{2}\nabla_w (f_{w_s(0)} (x_i))^2\rangle dt \right)\\
\end{align}
Otherwise, we get the usual 
\begin{align}
        &= \sum_{s=1}^S 2(y_i- f_{i, s}) \int_0^1 \langle \nabla_w f_{w_s(t,x))} (x), \nabla_w f_{w_s(t,x)} (x')\rangle dt\\
\end{align}
The question is two fold. One, in general theory (i.e. the lower example), can we contrive two pairs $(x_1,x'_1)$ and $(x_2,x'_2)$ that don't necessarily need to be training or test images for which this sum is positive for $1$ and negative for $2$. Second, in the case that we are always comparing against training images, do we get something more predictable since there is greater dependence on $x_i$ and we get the above way of re-writing  using the gradient of the square of $f(x_i)$. 

However, even accounting for this by removing the sign of the loss will still produce a non-symmetric function. This limitation is more difficult to overcome.

\subsection{Multi-Class Case}

There are two ways of treating our loss function $L$ for a number of classes (or number of output activations) $K$:
\begin{align}
    \text{Case 1: } L &: \mathbb{R}^K \to \mathbb{R}\\
    \text{Case 2: } L &: \mathbb{R}^K \to \mathbb{R}^K\\
\end{align}

\subsubsection{Case 1 Scalar Loss}

Let $L : \mathbb{R}^K \to \mathbb{R}$. We use the chain rule $D (g \circ f) (x) = Dg(f(x))Df(x)$. 

Let $f$ be a vector valued function so that $f : \mathbb{R}^D \to \mathbb{R}^K$  satisfying the conditions from [representation theorem above] with $x \in \mathbb{R}^D$ and $y_i \in \mathbb{R}^K$ for every $i$. We note that $\dfrac{\partial f}{\partial t}$ is a column and has shape $Kx1$ and our first chain rule can be done the old fashioned way on each row of that column:
\begin{align}
    \dfrac{d f}{d t} &= \sum_{j=1}^M \dfrac{d f(x)}{\partial w_j} \dfrac{d w_j}{d t}\\
    &= -\varepsilon \sum_{j=1}^M \dfrac{d f(x)}{\partial w_j} \sum_{i=1}^N \dfrac{\partial L(f(x_i), y_i)}{\partial w_j}\\
    &\text{Apply chain rule}\\
    &= -\varepsilon \sum_{j=1}^M \dfrac{d f(x)}{\partial w_j} \sum_{i=1}^N \dfrac{\partial L(f(x_i), y_i)}{\partial f}\dfrac{d f(x_i)}{\partial w_j}\\
    &\text{Let}\\
    A &= \dfrac{d f(x)}{\partial w_j} \in \mathbb{R}^{K \times 1}\\
    B &= \dfrac{d L(f(x_i), y_i)}{d f} \in \mathbb{R}^{1 \times K}\\
    C &= \dfrac{d f(x_i)}{\partial w_j} \in \mathbb{R}^{K \times 1}
\end{align}
We have a matrix multiplication $ABC$ and we wish to swap the order so somehow we can pull $B$ out, leaving $A$ and $C$ to compose our product for the representation. Since $BC \in \mathbb{R}$, we have $(BC) = (BC)^T$ and we can write
\begin{align}
    (ABC)^T &= (BC)^TA^T = BCA^T\\
    ABC &= (BCA^T)^T
\end{align}
Note: This condition needs to be checked carefully for other formulations so that we can re-order the product as follows:
\begin{align}
        &= -\varepsilon \sum_{j=1}^M  \sum_{i=1}^N \left(\dfrac{d L(f(x_i), y_i)}{d f} 
        \dfrac{d f(x_i)}{\partial w_j} \left(\dfrac{d f(x)}{\partial  w_j}\right)^T\right)^T
        \\
    &= -\varepsilon \sum_{i=1}^N \left(\dfrac{d L(f(x_i), y_i)}{d f} 
    \sum_{j=1}^M \dfrac{d f(x_i)}{\partial w_j} \left(\dfrac{d f(x)}{\partial w_j}\right)^T\right)^T\\        
\end{align}
Note, now that we are summing over $j$, so we can write this as an inner product on $j$ with the $\nabla$ operator which in this case is computing the jacobian of $f$ along the dimensions of class (index k) and weight (index j). We can define 
\begin{align}
    (\nabla f(x))_{k,j} &= \dfrac{d f_{k}(x)}{\partial w_j}\\
    &= -\varepsilon \sum_{i=1}^N \left(\dfrac{d L(f(x_i), y_i)}{d f} 
     \nabla f(x_i) (\nabla f(x))^T\right)^T\\    
\end{align}
We note that the dimensions of each of these matrices in order are $[1,K]$, $[K,M]$, and $[M,K]$ which will yield a matrix of dimension $[1, K]$ i.e. a row vector which we then transpose to get back a column of shape $[K, 1]$. Also, we note that our kernel inner product now has shape $[K,K]$. 

\subsection{Schemes Other than Forward Euler (SGD)}

\textbf{Variable Step Size:}
Suppose $f$ is being trained using Variable step sizes so that across the training set $X$:
\begin{align}
    \dfrac{d w_s(t)}{dt} &= -\varepsilon_s \nabla_w L(f_{w_s(0)}(X), y_i) = -\varepsilon \sum_{j = 1}^{d} \sum_{i=1}^M  \dfrac{\partial L(f_{w_s(0)}(X),  y_i)}{\partial w_j} \label{eq10}
\end{align}
This additional dependence of $\varepsilon$ on $s$ simply forces us to keep $\varepsilon$ inside the summation in equation~\ref{eq11}. 

\textbf{Other  Numerical Schemes:} Suppose $f$ is being trained using another numerical scheme so that:
\begin{align}
    \dfrac{d w_s(t)}{dt} &= \varepsilon_{s,l} \nabla_w L(f_{w_s(0)}(x_i), y_i) + \varepsilon_{s-1, l}\nabla_w L(f_{w_{s-1}}(x_i), y_i) + \cdots \\
    &= \varepsilon_{s,l} \sum_{j = 1}^{d} \sum_{i=1}^M  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial w_j} + \varepsilon_{s-1, l} \sum_{j = 1}^{d} \sum_{i=1}^M  \dfrac{\partial L(f_{w_{s-1}(0)}(x_i),  y_i)}{\partial w_j} + \cdots
\end{align}
This additional dependence of $\varepsilon$ on $s$ and $l$ simply results in an additional summation in equation~\ref{eq11}. Since addition commutes through kernels, this allows separation into a separate kernel for each step contribution. Leapfrog and other first order schemes will fit this category. 

\textbf{Higher Order Schemes:} Luckily these are intractable for for most machine-learning models because they would require introducing dependence of the kernel on input data or require drastic changes. It is an open but intractable problem to derive kernels corresponding to higher order methods. 


\subsection{Variance Estimation}
In order to estimate variance we treat our derived kernel function $K$ as the covariance function for Gaussian process regression. Given training data $X$ and test data $X'$, we can use the Kriging to write the mean prediction and it variance from true $\mu(x)$ as
\begin{align}
    \bar \mu(X') &= \left[ K(X', X)\right] \left[ K(X, X) \right]^{-1} \left[ Y \right]\\
    \text{Var}(\bar \mu(X') - \mu(X')) &= \left[K(X', X') \right] - \left[ K(X', X)\right] \left[ K(X, X) \right]^{-1} \left[ K(X, X') \right]\\
\end{align}
Where 
\begin{align}
    \left[K(A, B) \right] &= \begin{bmatrix} K(A_1, B_1) & K(A_1, B_2) & \cdots \\
                                             K(A_2, B_1) & K(A_2, B_2) &  \\
                                             \vdots &  & \ddots \\ \end{bmatrix}
\end{align}
To finish our Gaussian estimation, we note that each $K(A_i, B_i)$ will me a $k$ by $k$ matrix where $k$ is the number of classes. We take $\text{tr}(K(A_i, B_i)$ to determine total variance for each prediction. 

% \subsection{The Hunt for Uniqueness}
% @juston Long story summarized. We're going to divide the weights into $w = w_H | w_F$. Our goal was to hold $w_F = 0$ and move $w_H$ around until $\nabla w_F$ satisfied $(1-\nabla w_F) (w_F^T - w_F^0) = 0$ AND $(1-\nabla w_H) (w_H^T - w_H) = 0$. Since we have two minimization constraints we might have just taken the gradient of that (albeit requiring a hessian) and been done. 
% @juston Problem is that when $w_F^0 = 0$, $\nabla w_H = 0$. This means that by definition of $w_F^0 = 0$, there is no solution which satisfies both of the above constraints. 
% Geyer found this out when he got the optimization set up with those constraints and discovered the zero gradient on $w_H$ whenever $w_F = 0$. 
% We determined that you can add an intermediate step, now indices are going to get a bit wild, so we will use $w_{F, i}^j$ where $F$ tells us it's the final layer, $i$ tells us we're in the first of the two steps in weight space, and $j$ tells us we are in the $j$th step of the iterative optimization. 
% Let $w_{H, 0}^0 = W_{H}^T$, i.e. let the hidden weights all be equal to their fully trained state. 
% Let $w_{F, 0}^0 = 0$. 
% Now we want to update from $w_{H, 0}^0$ to $w_{H, 0}^1$ in such a way that $(1-\nabla_{L} w_{1}^1)(w^T - w_1^1)$ is minimized. 
% This means we want to compute the corresponding gradient on the 0th step to accomplish this minimization. We can define our iteration as follows:  
% $w_{H,0}^{s+1} = w_{H,0}^s - \nabla_{w_{H}}(1-\nabla_{L} w_{1}^0)(w^T - w_1^0)$. We can rewrite this iteration: $w_{H,0}^{s+1} = w_{H,0}^s + \nabla_{w_{H}}(\nabla_{L} (w_{1}^0) w^T) +\nabla_{w_{H}}(w_1^0)- \nabla_{w_{H}}(\nabla_{L}( w_{1}^0) w_1^0))$

% \subsection{Multi Class Case}


% In the case where $\hat y$ is a vector we denote the data index by the superscript $y^{[i]}$ and the vector component by the subscript $y_k$.

% query point may need to know about other class gradients than the target class. 

% k by k

% yen ting's approach is needed because we are making some assumption about the model that allows us to measure the query change along a linear path. 

% The negative log likelyhood function where $y$ and $\hat y$ are vectors.
% \begin{align}
%     NLL(y, \hat y) = \sum_k^{K} -y_k \ln(\hat y_k)
% \end{align}

% \begin{align}
%     \dfrac{\partial \left[ \hat y_0 ... y_k \right]}{dt} &= \sum_{j=1}^M \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \dfrac{\partial w_j}{dt}\\
%     &= \sum_{j=1}^M \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \left( -\varepsilon \sum_{i = 1}^N \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial w_j} \right)\\
%     &= \sum_{j=1}^M \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \left( -\varepsilon \sum_{i = 1}^N \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]} \dfrac{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]}{\partial w_j} \right)\\
%     &=  -\varepsilon \sum_{i = 1}^N \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]} \sum_{j=1}^M  \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \dfrac{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]}{\partial w_j} 
% \end{align}

% Now we use the fact that L is CCE
% \begin{align}
%     \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial y^{[i]}_k} &= \begin{cases} 0, k \not = y^{[i]}\\ -1 \end{cases}
% \end{align}

% \begin{align}
%     \dfrac{\partial \hat y_k}{dt} &=  -\varepsilon \sum_{i = 1}^N 1 \sum_{j=1}^M  \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \dfrac{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]}{\partial w_j} 
% \end{align}

% \begin{align}
%     y_0 +  \left[ \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial y^{[i]}_0} \sum_{j=1}^M \dfrac{\partial f(x_i)_0}{\partial w_j} \cdot\dfrac{\partial f(x)_0}{\partial w_j}, \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial y^{[i]}_1} \sum_{j=1}^M \dfrac{\partial f(x_i)_1}{\partial w_j} \cdot\dfrac{\partial f(x)_1}{\partial w_j} \right]
% \end{align}

% \section{Electronic Submission}
% \label{submission}

% Submission to ICML 2023 will be entirely electronic, via a web site
% (not email). Information about the submission process and \LaTeX\ templates
% are available on the conference web site at:
% \begin{center}
% \textbf{\texttt{http://icml.cc/}}
% \end{center}

% The guidelines below will be enforced for initial submissions and
% camera-ready copies. Here is a brief summary:
% \begin{itemize}
% \item Submissions must be in PDF\@. 
% \item \textbf{New to this year}: If your paper has appendices, submit the appendix together with the main body and the references \textbf{as a single file}. Reviewers will not look for appendices as a separate PDF file. So if you submit such an extra file, reviewers will very likely miss it.
% \item Page limit: The main body of the paper has to be fitted to 8 pages, excluding references and appendices; the space for the latter two is not limited. For the final version of the paper, authors can add one extra page to the main body.
% \item \textbf{Do not include author information or acknowledgements} in your
%     initial submission.
% \item Your paper should be in \textbf{10 point Times font}.
% \item Make sure your PDF file only uses Type-1 fonts.
% \item Place figure captions \emph{under} the figure (and omit titles from inside
%     the graphic file itself). Place table captions \emph{over} the table.
% \item References must include page numbers whenever possible and be as complete
%     as possible. Place multiple citations in chronological order.
% \item Do not alter the style template; in particular, do not compress the paper
%     format by reducing the vertical spaces.
% \item Keep your abstract brief and self-contained, one paragraph and roughly
%     4--6 sentences. Gross violations will require correction at the
%     camera-ready phase. The title should have content words capitalized.
% \end{itemize}

% \subsection{Submitting Papers}

% \textbf{Paper Deadline:} The deadline for paper submission that is
% advertised on the conference website is strict. If your full,
% anonymized, submission does not reach us on time, it will not be
% considered for publication. 

% \textbf{Anonymous Submission:} ICML uses double-blind review: no identifying
% author information may appear on the title page or in the paper
% itself. \cref{author info} gives further details.

% \textbf{Simultaneous Submission:} ICML will not accept any paper which,
% at the time of submission, is under review for another conference or
% has already been published. This policy also applies to papers that
% overlap substantially in technical content with conference papers
% under review or previously published. ICML submissions must not be
% submitted to other conferences and journals during ICML's review
% period.
% %Authors may submit to ICML substantially different versions of journal papers
% %that are currently under review by the journal, but not yet accepted
% %at the time of submission.
% Informal publications, such as technical
% reports or papers in workshop proceedings which do not appear in
% print, do not fall under these restrictions.

% \medskip

% Authors must provide their manuscripts in \textbf{PDF} format.
% Furthermore, please make sure that files contain only embedded Type-1 fonts
% (e.g.,~using the program \texttt{pdffonts} in linux or using
% File/DocumentProperties/Fonts in Acrobat). Other fonts (like Type-3)
% might come from graphics files imported into the document.

% Authors using \textbf{Word} must convert their document to PDF\@. Most
% of the latest versions of Word have the facility to do this
% automatically. Submissions will not be accepted in Word format or any
% format other than PDF\@. Really. We're not joking. Don't send Word.

% Those who use \textbf{\LaTeX} should avoid including Type-3 fonts.
% Those using \texttt{latex} and \texttt{dvips} may need the following
% two commands:

% {\footnotesize
% \begin{verbatim}
% dvips -Ppdf -tletter -G0 -o paper.ps paper.dvi
% ps2pdf paper.ps
% \end{verbatim}}
% It is a zero following the ``-G'', which tells dvips to use
% the config.pdf file. Newer \TeX\ distributions don't always need this
% option.

% Using \texttt{pdflatex} rather than \texttt{latex}, often gives better
% results. This program avoids the Type-3 font problem, and supports more
% advanced features in the \texttt{microtype} package.

% \textbf{Graphics files} should be a reasonable size, and included from
% an appropriate format. Use vector formats (.eps/.pdf) for plots,
% lossless bitmap formats (.png) for raster graphics with sharp lines, and
% jpeg for photo-like images.

% The style file uses the \texttt{hyperref} package to make clickable
% links in documents. If this causes problems for you, add
% \texttt{nohyperref} as one of the options to the \texttt{icml2023}
% usepackage statement.


% \subsection{Submitting Final Camera-Ready Copy}

% The final versions of papers accepted for publication should follow the
% same format and naming convention as initial submissions, except that
% author information (names and affiliations) should be given. See
% \cref{final author} for formatting instructions.

% The footnote, ``Preliminary work. Under review by the International
% Conference on Machine Learning (ICML). Do not distribute.'' must be
% modified to ``\textit{Proceedings of the
% $\mathit{40}^{th}$ International Conference on Machine Learning},
% Honolulu, Hawaii, USA, PMLR 202, 2023.
% Copyright 2023 by the author(s).''

% For those using the \textbf{\LaTeX} style file, this change (and others) is
% handled automatically by simply changing
% $\mathtt{\backslash usepackage\{icml2023\}}$ to
% $$\mathtt{\backslash usepackage[accepted]\{icml2023\}}$$
% Authors using \textbf{Word} must edit the
% footnote on the first page of the document themselves.

% Camera-ready copies should have the title of the paper as running head
% on each page except the first one. The running title consists of a
% single line centered above a horizontal rule which is $1$~point thick.
% The running head should be centered, bold and in $9$~point type. The
% rule should be $10$~points above the main text. For those using the
% \textbf{\LaTeX} style file, the original title is automatically set as running
% head using the \texttt{fancyhdr} package which is included in the ICML
% 2023 style file package. In case that the original title exceeds the
% size restrictions, a shorter form can be supplied by using

% \verb|\icmltitlerunning{...}|

% just before $\mathtt{\backslash begin\{document\}}$.
% Authors using \textbf{Word} must edit the header of the document themselves.

% \section{Format of the Paper}

% All submissions must follow the specified format.

% \subsection{Dimensions}




% The text of the paper should be formatted in two columns, with an
% overall width of 6.75~inches, height of 9.0~inches, and 0.25~inches
% between the columns. The left margin should be 0.75~inches and the top
% margin 1.0~inch (2.54~cm). The right and bottom margins will depend on
% whether you print on US letter or A4 paper, but all final versions
% must be produced for US letter size.
% Do not write anything on the margins.

% The paper body should be set in 10~point type with a vertical spacing
% of 11~points. Please use Times typeface throughout the text.

% \subsection{Title}

% The paper title should be set in 14~point bold type and centered
% between two horizontal rules that are 1~point thick, with 1.0~inch
% between the top rule and the top edge of the page. Capitalize the
% first letter of content words and put the rest of the title in lower
% case.

% \subsection{Author Information for Submission}
% \label{author info}

% ICML uses double-blind review, so author information must not appear. If
% you are using \LaTeX\/ and the \texttt{icml2023.sty} file, use
% \verb+\icmlauthor{...}+ to specify authors and \verb+\icmlaffiliation{...}+ to specify affiliations. (Read the TeX code used to produce this document for an example usage.) The author information
% will not be printed unless \texttt{accepted} is passed as an argument to the
% style file.
% Submissions that include the author information will not
% be reviewed.

% \subsubsection{Self-Citations}

% If you are citing published papers for which you are an author, refer
% to yourself in the third person. In particular, do not use phrases
% that reveal your identity (e.g., ``in previous work \cite{langley00}, we
% have shown \ldots'').

% Do not anonymize citations in the reference section. The only exception are manuscripts that are
% not yet published (e.g., under submission). If you choose to refer to
% such unpublished manuscripts \cite{anonymous}, anonymized copies have
% to be submitted
% as Supplementary Material via CMT\@. However, keep in mind that an ICML
% paper should be self contained and should contain sufficient detail
% for the reviewers to evaluate the work. In particular, reviewers are
% not required to look at the Supplementary Material when writing their
% review (they are not required to look at more than the first $8$ pages of the submitted document).

% \subsubsection{Camera-Ready Author Information}
% \label{final author}

% If a paper is accepted, a final camera-ready copy must be prepared.
% %
% For camera-ready papers, author information should start 0.3~inches below the
% bottom rule surrounding the title. The authors' names should appear in 10~point
% bold type, in a row, separated by white space, and centered. Author names should
% not be broken across lines. Unbolded superscripted numbers, starting 1, should
% be used to refer to affiliations.

% Affiliations should be numbered in the order of appearance. A single footnote
% block of text should be used to list all the affiliations. (Academic
% affiliations should list Department, University, City, State/Region, Country.
% Similarly for industrial affiliations.)

% Each distinct affiliations should be listed once. If an author has multiple
% affiliations, multiple superscripts should be placed after the name, separated
% by thin spaces. If the authors would like to highlight equal contribution by
% multiple first authors, those authors should have an asterisk placed after their
% name in superscript, and the term ``\textsuperscript{*}Equal contribution"
% should be placed in the footnote block ahead of the list of affiliations. A
% list of corresponding authors and their emails (in the format Full Name
% \textless{}email@domain.com\textgreater{}) can follow the list of affiliations.
% Ideally only one or two names should be listed.

% A sample file with author names is included in the ICML2023 style file
% package. Turn on the \texttt{[accepted]} option to the stylefile to
% see the names rendered. All of the guidelines above are implemented
% by the \LaTeX\ style file.

% \subsection{Abstract}

% The paper abstract should begin in the left column, 0.4~inches below the final
% address. The heading `Abstract' should be centered, bold, and in 11~point type.
% The abstract body should use 10~point type, with a vertical spacing of
% 11~points, and should be indented 0.25~inches more than normal on left-hand and
% right-hand margins. Insert 0.4~inches of blank space after the body. Keep your
% abstract brief and self-contained, limiting it to one paragraph and roughly 4--6
% sentences. Gross violations will require correction at the camera-ready phase.

% \subsection{Partitioning the Text}

% You should organize your paper into sections and paragraphs to help
% readers place a structure on the material and understand its
% contributions.

% \subsubsection{Sections and Subsections}

% Section headings should be numbered, flush left, and set in 11~pt bold
% type with the content words capitalized. Leave 0.25~inches of space
% before the heading and 0.15~inches after the heading.

% Similarly, subsection headings should be numbered, flush left, and set
% in 10~pt bold type with the content words capitalized. Leave
% 0.2~inches of space before the heading and 0.13~inches afterward.

% Finally, subsubsection headings should be numbered, flush left, and
% set in 10~pt small caps with the content words capitalized. Leave
% 0.18~inches of space before the heading and 0.1~inches after the
% heading.

% Please use no more than three levels of headings.

% \subsubsection{Paragraphs and Footnotes}

% Within each section or subsection, you should further partition the
% paper into paragraphs. Do not indent the first line of a given
% paragraph, but insert a blank line between succeeding ones.

% You can use footnotes\footnote{Footnotes
% should be complete sentences.} to provide readers with additional
% information about a topic without interrupting the flow of the paper.
% Indicate footnotes with a number in the text where the point is most
% relevant. Place the footnote in 9~point type at the bottom of the
% column in which it appears. Precede the first footnote in a column
% with a horizontal rule of 0.8~inches.\footnote{Multiple footnotes can
% appear in each column, in the same order as they appear in the text,
% but spread them across columns and pages if possible.}

% \begin{figure}[ht]
% \vskip 0.2in
% \begin{center}
% \centerline{\includegraphics[width=\columnwidth]{icml_numpapers}}
% \caption{Historical locations and number of accepted papers for International
% Machine Learning Conferences (ICML 1993 -- ICML 2008) and International
% Workshops on Machine Learning (ML 1988 -- ML 1992). At the time this figure was
% produced, the number of accepted papers for ICML 2008 was unknown and instead
% estimated.}
% \label{icml-historical}
% \end{center}
% \vskip -0.2in
% \end{figure}

% \subsection{Figures}

% You may want to include figures in the paper to illustrate
% your approach and results. Such artwork should be centered,
% legible, and separated from the text. Lines should be dark and at
% least 0.5~points thick for purposes of reproduction, and text should
% not appear on a gray background.

% Label all distinct components of each figure. If the figure takes the
% form of a graph, then give a name for each axis and include a legend
% that briefly describes each curve. Do not include a title inside the
% figure; instead, the caption should serve this function.

% Number figures sequentially, placing the figure number and caption
% \emph{after} the graphics, with at least 0.1~inches of space before
% the caption and 0.1~inches after it, as in
% \cref{icml-historical}. The figure caption should be set in
% 9~point type and centered unless it runs two or more lines, in which
% case it should be flush left. You may float figures to the top or
% bottom of a column, and you may set wide figures across both columns
% (use the environment \texttt{figure*} in \LaTeX). Always place
% two-column figures at the top or bottom of the page.

% \subsection{Algorithms}

% If you are using \LaTeX, please use the ``algorithm'' and ``algorithmic''
% environments to format pseudocode. These require
% the corresponding stylefiles, algorithm.sty and
% algorithmic.sty, which are supplied with this package.
% %\cref{alg:example} shows an example.

% % \begin{algorithm}[tb]
% %   \caption{Bubble Sort}
% %   \label{alg:example}
% % \begin{algorithmic}
% %   \STATE {\bfseries Input:} data $x_i$, size $m$
% %   \REPEAT
% %   \STATE Initialize $noChange = true$.
% %   \FOR{$i=1$ {\bfseries to} $m-1$}
% %   \IF{$x_i > x_{i+1}$}
% %   \STATE Swap $x_i$ and $x_{i+1}$
% %   \STATE $noChange = false$
% %   \ENDIF
% %   \ENDFOR
% %   \UNTIL{$noChange$ is $true$}
% % \end{algorithmic}
% % \end{algorithm}

% \subsection{Tables}

% You may also want to include tables that summarize material. Like
% figures, these should be centered, legible, and numbered consecutively.
% However, place the title \emph{above} the table with at least
% 0.1~inches of space before the title and the same after it, as in
% \cref{sample-table}. The table title should be set in 9~point
% type and centered unless it runs two or more lines, in which case it
% should be flush left.

% % Note use of \abovespace and \belowspace to get reasonable spacing
% % above and below tabular lines.

% \begin{table}[t]
% \caption{Classification accuracies for naive Bayes and flexible
% Bayes on various data sets.}
% \label{sample-table}
% \vskip 0.15in
% \begin{center}
% \begin{small}
% \begin{sc}
% \begin{tabular}{lcccr}
% \toprule
% Data set & Naive & Flexible & Better? \\
% \midrule
% Breast    & 95.9$\pm$ 0.2& 96.7$\pm$ 0.2& $\surd$ \\
% Cleveland & 83.3$\pm$ 0.6& 80.0$\pm$ 0.6& $\times$\\
% Glass2    & 61.9$\pm$ 1.4& 83.8$\pm$ 0.7& $\surd$ \\
% Credit    & 74.8$\pm$ 0.5& 78.3$\pm$ 0.6&         \\
% Horse     & 73.3$\pm$ 0.9& 69.7$\pm$ 1.0& $\times$\\
% Meta      & 67.1$\pm$ 0.6& 76.5$\pm$ 0.5& $\surd$ \\
% Pima      & 75.1$\pm$ 0.6& 73.9$\pm$ 0.5&         \\
% Vehicle   & 44.9$\pm$ 0.6& 61.5$\pm$ 0.4& $\surd$ \\
% \bottomrule
% \end{tabular}
% \end{sc}
% \end{small}
% \end{center}
% \vskip -0.1in
% \end{table}

% Tables contain textual material, whereas figures contain graphical material.
% Specify the contents of each row and column in the table's topmost
% row. Again, you may float tables to a column's top or bottom, and set
% wide tables across both columns. Place two-column tables at the
% top or bottom of the page.

% \subsection{Theorems and such}
% The preferred way is to number definitions, propositions, lemmas, etc. consecutively, within sections, as shown below.
% \begin{definition}
% \label{def:inj}
% A function $f:X \to Y$ is injective if for any $x,y\in X$ different, $f(x)\ne f(y)$.
% \end{definition}
% Using \cref{def:inj} we immediate get the following result:
% \begin{proposition}
% If $f$ is injective mapping a set $X$ to another set $Y$, 
% the cardinality of $Y$ is at least as large as that of $X$
% \end{proposition}
% \begin{proof} 
% Left as an exercise to the reader. 
% \end{proof}
% \cref{lem:usefullemma} stated next will prove to be useful.
% \begin{lemma}
% \label{lem:usefullemma}
% For any $f:X \to Y$ and $g:Y\to Z$ injective functions, $f \circ g$ is injective.
% \end{lemma}
% \begin{theorem}
% \label{thm:bigtheorem}
% If $f:X\to Y$ is bijective, the cardinality of $X$ and $Y$ are the same.
% \end{theorem}
% An easy corollary of \cref{thm:bigtheorem} is the following:
% \begin{corollary}
% If $f:X\to Y$ is bijective, 
% the cardinality of $X$ is at least as large as that of $Y$.
% \end{corollary}
% \begin{assumption}
% The set $X$ is finite.
% \label{ass:xfinite}
% \end{assumption}
% \begin{remark}
% According to some, it is only the finite case (cf. \cref{ass:xfinite}) that is interesting.
% \end{remark}
% %restatable

% \subsection{Citations and References}

% Please use APA reference format regardless of your formatter
% or word processor. If you rely on the \LaTeX\/ bibliographic
% facility, use \texttt{natbib.sty} and \texttt{icml2023.bst}
% included in the style-file package to obtain this format.

% Citations within the text should include the authors' last names and
% year. If the authors' names are included in the sentence, place only
% the year in parentheses, for example when referencing Arthur Samuel's
% pioneering work \yrcite{Samuel59}. Otherwise place the entire
% reference in parentheses with the authors and year separated by a
% comma \cite{Samuel59}. List multiple references separated by
% semicolons \cite{kearns89,Samuel59,mitchell80}. Use the `et~al.'
% construct only for citations with three or more authors or after
% listing all authors to a publication in an earlier reference \cite{MachineLearningI}.

% Authors should cite their own work in the third person
% in the initial version of their paper submitted for blind review.
% Please refer to \cref{author info} for detailed instructions on how to
% cite your own papers.

% Use an unnumbered first-level section heading for the references, and use a
% hanging indent style, with the first line of the reference flush against the
% left margin and subsequent lines indented by 10 points. The references at the
% end of this document give examples for journal articles \cite{Samuel59},
% conference publications \cite{langley00}, book chapters \cite{Newell81}, books
% \cite{DudaHart2nd}, edited volumes \cite{MachineLearningI}, technical reports
% \cite{mitchell80}, and dissertations \cite{kearns89}.

% Alphabetize references by the surnames of the first authors, with
% single author entries preceding multiple author entries. Order
% references for the same authors by year of publication, with the
% earliest first. Make sure that each reference includes all relevant
% information (e.g., page numbers).

% Please put some effort into making references complete, presentable, and
% consistent, e.g. use the actual current name of authors.
% If using bibtex, please protect capital letters of names and
% abbreviations in titles, for example, use \{B\}ayesian or \{L\}ipschitz
% in your .bib file.

% \section*{Accessibility}
% Authors are kindly asked to make their submissions as accessible as possible for everyone including people with disabilities and sensory or neurological differences.
% Tips of how to achieve this and what to pay attention to will be provided on the conference website \url{http://icml.cc/}.

% \section*{Software and Data}

% If a paper is accepted, we strongly encourage the publication of software and data with the
% camera-ready version of the paper whenever appropriate. This can be
% done by including a URL in the camera-ready copy. However, \textbf{do not}
% include URLs that reveal your institution or identity in your
% submission for review. Instead, provide an anonymous URL or upload
% the material as ``Supplementary Material'' into the CMT reviewing
% system. Note that reviewers are not required to look at this material
% when writing their review.

% Acknowledgements should only appear in the accepted version.
\section*{Acknowledgements}

This material is based upon work supported by the Department of Energy (National Nuclear Security Administration Minority Serving Institution Partnership Program's CONNECT - the COnsortium on Nuclear sECurity Technologies) DE-NA0004107.
This report was prepared as an account of work sponsored by an agency of the United States Government.
Neither the United States Government nor any agency thereof, nor any of their employees, makes any warranty, express or implied, or assumes any legal liability or responsibility for the accuracy, completeness, or usefulness of any information, apparatus, product, or process disclosed, or represents that its use would not infringe privately owned rights. The views and opinions of authors expressed herein do not necessarily state or reflect those of the United States Government or any agency thereof.

This material is based upon work supported by the National Science Foundation under Grant No. 2134237. We would like to additionally acknowledge funding from NSF TRIPODS Award Number 1740858 and NSF RTG Applied Mathematics and Statistics for Data-Driven Discovery Award Number 1937229. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
% \textbf{Do not} include acknowledgements in the initial version of
% the paper submitted for blind review.

% If a paper is accepted, the final camera-ready version can (and
% probably should) include acknowledgements. In this case, please
% place such acknowledgements in an unnumbered section at the
% end of the paper. Typically, this will include thanks to reviewers
% who gave useful comments, to colleagues who contributed to the ideas,
% and to funding agencies and corporate sponsors that provided financial
% support.


% % In the unusual situation where you want a paper to appear in the
% % references without citing it in the main text, use \nocite
% \nocite{langley00}




% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% % APPENDIX
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \newpage
% \appendix
% \onecolumn
% \section{You \emph{can} have an appendix here.}

% You can have as much text here as you want. The main body must be at most $8$ pages long.
% For the final version, one more page can be added.
% If you want, you can use an appendix like this one, even using the one-column format.
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was created
% by Iain Murray in 2018, and modified by Alexandre Bouchard in
% 2019 and 2021 and by Csaba Szepesvari, Gang Niu and Sivan Sabato in 2022.
% Modified again in 2023 by Sivan Sabato and Jonathan Scarlett.
% Previous contributors include Dan Roy, Lise Getoor and Tobias
% Scheffer, which was slightly modified from the 2010 version by
% Thorsten Joachims & Johannes Fuernkranz, slightly modified from the
% 2009 version by Kiri Wagstaff and Sam Roweis's 2008 version, which is
% slightly modified from Prasad Tadepalli's 2007 version which is a
% lightly changed version of the previous year's version by Andrew
% Moore, which was in turn edited from those of Kristian Kersting and
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.


% \chapter{Model Geometry}

% \label{Chapter3} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{Chapter3}

% \section{Neural Networks are (Mostly) Kernel Machines}

% \begin{abstract}
% In this work, we write an exact representation for an arbitrary gradient trained model as a kernel based method extending the theory of the path kernel described in ~\cite{domingos2020} and discussed by \cite{chen2021equivalence}. We discuss the conditions under which this representation is exact, measure approximation error, and compare to the well known Neural Tangent Kernel (NTK) ~\cite{jacot2018neural}. We implement this representation for an artificial neural network and demonstrate that it is computationally tractable and accurate in practice. Using this kernel, we quantify uncertainty according to this kernel using Gaussian process regression and discuss the implicit limitations that this reveals about neural networks. In particular, we show that the kernel resulting from a typical neural network is non-stationary and has highly unusual spatial properties. 
% \end{abstract}

% %\tableofcontents
% %\flushleft

% \section{Introduction}
% TODO rewrite to focus on the seconod paper. 

% This study investigates the relationship between kernel methods and models whose parameters are determined using gradient methods. The neural tangent kernel (NTK) is a well established method which represents training gradients as a kernel method \cite{jacot2018neural} and compares trained models with this kernel in the case of models as their number of parameters approaches infinity. A theory of the neural path kernel (NPK) has been proposed which integrates finite tangent kernels \cite{domingos2020every}. The path kernel extends from the NTK by directly constructing a kernel method approximation of a model by integrating tangent kernels along the continuous gradient flow defined by the model's gradient with respect to its training data and loss function. The case is made that this continuous path kernel is an exact representation of a continuous gradient trained model and that any practical model trained using discrete steps according to gradient descent is  therefore approximately a kernel method. This opens up such models, including artificial neural networks (ANNs), to many theoretical tools available to kernel methods (\cite{ghojogh2021, shawe2004kernel, zhao2005extracting}). However usage of such tools is highly dependent on the accuracy and dynamics of this approximation. Furthermore, the smooth path kernel requires difficult measurements of convergence and error in order to make practical comparisons with real discretely trained models. Although the convergence argued for likely holds as training step size converges to zero, these arguments do not help us understand the dynamics of this convergence and are very difficult to compute in practice. 

% In this work we propose an exact kernel representation for any gradient trained model satisfying very few conditions and demonstrate that this method is computable, practical, and exposes convergence and approximation properties to rigorous analysis. Our results demonstrate the potential for using the path kernel to study deep neural networks and provide a foundation for further research in this area. Furthermore, we will discuss relaxations of the required conditions on this kernel that allows very general functions to be represented with a small measurable approximation that can be bounded in practice. In addition, we study methods for approximating our discrete path kernel to reduce computation costs while maintaining many of these useful properties.

% \section{Related Work}

% \cite{he2020bayesian} NTK % TODO

% \section{Discrete Path Kernels}

% Models trained by gradient descent can be characterized by a discrete set of intermediate states in the space of their parameters. These states are not bound to the gradient flow defined for such models, so we must consider how to integrate a discrete path for weights whose states differ from the gradient flow. In order to write this representation we must carefully define both kernel methods and kernels:

% \begin{definition}
% A {kernel} is a function of two variables which is symmetric and positive definite. 
% \end{definition}

% \begin{definition}
% Given a Hilbert space $X$, a query point $x \in X$, and a training set $X_T \subset X$, a \emph{Kernel Method} is a model characterized by 
% \begin{align}
%     \hat y(x) = b + \sum_{i} a_i k(x,x_i)
% \end{align}
% where the $a_i \in \mathbb{R}$ do not depend on $x$, $b \in \mathbb{R}$ is a constant, and $k$ is a kernel. 
% \end{definition}

% By Mercer's theorem ~\cite{ghojogh2021} a kernel can be produced by composing an inner product on a Hilbert space with a mapping $\phi$ from the space of data into the chosen Hilbert space. We will first derive a kernel which is an exact representation of the change in model output over one training step, and then compose our final representation by summing along the finitely many steps. 

% \begin{definition}
% Let $ y_{w}$ be a differentiable function parameterized by $w \in \mathbb{R}^d$ which is trained via $N$ forward Euler steps of fixed size $\varepsilon$ on a finite subset $X_T = \{x_i\}_{i=1}^M$ of a Hilbert space $X$ of size $M$ with labels $Y_T = \{y_i\}_{i=1}^M$, with initial parameters $w_0$ so that there is a constant $b \in \mathbb{R}$ such that for all $x$, $ \hat y_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x \in X$ be arbitrary and within the domain of $\hat y_w$ for every $w$. Then the \emph{discrete path kernel} (DPK) can be written  
% \begin{equation}
%  K_{\text{DPK}}(x, x') = \int_0^1\langle \phi_{s,t}(x), \phi_{s,t}(x')\rangle dt
% \end{equation}
% where
% \begin{align}
% a_{i, s} &= -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i} \in \mathbb{R} \\
% \phi_{s,t}(x) &=  \nabla_w \hat y_{w_s(t,x)} (x)\\
% w_s(t,x) &= \begin{cases} w_s, x \in X_T\\ w_s(t), x \notin X_T \end{cases}
% \end{align}
% \end{definition}
% \begin{restatable}{lemma}{ker}
% The discrete path kernel (DPK) is a kernel.
% \end{restatable}
% % \begin{proof}
% % We must show that the associated kernel matrix $K_{\text{DPK}} \in \mathbb{R}^{n\times n}$ defined for an arbitrary subset of data $\{x_i\}_{i=1}^M \subset X$ as $K_{\text{DPK},i,j} = \int_0^1\langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt$ is both symmetric and positive semi-definite.

% % Since the inner product on a Hilbert space $\langle \cdot, \cdot \rangle$ is symmetric and since the same mapping $\varphi$ is used on the left and right, $K_{\text{DPK}}$ is \textbf{symmetric}. 

% % To see that $K_{\text{DPK}}$ is \textbf{Positive Semi-Definite}, let $f = (f_1, f_2, \dots, f_n)^\top \in \mathbb{R}^n$ be any vector. We need to show that $f^\top K_{\text{DPK}} f \geq 0$. We have

% % \begin{align*}
% % f^\top K_{\text{DPK}} f &= \sum_{i=1}^n \sum_{j=1}^n f_i f_j \int_0^1 \langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt \\
% % &= \sum_{i=1}^n \sum_{j=1}^n f_i f_j \int_0^1 \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% % &= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n f_i f_j \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% % &= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n  \langle f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, f_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% % &= \int_0^1    \langle \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, \sum_{j=1}^n f_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% % & \text{Re-ordering the sums so that their indices match, we have}\\
% % &= \int_0^1 \left\lVert \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}\right\rVert^2 dt \\
% % &\geq 0,
% % \end{align*}

% % We note that this reordering does not depend on the continuity of our mapping function $\phi_{s,t}(x_i)$. 
% % \end{proof}
% \begin{restatable}[Exact Kernel Representation]{theorem}{ekr}
% \label{thm:ekr}
% A model $\hat y_{w_N}$ trained using discrete steps matching the conditions of the discrete path kernel has the following exact kernel method representation:
% \begin{equation}
% \hat y_{w_N}(x) = b + \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i,s} K_{\text{DPK}}(x, x')
% \label{exact}
% \end{equation}
% \end{restatable}
% % \begin{tikzpicture}
% % \node[\text{[insert graphic of forward euler with vectors]}
% % \end{tikzpicture}


% % \begin{theorem}
% % Let $ y_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed size $\varepsilon$ on a finite subset $X_T = \{x_i\}_{i=1}^M$ of a Hilbert space $X$ of size $M$ with labels $Y_T = \{y_i\}_{i=1}^M$, with initial parameters $w_0$ so that there is a constant $b \in \mathbb{R}$ such that $\forall x$, $ y_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x$ be an arbitrary point in the domain of $\hat y_w$ for every $w$. Then $\hat y_{w_N}$ (the final trained state of the model) has the following exact representation: 
% % \begin{equation}
% % \hat y_{w_N}(x) = b + \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i,s} \int_0^1\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\rangle dt
% % \end{equation}
% % where
% % \begin{align}
% % a_{i, s} &= -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i} \in \mathbb{R} \\
% % \phi_{s,t}(x) &=  \nabla_w \hat y_{w_s(t,x)} (x)\\
% % w_s(t,x) &= \begin{cases} w_s, x \in X_T\\ w_s(t), x \notin X_T \end{cases}
% % \end{align}
% % Which is to say that all such models are Kernel Methods. 
% % \end{theorem}

% % \begin{proof}
% % Let $\hat y_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed step size $\varepsilon$ on a training dataset $X$ with labels $ Y$, with initial parameters $w_0$ so that there is a constant $b$ such that $\forall x$, $\hat y_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x$ be an arbitrary point in the domain of $\hat y_w$ for every $w$. For the final trained state of this model $\hat y_{w_N}$, let $y = \hat y_{w_N}(x)$. 

% % For one step of training, we consider $y_s  = \hat y_{w_s}(x)$ and $y_{s+1} = \hat y_{w_{s+1}}(x)$. We wish to account for the change $y_{s+1} - y_s$ in terms of a gradient flow, so we must compute $\dfrac{\partial \hat y}{dt}$ for a continuously varying parameter $t$. Since $f$ is trained using forward Euler with a step size of $\varepsilon > 0$, this derivative is determined by a step of fixed size of the weights $w_s$ to $w_{s+1}$. We will parameterize this step in terms of the weights:

% % \begin{align}
% %     \dfrac{\partial w_s(t)}{dt} &= (w_{s+1} - w_s)\\   
% %     \int \dfrac{\partial w_s(t)}{dt} dt &= \int (w_{s+1} - w_s)dt\\
% %     w_s(t) &= w_s + t(w_{s+1} - w_s)\\
% % \end{align}
% % Since $f$ is being trained using forward Euler, we can write:
% % \begin{align}
% %     \dfrac{\partial w_s(t)}{dt} &= -\varepsilon \nabla_w L(\hat y_{w_s}(x_i), y_i) = -\varepsilon \sum_{j = 1}^{d} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial w_j} \label{eq10}
% % \end{align}
% % Applying chain rule and the above substitution, we can write
% % \begin{align}
% %     \dfrac{\partial \hat y}{dt} = \dfrac{d \hat y_{w_s(t)}}{dt} &= \sum_{j = 1}^{d} \dfrac{\partial \hat y}{\partial w_j} \dfrac{\partial w_j}{dt}\\
% % &= \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j} \left(-\varepsilon \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial w_j}\right)\\
% % &= \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j} \left(-\varepsilon \sum_{i = 1}^{M}\dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}\dfrac{\partial \hat y_{w_s}(x_i)}{\partial w_j}\right)\\
% % &= -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i} \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j}  \dfrac{\partial \hat y_{w_s}(x_i)}{\partial w_j}\\
% % &= -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(_i),  y_i)}{\partial \hat y_i}  \nabla_w \hat y_{w_s(t)}(x) \cdot \nabla_w \hat y_{w_s}(x_i)\\
% % \end{align}
% % Using the fundamental theorem of calculus, we can compute the change in the model's output over step $s$
% % \begin{align}
% %     y_{s+1} - y_s &= \int_0^1 -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \nabla_w \hat y_{w_s(t)}(x) \cdot \nabla_w \hat y_{w_s}(x_i)dt\\
% %  &=  -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s}(x_i)\\
% % \end{align}
% % For all $N$ training steps, we have
% % \begin{align}
% % y_N &= b + \sum_{s=1}^N y_{s+1} - y_s\\
% % y_N &= \sum_{s = 1}^N -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s}(x_i)\\
% % % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s}(x_i)\\
% % % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t)}(x), \nabla_w \hat y_{w_s}(x_i) \right\rangle dt\\ 
% % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t,x)}(x), \nabla_w \hat y_{w_s(t,x_i)}(x_i) \right\rangle dt\\ 
% % &= \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i, s}  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt
% % \end{align}
% % Since an integral of a symmetric positive semi-definite function is still symmetric and positive-definie and likewise for discrete sums, this represention is a kernel method. 

% % \end{proof}

% \textbf{Remark 0} We can see that by changing equation ~\ref{exact} we can produce an exact representation for any discrete optimization scheme that can be written in terms of model gradients. This could include backward Euler, leapfrog, higher order schemes (which are generally intractable for Artificial Neural Networks), and any variation of adaptive step sizes. 

% \textbf{Remark 1} \label{rem:asym}$\phi_{s,t}(x)$ depends on both $s$ and $t$, which is non-standard but valid, however an important consequence of this mapping is that the output of this representation is not guaranteed to be continuous. This discontinuity is exactly measuring the error between the model along the discrete path compared with the gradient flow for each step. We can write another function $k'$ which is continuous but not symmetric, but still produces an exact representation:
% \begin{align}
% k'(x, x') = \langle \nabla_w \hat y_{w_s(t)}(x), \nabla_w \hat y_{w_s(0)}(x')\rangle
% \end{align}
% The resulting function is a valid kernel if and only if for every $s$ and every $x$, 
% \begin{align}
% \label{eq:cond}
%     \int_0^1 \nabla_w \hat y_{w_s(t)}(x)dt = \nabla_w \hat y_{w_s(0)}(x)
% \end{align}
% The asymmetry of this function is exactly measuring the disagreement between the discrete steps taken during training with the gradient field defined by the loss function composed with the model. This function is one of several subjects for further study, particularly in the context of gaussian processes whereby the asymmetric matrix corresponding with this function can stand in for a covariance matrix. It may be that gaussian thermostats can be used to repair they asymmetry while accounting for any loss accrued due to disagreement between the discrete steps and the gradient flow or it may be that the not-symmetric analogue of the covariance in this case has physical meaning relative to uncertainty. 

% \textbf{Remark 2} In order to obtain an exact representation, one must start with a model that has constant output for all input, i.e. for every $x$ and $\hat y_0(x) = b$ (e.g. an ANN with all weights in the final layer initialized to 0). When relaxing this property, to allow for models that have a non-constant starting output, we note that this representation ceases to be exact. The resulting approximate representation will still agree strongly with the ANN, and will converge quickly in output as the ratio of the length of the training path divided by the step size goes to infinity. In fact this convergence is very rapid and useful approximation will be achieved within typical training length. (TODO : lipshitz argument)

% \textbf{Remark 3} We note that since $f$ is being trained using forward Euler, we can write:
% \begin{align}
%     \dfrac{\partial w_s(t)}{dt} &= -\varepsilon \nabla_w L(\hat y_{w_s}(x_i), y_i) = -\varepsilon \sum_{j = 1}^{d} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial w_j} \label{eq10}
% \end{align}
% In other words, our parameterize of this step depends on the step size $\varepsilon$ and as $\varepsilon \to 0$, we have 
% \begin{align}
%     \int_0^1 \nabla_w \hat y_{w_{s, \varepsilon}(t)}(x)dt \to \nabla_w \hat y_{w_s(0)}(x)
% \end{align}
% In particular, given a model $\hat y$ that admits a Lipshitz constant $K$ this approximation has error bounded by $\varepsilon K$ and a proof of this convergence is direct. 

% \section{Experimental Results}

%     Our first experiments test the kernel formulation on a dataset 
%     which can be visualized in 2d. These experiments serve as a sanity check
%     and provide an interpretable representation of what the kernel is learning.

%     \begin{figure}
%         \centering
%         \begin{minipage}{0.45\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/sample_dataset.pdf}
%         \end{minipage}
%         \begin{minipage}{0.45\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/model_kernel_predictions.pdf}
%         \end{minipage}
%         \caption{On the left is a 2d dataset of points sampled from gaussians with different means. Specifically, class A is normally distributed with $\mu = \left[1, 4\right]$ and $\sigma^2 = 1$ while class B is $\mu = \left[4, 1\right]$ and $\sigma^2 = 1$. 2000 data points were sampled for each class. These values were chosen arbitrarily to provide separation with a limited amount of overlap. On the right is the prediction similarity between the kernel and the original model. This demonstrates that our kernel formulation accurately represents the trained network.}
%         \label{fig:sample_data}
%     \end{figure}

% \subsection{Evaluating The Kernel}

%     Examples of the kernel values across 4 test points are shown in Figure \ref{fig:kernel}.
%     We are interested in how the kernel is learning and whether this kernel will allow out-of-distribution (OOD) detection.

%     \begin{figure}[!htb]
%         \centering
%         \begin{minipage}{0.45\textwidth}
%             \centering
%             % \begin{wrapfigure}{l}
%             \includegraphics[width=0.95\linewidth]{c3_figures/in_distribution_uncertan.pdf}
%             % \caption{In Distribution}
%             % \end{wrapfigure}
%             % \captionof{figure}{Figure 1 is a figure}
%         \end{minipage}
%             \begin{minipage}{0.45\textwidth}
%                 \centering
%                 \includegraphics[width=0.95\linewidth]{c3_figures/ood_positive_2.pdf}
%                 % \captionof{figure}{Figure 1 is a figure}
%             \end{minipage}
%     %   \caption{Another figure caption.}
%     % \end{figure}

%     % \begin{figure}
%         \centering
%         \begin{minipage}{0.45\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/in_distribution.pdf}
%             \captionsetup{labelformat=empty}
%             \captionof{figure}{In-Distribution}
%             \addtocounter{figure}{-1}
%             \end{minipage}
%             \begin{minipage}{0.45\textwidth}
%                 \centering
%                 \includegraphics[width=0.95\linewidth]{c3_figures/ood.pdf}
%             \captionsetup{labelformat=empty}
%             \captionof{figure}{Out-Of-Distribution}
%             \addtocounter{figure}{-1}
%             \end{minipage}
%         \caption{Example of the kernel values on in-distribution and out-of-distribution (OOD) data. Left column shows samples which are in-distribution for our dataset. Right column row shows OOD samples.}
%         \label{fig:kernel}
%     \end{figure}
%     From these plots we see that the in-distribution samples have a significantly higher sum over kernel distances than the OOD examples. Of note is that the OOD detection is not perfect. For the test point $\left(4.0, 15.0\right)$ it still identifies a large portion of class A samples as being relatively close in kernel space. Despite this, both OOD examples shown are significantly lower in total kernel distance than the in-distribution samples. Further experiments will be required to better understand why some OOD regions are closer than others

% \subsection{Extending To Image Data}


%     We perform experiments on MNIST to demonstrate the applicability to image data. 
%     This kernel representation was generated for a two-layer fully connected ReLU Network with the cross-entropy loss-function, using Pytorch (citation). The model was trained using forward Euler (gradient descent) using gradients generated as a sum over all training data for each step. The state of the model was saved for every training step. In order to compute the per-training-point gradients needed for the kernel representation, the per-input jacobians are computed at execution time in the representation by loading the model for each training step $i$ , computing the jacobians for each training input to compute $\nabla_w \hat y_{w_s(0)}(x_i)$, and then repeating this procedure for 100 $t$ values between 0 and 1 in order to approximate $\int_0^1 \hat y_{w_s(t)}(x)$. Torch is not currently optimized to provide per-input jacobians or to provide jacobians for multiple weight states, so this procedure can be accelerated greatly by deeper integration with existing pytorch tools. 
    
%     We are able to see the agreement between the neural network and its kernel representation in  figure ~\ref{fig:agree}. In figure ~\ref{fig:near} we see that the function learned by the kernel does not directly mimic euclidean distance in the image space. Samples which are nearby in kernel space are not necessarily nearby in pixel space. The similarity metric learned is a direct explanation of how the neural network is making decisions.

%         \begin{figure}
%         \centering
%         \begin{minipage}{0.2\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/samples/original_0.png}
%             % \captionof{figure}{Figure 1 is a figure}
%         \end{minipage}
%         \begin{minipage}{0.2\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/samples/8409_0.png}
%             % \captionof{figure}{Figure 1 is a figure}
%         \end{minipage}
%         \begin{minipage}{0.2\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/samples/euclid_12516_0.png}
%             % \captionof{figure}{Figure 1 is a figure}
%         \end{minipage} \\
%         \vspace{.2cm}
%     %   \caption{Another figure caption.}
%         \centering
%         \begin{minipage}{0.2\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/samples/original_1.png}
%             \captionsetup{labelformat=empty}
%             \captionof{figure}{Original Image}            % \captionof{figure}{Figure 1 is a figure}
%             \addtocounter{figure}{-1}
%         \end{minipage}
%         \begin{minipage}{0.2\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/samples/322_0.png}
%             \captionsetup{labelformat=empty}
%             \captionof{figure}{Kernel Distance}            % \captionof{figure}{Figure 1 is a figure}
%             \addtocounter{figure}{-1}
%         \end{minipage}
%         \begin{minipage}{0.2\textwidth}
%             \centering
%             \includegraphics[width=0.95\linewidth]{c3_figures/samples/euclid_3233_1.png}
%             % Pixel Distance
%             \captionsetup{labelformat=empty}
%             \captionof{figure}{Pixel Distance}
%             \addtocounter{figure}{-1}
%         \end{minipage}
%       \caption{Comparison between the nearest samples in kernel space and pixel space. From left to right in each column: Test set point, nearest sample in kernel space, nearest sample in pixel space using euclidean distance.}
%       \label{fig:near}
%     \end{figure}
    
%     % \begin{figure}
%     % \end{figure}

% \begin{figure}[!h]
% \centering
% \includegraphics[width=8cm]{c3_figures/image.png}
% \caption{This plot shows output of the ANN versus output of the corresponding kernel representation for a set of test images from the MNIST dataset. We note the very strong agreement between the two outputs.}  
% \label{fig:agree}
% \end{figure}



% %I will leave it to michael to add any out-of-sample plots and plots related to $a_{i,s}$ weights and such. 

% \section{Discussion}

% The implications of a practical and finite kernel representation for neural networks are wide and profound. For most gradient trained models, there is a disconnect between the problem space (e.g. images) and the parameter space of a network. Parameters are intrinsically un-interpretable and much work has been spent building approximate mappings that convert model understanding back into the problem space in order to interpret features, sample importance, and other details (~\cite{simonyan2013deep}, ~\cite{lundberg2017unified}, and ~\cite{Selvaraju_2019}). A kernel is composed of a direct mapping from the problem space into parameter space. This mapping allows much deeper understanding of gradient trained models because the internal state of the method has an exact representation mapped from the problem space. Sample importance is produced directly by looking at the kernel and its corresponding weights per training input. 

% As stated in previous work ~\cite{domingos2020}, this representation has strong implications about the structure of gradient trained models and how they can understand the problems that they solve. Since the kernel weights in this representation are fixed derivatives with respect to the loss function $L$, $a_{i, s} = -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}$ nearly all of the information used by the network is represented by the kernel mapping function and inner product. Inner products are not just measures of distance, they also measure angle. In fact, figure \ref{fig:grad} shows that for a typical training example, the $L_2$ norm of the weights changes monotonically by only 20-30\% during training. This means that the "learning" of a gradient trained model is dominated by change in angle, which is predicted for kernel methods in high dimensions ~\cite{hardle2004nonparametric}.

% \begin{figure}[!h]
% \centering
% \includegraphics[width=12cm]{c3_figures/stab-n-201mnist-C32-100-100-10-0.001-0.0001-eval-mod_int_acc-trn.png}
% \caption{This plot shows a linear interpolation $w(t) = w_0 + t(w_{1} - w_0)$ of model parameters $w$ for a convolutional neural network $\hat y_w$ from their starting random state $w_0$ to their ending trained state $w_1$. The hatched blue line shows the dot product of the aggregated gradient over the training data $X$, $\langle \nabla_w \hat y_{w(t)}(X), (w_1 - w_0)/|w_1 - w_0| \rangle$. The other lines indicate accuracy (blue), total loss (red decreasing), and L2 Regularization (red increasing)}  
% \label{fig:grad}
% \end{figure}

% Perhaps the most significant advantage for gradient trained models of an exact kernel representation is that the combination of kernel and kernel weights provides a spatial representation of the model's understanding relative to the training data. In previous work (~\cite{gillette2022data} ~\cite{yousefzadeh2021deep} it has been shown that image classification can be represented by projection onto the convex hull of training data. This projection is computationally infeasible, but it provides a geometric gold-standard classifier. Since kernel methods provide a spatial representation of their prediction, this representation can be directly compared with convex hull projections. It also provides data through which we can infer the gradient model's understanding of the data spatially. 

% For kernel methods, this also represents a significant step. Despite their firm mathematical foundations, kernel methods have lost ground since the early 2000s due to the limitations of developing new kernels for complex high-dimensional problems ~\cite{NIPS2005_663772ea}. This opens up many modern problems to the powerful tools available to kernel methods. Of these, Gaussian Processes (GPs) may be the most exciting. Given our kernel function, we can generate covariance matrices for GP which will allow direct uncertainty measurement. This will allow much more significant analysis for out-of-distribution samples including adversarial attacks (~\cite{szegedy2013intriguing} ~\cite{ilyas2019adversarial}). 

% Gaussian processes are one of many research directions that naturally follow this work. It is worth noting that since the kernel from our representation can be either continuous or symmetric but not both (See remark ~\ref{rem:asym}), covariance matrices used in GPs will have slightly unusual properties that will reflect the divergence of the discrete training path from the smooth gradient flow. In the case of the asymmetric version of our representation, this asymmetry measures this divergence in a way that may be tractably explored using gaussian thermostats (~\cite{scherer2020kernel}, ~\cite{nose1990constant}). In addition to exploring GPs, it is natural to pursue more efficient computation of these representations by exploiting features of pytorch and incorporating the necessary integral computation with some codes that have been developed for NeuralODEs which require similar information (~\cite{bilovs2021neural}, ~\cite{neuralode2018})

% Another implication from this representation is the increased importance of models following their gradient flow during training. Since this derived kernel is either discontinuous or asymmetric depending on the neural network's training trajectory, developing training restrictions which satisfy equation ~\ref{eq:cond} may produce more useful kernels and have implications about the accuracy and generalizability of ANN models. This will provide a new motivation for such research separate from just the question of efficiency of training. Approaches in this direction may be found in control theory (~\cite{lin2020gradient}) and the neural ODE approach (~\cite{bilovs2021neural}, ~\cite{neuralode2018}). Also in this vein is the precise formulation of the divergence error from the discrete training path to the smooth gradient flow. Such a formulation should shed light on the dynamics of how such representations converge in performance under various step refinements. 

% \section{Acknowledgements}

% This research was funded by Los Alamos National Lab LDRD-DR XX9C UQ4ML (help with how to acknowledge this LDRD funding juston?) Thanks to Yen Ting Lin, Philip Hoskins, Keenan Eikenberry, and Craig Thompson for feedback on early iterations of this paper. 


% % \appendix
% % \section{Appendix}

% ex
% \subsection{The DPK is a Kernel}

% \ker*
% \begin{proof}
% We must show that the associated kernel matrix $K_{\text{DPK}} \in \mathbb{R}^{n\times n}$ defined for an arbitrary subset of data $\{x_i\}_{i=1}^M \subset X$ as $K_{\text{DPK},i,j} = \int_0^1\langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt$ is both symmetric and positive semi-definite.

% Since the inner product on a Hilbert space $\langle \cdot, \cdot \rangle$ is symmetric and since the same mapping $\varphi$ is used on the left and right, $K_{\text{DPK}}$ is \textbf{symmetric}. 

% To see that $K_{\text{DPK}}$ is \textbf{Positive Semi-Definite}, let $f = (f_1, f_2, \dots, f_n)^\top \in \mathbb{R}^n$ be any vector. We need to show that $f^\top K_{\text{DPK}} f \geq 0$. We have

% \begin{align}
% f^\top K_{\text{DPK}} f &= \sum_{i=1}^n \sum_{j=1}^n f_i f_j \int_0^1 \langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt \\
% &= \sum_{i=1}^n \sum_{j=1}^n f_i f_j \int_0^1 \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n f_i f_j \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n  \langle f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, f_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1    \langle \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, \sum_{j=1}^n f_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% & \text{Re-ordering the sums so that their indices match, we have}\\
% &= \int_0^1 \left\lVert \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}\right\rVert^2 dt \\
% &\geq 0,
% \end{align}

% Note that this reordering does not depend on the continuity of our mapping function $\phi_{s,t}(x_i)$.

% \end{proof}

% \textbf{Remark} In the case that our mapping function $\varphi$ is not symmetric, after re-ordering, we still yield something of the form:
% \begin{align}
% &= \int_0^1 \left\lVert \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}\right\rVert^2 dt \\
% \end{align}
% The natural asymmetric $\varphi$ is symmetric for every non-training point, so we can partition this sum. For the non-training points, we have symmetry, so for those points we yield exactly the $L^2$ metric. For the remaining points, if we can pick a Lipschitz constant $E$ along the entire gradient field, then if training steps are enough, then the integral and the discrete step side of the asymmetric kernel will necessarily have positive inner product. In practice, this Lipschitz constant will change during training and for appropriately chosen step size (smaller early in training, larger later in training) we can guarantee positive-definiteness. In particular this only needs to be checked for training points. 

% \subsection{The DPK is an Exact Representation}

% \ekr*
% \begin{proof}

% Let $\hat y_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed step size $\varepsilon$ on a training dataset $X$ with labels $ Y$, with initial parameters $w_0$ so that there is a constant $b$ such that for every $x$, $\hat y_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x \in X$ be arbitrary and within the domain of $\hat y_w$ for every $w$. For the final trained state of this model $\hat y_{w_N}$, let $y = \hat y_{w_N}(x)$. 

% For one step of training, we consider $y_s  = \hat y_{w_s}(x)$ and $y_{s+1} = \hat y_{w_{s+1}}(x)$. We wish to account for the change $y_{s+1} - y_s$ in terms of a gradient flow, so we must compute $\dfrac{\partial \hat y}{dt}$ for a continuously varying parameter $t$. Since $f$ is trained using forward Euler with a step size of $\varepsilon > 0$, this derivative is determined by a step of fixed size of the weights $w_s$ to $w_{s+1}$. We will parameterize this step in terms of the weights:

% \begin{align}
%     \dfrac{\partial w_s(t)}{dt} &= (w_{s+1} - w_s)\\   
%     \int \dfrac{\partial w_s(t)}{dt} dt &= \int (w_{s+1} - w_s)dt\\
%     w_s(t) &= w_s + t(w_{s+1} - w_s)\\
% \end{align}
% Since $f$ is being trained using forward Euler, we can write:
% \begin{align}
%     \dfrac{\partial w_s(t)}{dt} &= -\varepsilon \nabla_w L(\hat y_{w_s}(x_i), y_i) = -\varepsilon \sum_{j = 1}^{d} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial w_j} \label{eq10}
% \end{align}
% Applying chain rule and the above substitution, we can write
% \begin{align}
%     \dfrac{\partial \hat y}{dt} = \dfrac{d \hat y_{w_s(t)}}{dt} &= \sum_{j = 1}^{d} \dfrac{\partial \hat y}{\partial w_j} \dfrac{\partial w_j}{dt}\\
% &= \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x)}{\partial w_j} \left(-\varepsilon \dfrac{\partial L(\hat y_{w_s}(X_T),  Y_T)}{\partial w_j}\right)\\
% &= \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x)}{\partial w_j} \left(-\varepsilon \sum_{i = 1}^{M}\dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}\dfrac{\partial \hat y_{w_s}(x_i)}{\partial w_j}\right)\\
% &= -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i} \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x)}{\partial w_j}  \dfrac{\partial \hat y_{w_s}(x_i)}{\partial w_j}\\
% &= -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \nabla_w \hat y_{w_s(t)}(x) \cdot \nabla_w \hat y_{w_s}(x_i)\\
% \end{align}
% Using the fundamental theorem of calculus, we can compute the change in the model's output over step $s$
% \begin{align}
%     y_{s+1} - y_s &= \int_0^1 -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \nabla_w \hat y_{w_s(t)}(x) \cdot \nabla_w \hat y_{w_s}(x_i)dt\\
%  &=  -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s}(x_i)\\
% \end{align}
% For all $N$ training steps, we have
% \begin{align}
% y_N &= b + \sum_{s=1}^N y_{s+1} - y_s\\
% y_N &= b + \sum_{s = 1}^N -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s}(x_i)\\
% % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s}(x_i)\\
% % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t)}(x), \nabla_w \hat y_{w_s}(x_i) \right\rangle dt\\ 
% &= b + \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t,x)}(x), \nabla_w \hat y_{w_s(t,x_i)}(x_i) \right\rangle dt\\ 
% &= b + \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i, s}  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt
% \end{align}
% Since an integral of a symmetric positive semi-definite function is still symmetric and positive-definie and likewise for discrete sums, this represention is a kernel method. 

% \end{proof}
% \subsection{When is an Ensemble of Kernel Machines itself a Kernel Machine?}
% Here we investigate when our derived ensemble of kernel machines composes to a single kernel machine.
% In order to show that a linear combination of kernels also equates to a kernel it is sufficient to show that $sign(a_{i,s}) = sign(a_{i,0})$ for all $a_{i,s}$. 
% In this case it is possible to let the sample weights of our final kernel machine equal $sign(a_{i,0})$.
% In order to show this, we impose some structure on the loss function and network.
% Here we show this is the case for binary crossentropy on a network with sigmoid activations on the logits.
% (TODO: More argument here using mercer's theorem. All positive linear combinations of kernels are kernels. There are cases where some negative coefficients are allowed but that's going to take a lot more thought. How do we extend this to say $aKa > 0$ for all $a$?)

% \begin{proof}

% \begin{align}
%     L(\hat y_i,  y_i) 
%     &= -  y_i \ln(\hat y_i) - (1 - y_i)\ln(1 - \hat y_i)\\
%     \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= \dfrac{y_i - \hat y_i}{(\hat y_i - 1) \hat y_i}
% \end{align}
% For a binary classification problem it is standard to have $y_i \in \{0, 1\}$ and using a sigmoid activation on the final layer we have $\hat y_i \in (0, 1)$. \\

% \begin{center}
    
% \begin{minipage}{0.45\textwidth}
% Assume $y_i = 0$.
% \begin{align}
%     \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= \dfrac{0 - \hat y_i}{(\hat y_i - 1) \hat y_i}\\
%     &= \dfrac{-1}{\hat y_i - 1}\\
%     &= \dfrac{1}{|\hat y_i - 1|}
% \end{align}
% The last equality relies on the fact that $\hat y_i < 1$.
% \begin{equation}
%     y_i = 0 \implies \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} > 0
% \end{equation}
% \end{minipage}
% \hspace{0.04\textwidth}
% \begin{minipage}{0.45\textwidth}
% Assume $y_i = 1$.
% \begin{align}
%     \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= \dfrac{1 - \hat y_i}{(\hat y_i - 1) \hat y_i}\\
%     &= \dfrac{1 - \hat y_i}{-(1-\hat y_i) \hat y_i}\\
%     &= -\dfrac{1}{\hat y_i}
% \end{align}
% Because $\hat y_i > 0$.
% \begin{equation}
%     y_i = 1 \implies \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} < 0
% \end{equation}
% \end{minipage}
% \end{center}

% This shows that the sign of the gradient of the loss function depends only on the label $y_i$, not on the predicted value of our model $\hat y_i$ and is constant through training. 
% Therefore:
% \begin{align}
%     y_S &= b - \varepsilon \sum_{i = 1}^{N}\sum_{s = 1}^S a_{i, s}  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt\\
%      &= b - \varepsilon \sum_{i = 1}^{N}sign(a_{i, 0}) \sum_{s = 1}^S |a_{i, s}| \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt
% \end{align}
% This formulates a kernel machine where
% \begin{align}
% a_{i, 0} &= sign(\dfrac{\partial L(\hat y_{w_0}(x_i),  y_i)}{\partial \hat y_i}) \in \{-1, 1\} \\
% K(x, x_i) &= \sum_{s = 1}^S |a_{i, s}| \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt \\
% \phi_{s,t}(x) &=  \nabla_w \hat y_{w_s(t,x)} (x)\\
% w_s(t,x) &= \begin{cases} w_s, x \in X_T\\ w_s(t), x \notin X_T \end{cases}\\
% b &= 0
% \end{align}
% \end{proof}


% This argument does not hold in the simple case of linear regression. 
% Assume our loss is instead squared error. Our labels are continuous on $\mathds{R}$ and our activation is the identity function.
% \begin{align}
%     L(\hat y_i,  y_i) 
%     &= (y_i - \hat y_{i, s})^2 \\
%     \dfrac{\partial L(\hat y_i,  y_i)}{\partial \hat y_i} &= 2(y_i- \hat y_{i, s})
% \end{align}

% This quantity is dependent on $\hat y_i$ and its sign is changing throughout training. (TODO: Make this more formal and rigorous)

% In order for 
% \begin{align}
%     \sum_{s=1}^S a_{i,s} \int_0^1 \langle \phi_{s,t}(x), \phi_{s,t}(x_i)\rangle dt
% \end{align}
% to be a kernel on its own, we need it to be a positive (or negative) definite operator. In the specific case of our practical path kernel, i.e. that in $K(x,x')$ if $x'$ happens to be equal to $x_i$, then:
% \begin{align}
%     &= \sum_{s=1}^S 2(y_i- \hat y_{i, s}) \int_0^1 \langle \phi_{s,t}(x), \phi_{s,t}(x_i)\rangle dt\\
%     &= \sum_{s=1}^S 2(y_i- \hat y_{i, s}) \int_0^1 \langle \nabla_w \hat y_{w_s(t))} (x), \nabla_w \hat y_{w_s(0)} (x_i)\rangle dt\\
%     &= \sum_{s=1}^S 2 \left(y_i \cdot \int_0^1 \langle \nabla_w \hat y_{w_s(t))} (x), \nabla_w \hat y_{w_s(0)} (x_i)\rangle dt - \hat y_{i, s} \int_0^1 \langle \nabla_w \hat y_{w_s(t))} (x), \nabla_w \hat y_{w_s(0)} (x_i)\rangle dt \right)\\
%     &= \sum_{s=1}^S 2 \left(y_i \cdot \int_0^1 \langle \nabla_w \hat y_{w_s(t))} (x), \nabla_w \hat y_{w_s(0)} (x_i)\rangle dt -  \int_0^1 \langle \nabla_w \hat y_{w_s(t))} (x), \hat y_{i, s} \nabla_w \hat y_{w_s(0)} (x_i)\rangle dt \right)\\
%     &= \sum_{s=1}^S 2 \left(y_i \cdot \int_0^1 \langle \nabla_w \hat y_{w_s(t))} (x), \nabla_w \hat y_{w_s(0)} (x_i)\rangle dt -  \int_0^1 \langle \nabla_w \hat y_{w_s(t))} (x), \dfrac{1}{2}\nabla_w (\hat y_{w_s(0)} (x_i))^2\rangle dt \right)\\
% \end{align}
% Otherwise, we get the usual 
% \begin{align}
%         &= \sum_{s=1}^S 2(y_i- \hat y_{i, s}) \int_0^1 \langle \nabla_w \hat y_{w_s(t,x))} (x), \nabla_w \hat y_{w_s(t,x)} (x')\rangle dt\\
% \end{align}
% The question is two fold. One, in general theory (i.e. the lower example), can we contrive two pairs $(x_1,x'_1)$ and $(x_2,x'_2)$ that don't necessarily need to be training or test images for which this sum is positive for $1$ and negative for $2$. Second, in the case that we are always comparing against training images, do we get something more predictable since there is greater dependence on $x_i$ and we get the above way of re-writing  using the gradient of the square of $\hat y(x_i)$. 


% \subsection{Multi-Class Case}

% There are two ways of treating our loss function $L$ for a number of classes (or number of output activations) $K$:
% \begin{align}
%     \text{Case 1: } L &: \mathbb{R}^K \to \mathbb{R}\\
%     \text{Case 2: } L &: \mathbb{R}^K \to \mathbb{R}^K\\
% \end{align}

% \subsubsection{Case 1 Scalar Loss}

% Let $L : \mathbb{R}^K \to \mathbb{R}$. We will be using the chain rule $D (g \circ f) (x) = Dg(f(x))Df(x)$. 

% Let $\hat y$ be a vector valued function so that $\hat y : \mathbb{R}^D \to \mathbb{R}^K$  satisfying the conditions from [representation theorem above] with $x \in \mathbb{R}^D$ and $y_i \in \mathbb{R}^K$ for every $i$. We note that $\dfrac{\partial \hat y}{\partial t}$ is a column and has shape $Kx1$ and our first chain rule can be done the old fashioned way on each row of that column:
% \begin{align}
%     \dfrac{\partial \hat y}{\partial t} &= \sum_{j=1}^M \dfrac{\partial \hat y(x)}{\partial w_j} \dfrac{\partial w_j}{\partial t}\\
%     &= -\varepsilon \sum_{j=1}^M \dfrac{\partial \hat y(x)}{\partial w_j} \sum_{i=1}^N \dfrac{\partial L(\hat y(x_i), y_i)}{\partial w_j}\\
%     &\text{Apply chain rule}\\
%     &= -\varepsilon \sum_{j=1}^M \dfrac{\partial \hat y(x)}{\partial w_j} \sum_{i=1}^N \dfrac{\partial L(\hat y(x_i), y_i)}{\partial \hat y}\dfrac{\partial \hat y(x_i)}{\partial w_j}\\
%     &\text{Let}\\
%     A &= \dfrac{\partial \hat y(x)}{\partial w_j} \in \mathbb{R}^{K \times 1}\\
%     B &= \dfrac{\partial L(\hat y(x_i), y_i)}{\partial \hat y} \in \mathbb{R}^{1 \times K}\\
%     C &= \dfrac{\partial \hat y(x_i)}{\partial w_j} \in \mathbb{R}^{K \times 1}
% \end{align}
% We have a matrix multiplication $ABC$ and we wish to swap the order so somehow we can pull $B$ out, leaving $A$ and $C$ to compose our product for the representation. Since $BC \in \mathbb{R}$, we have $(BC) = (BC)^T$ and we can write
% \begin{align}
%     (ABC)^T &= (BC)^TA^T = BCA^T\\
%     ABC &= (BCA^T)^T
% \end{align}
% Note: This condition needs to be checked carefully for other formulations so that we can re-order the product as follows:
% \begin{align}
%         &= -\varepsilon \sum_{j=1}^M  \sum_{i=1}^N \left(\dfrac{\partial L(\hat y(x_i), y_i)}{\partial \hat y} 
%         \dfrac{\partial \hat y(x_i)}{\partial w_j} \left(\dfrac{\partial \hat y(x)}{\partial  w_j}\right)^T\right)^T
%         \\
%     &= -\varepsilon \sum_{i=1}^N \left(\dfrac{\partial L(\hat y(x_i), y_i)}{\partial \hat y} 
%     \sum_{j=1}^M \dfrac{\partial \hat y(x_i)}{\partial w_j} \left(\dfrac{\partial \hat y(x)}{\partial w_j}\right)^T\right)^T\\        
% \end{align}
% Note, now that we are summing over $j$, so we can write this as an inner product on $j$ with the $\nabla$ operator which in this case is computing the jacobian of $\hat y$ along the dimensions of class (index k) and weight (index j). We can define 
% \begin{align}
%     (\nabla \hat y(x))_{k,j} &= \dfrac{\partial \hat y_{k}(x)}{\partial w_j}\\
%     &= -\varepsilon \sum_{i=1}^N \left(\dfrac{\partial L(\hat y(x_i), y_i)}{\partial \hat y} 
%      \nabla \hat y(x_i) (\nabla \hat y(x))^T\right)^T\\    
% \end{align}
% We note that the dimensions of each of these matrices in order are $[1,K]$, $[K,M]$, and $[M,K]$ which will yield a matrix of dimension $[1, K]$ i.e. a row vector which we then transpose to get back a column of shape $[K, 1]$

% \subsection{Multi Class Case}


% In the case where $\hat y$ is a vector we denote the data index by the superscript $y^{[i]}$ and the vector component by the subscript $y_k$.

% query point may need to know about other class gradients than the target class. 

% k by k

% yen ting's approach is needed because we are making some assumption about the model that allows us to measure the query change along a linear path. 

% The negative log likelyhood function where $y$ and $\hat y$ are vectors.
% \begin{align}
%     NLL(y, \hat y) = \sum_k^{K} -y_k \ln(\hat y_k)
% \end{align}

% \begin{align}
%     \dfrac{\partial \left[ \hat y_0 ... y_k \right]}{dt} &= \sum_{j=1}^M \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \dfrac{\partial w_j}{dt}\\
%     &= \sum_{j=1}^M \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \left( -\varepsilon \sum_{i = 1}^N \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial w_j} \right)\\
%     &= \sum_{j=1}^M \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \left( -\varepsilon \sum_{i = 1}^N \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]} \dfrac{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]}{\partial w_j} \right)\\
%     &=  -\varepsilon \sum_{i = 1}^N \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]} \sum_{j=1}^M  \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \dfrac{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]}{\partial w_j} 
% \end{align}

% Now we use the fact that L is CCE
% \begin{align}
%     \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial y^{[i]}_k} &= \begin{cases} 0, k \not = y^{[i]}\\ -1 \end{cases}
% \end{align}

% \begin{align}
%     \dfrac{\partial \hat y_k}{dt} &=  -\varepsilon \sum_{i = 1}^N 1 \sum_{j=1}^M  \dfrac{\partial \left[\hat y_0 ... \hat y_k \right]}{\partial w_j} \dfrac{\partial \left[ \hat y^{[i]}_{0} ... \hat y^{[i]}_{k} \right]}{\partial w_j} 
% \end{align}

% \begin{align}
%     y_0 +  \left[ \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial y^{[i]}_0} \sum_{j=1}^M \dfrac{\partial f(x_i)_0}{\partial w_j} \cdot\dfrac{\partial f(x)_0}{\partial w_j}, \dfrac{\partial L(\hat y^{[i]}, y^{[i]})}{\partial y^{[i]}_1} \sum_{j=1}^M \dfrac{\partial f(x_i)_1}{\partial w_j} \cdot\dfrac{\partial f(x)_1}{\partial w_j} \right]
% \end{align}

% % \begin{align}
% % y_N &= b + \sum_{s=1}^N y_{s+1} - y_s\\
% %     &= b + \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t,x)}(x), \nabla_w \hat y_{w_s(t,x_i)}(x_i) \right\rangle dt
% % \end{align}


% \section{Neural Networks are Gaussian Processes}
% With Dropout on, the interpolant from one class to another will go into a variety of other classes. If you make a histogram of the locations where these boundary crossings occur, that will show a gaussian. 

% insert figure(s)

% \section{prove path kernel result in context of differential flow of
% gradients on neural network. using forward euler approx of grad flow. }

% \section{** look for sample in weight space and look for gradients}
%   that are pointing toward the final point versus wanting a different
%   direction. Then dot product those with the training direction. 

% \section{training gradients are smooth}
% \section{robust network types : regularized, michael's pca, Soft Nearest Neighbor Loss (SNNL) and
% adversarially trained. }
% \section{high dimensional arcs are very similar to chords}
% \section{linear interpolated model parameters from random to trained
% state yield robust models}
% \subsection{For Mnist Inner Products in weight space matter more than
% distances} -- does this generalize to ImNet?
% \section{define robustness in terms of skew versus orthogonal}
% \section{define robustness in terms of attack perturbation magnitude}
