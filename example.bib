@article{vonneuman-pocs,
 ISSN = {0003486X},
 URL = {http://www.jstor.org/stable/1968653},
 author = {P. Jordan and J. V. Neumann},
 journal = {Annals of Mathematics},
 number = {3},
 pages = {719--723},
 publisher = {Annals of Mathematics},
 title = {On Inner Products in Linear, Metric Spaces},
 urldate = {2023-07-21},
 volume = {36},
 year = {1935}
}

@article{burgess1996estimating,
	author = {Burgess, A},
	year = 1996,
	title = {Estimating equivalent kernels For neural networks: A data perturbation approach},
	journal = {Advances in Neural Information Processing Systems},
	volume = 9
}
@inproceedings{lin2020gradient,
	author = {Lin, Tianyi and Jin, Chi and Jordan, Michael},
	year = 2020,
	title = {On gradient descent ascent for nonconvex-concave minimax problems},
	booktitle = {International Conference on Machine Learning},
	pages = {6083--6093},
	organization = {PMLR}
}
@misc{neuralode2018,
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	year = 2018,
	title = {Neural Ordinary Differential Equations},
	publisher = {arXiv},
	doi = {10.48550/ARXIV.1806.07366},
	url = {https://arxiv.org/abs/1806.07366},
	copyright = {arXiv.org perpetual, non-exclusive license},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{bilovs2021neural,
	author = {Bilo{\v{s}}, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and G{\"u}nnemann, Stephan},
	year = 2021,
	title = {Neural flows: Efficient alternative to neural ODEs},
	journal = {Advances in Neural Information Processing Systems},
	volume = 34,
	pages = {21325--21337}
}
@inproceedings{NIPS2005_663772ea,
	author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas},
	year = 2005,
	title = {The Curse of Highly Variable Functions for Local Kernel Machines},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {MIT Press},
	volume = 18,
	url = {https://proceedings.neurips.cc/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf}
}
@article{nose1990constant,
	author = {Nose, Shuichi},
	year = 1990,
	title = {Constant-temperature molecular dynamics},
	journal = {Journal of Physics: Condensed Matter},
	publisher = {IOP Publishing},
	volume = 2,
	number = {S},
	pages = {SA115}
}
@article{scherer2020kernel,
	author = {Scherer, Christoph and Scheid, Ren{\'e} and Andrienko, Denis and Bereau, Tristan},
	year = 2020,
	title = {Kernel-based machine learning for efficient simulations of molecular liquids},
	journal = {Journal of chemical theory and computation},
	publisher = {ACS Publications},
	volume = 16,
	number = 5,
	pages = {3194--3204}
}
@article{ilyas2019adversarial,
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	year = 2019,
	title = {Adversarial examples are not bugs, they are features},
	journal = {Advances in neural information processing systems},
	booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS 2019) Vancouver, BC, Canada},
	volume = 32,
	pages = {125--136},
	editor = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett}
}
                  
@article{yousefzadeh2021deep,
	author = {Yousefzadeh, Roozbeh},
	year = 2021,
	title = {Deep learning generalization and the convex hull of training sets},
	journal = {arXiv preprint arXiv:2101.09849}
}
@book{hardle2004nonparametric,
	author = {H{\"a}rdle, Wolfgang and M{\"u}ller, Marlene and Sperlich, Stefan and Werwatz, Axel and others},
	year = 2004,
	title = {Nonparametric and semiparametric models},
	publisher = {Springer},
	volume = 1
}
@article{Selvaraju_2019,
	author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
	year = 2019,
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization},
	month = {oct},
	journal = {International Journal of Computer Vision},
	publisher = {Springer Science and Business Media {LLC}},
	volume = 128,
	number = 2,
	pages = {336--359},
	doi = {10.1007/s11263-019-01228-7},
	url = {https://doi.org/10.1007%2Fs11263-019-01228-7}
}
                  
@article{simonyan2013deep,
  title={Deep inside convolutional networks: Visualising image classification models and saliency maps},
  author={Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1312.6034},
  year={2013}
}
@article{jacot2018neural,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	year = 2018,
	title = {Neural tangent kernel: Convergence and generalization in neural networks},
	journal = {Advances in neural information processing systems},
	volume = 31
}
@article{smola2002support,
	author = {Smola, A},
	year = 2002,
	title = {Support vector machines, regularization, optimization, and beyond},
	journal = {Learning with Kernels}
}
@article{cortes2009learning,
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	year = 2009,
	title = {Learning non-linear combinations of kernels},
	journal = {Advances in neural information processing systems},
	volume = 22
}
@book{shawe2004kernel,
	author = {Shawe-Taylor, John and Cristianini, Nello and others},
	year = 2004,
	title = {Kernel methods for pattern analysis},
	publisher = {Cambridge university press}
}
@article{he2020bayesian,
	author = {He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
	year = 2020,
	title = {Bayesian deep ensembles via the neural tangent kernel},
	journal = {Advances in neural information processing systems},
	volume = 33,
	pages = {1010--1022}
}
@article{Poupon2004,
	author = {Anne Poupon},
	year = 2004,
	title = {Voronoi and Voronoi-related tessellations in studies of protein structure and interaction},
	journal = {Current Opinion in Structural Biology},
	volume = 14,
	number = 2,
	pages = {233--241},
	doi = {https://doi.org/10.1016/j.sbi.2004.03.010},
	issn = {0959-440X},
	url = {http://www.sciencedirect.com/science/article/pii/S0959440X04000442}
}
@article{Guo1997,
	author = {Guo, Baining and Menon, Jai and Willette, Brian},
	year = 1997,
	title = {Surface Reconstruction Using Alpha Shapes},
	journal = {Computer Graphics Forum},
	publisher = {Blackwell Publishers},
	volume = 16,
	number = 4,
	pages = {177--190},
	doi = {10.1111/1467-8659.00178},
	issn = {1467-8659},
	url = {http://dx.doi.org/10.1111/1467-8659.00178},
	keywords = {Surface topology, alpha shapes, manifolds, surface fitting}
}
@article{Maus1984,
	author = {Maus, Arne},
	year = 1984,
	title = {Delaunay triangulation and the convex hull ofn points in expected linear time},
	month = {Jun},
	day = {01},
	journal = {BIT Numerical Mathematics},
	volume = 24,
	number = 2,
	pages = {151--163},
	doi = {10.1007/BF01937482},
	issn = {1572-9125},
	url = {https://doi.org/10.1007/BF01937482},
	abstract = {An algorithm is presented which produces a Delaunay triangulation ofn points in the Euclidean plane in expected linear time. The expected execution time is achieved when the data are (not too far from) uniformly distributed. A modification of the algorithm discussed in the appendix treats most of the non-uniform distributions. The basis of this algorithm is a geographical partitioning of the plane into boxes by the well-known Radix-sort algorithm. This partitioning is also used as a basis for a linear time algorithm for finding the convex hull ofn points in the Euclidean plane.}
}
@article{Lee1980,
	author = {Lee, D. T. and Schachter, B. J.},
	year = 1980,
	title = {Two algorithms for constructing a Delaunay triangulation},
	month = {Jun},
	day = {01},
	journal = {International Journal of Computer {\&} Information Sciences},
	volume = 9,
	number = 3,
	pages = {219--242},
	doi = {10.1007/BF00977785},
	issn = {1573-7640},
	url = {https://doi.org/10.1007/BF00977785},
	abstract = {This paper provides a unified discussion of the Delaunay triangulation. Its geometric properties are reviewed and several applications are discussed. Two algorithms are presented for constructing the triangulation over a planar set ofN points. The first algorithm uses a divide-and-conquer approach. It runs inO(N logN) time, which is asymptotically optimal. The second algorithm is iterative and requiresO(N2) time in the worst case. However, its average case performance is comparable to that of the first algorithm.}
}
@article{Edelsbrunner1983,
	author = {H. Edelsbrunner and D. Kirkpatrick and R. Seidel},
	year = 1983,
	title = {On the shape of a set of points in the plane},
	month = {July},
	journal = {IEEE Transactions on Information Theory},
	volume = 29,
	number = 4,
	pages = {551--559},
	doi = {10.1109/TIT.1983.1056714},
	issn = {0018-9448},
	keywords = {Geometry;Image analysis, shape;Image shape analysis}
}
% dynamical clustering paper
@article{Drieme2017,
	author = {Anne Driemel and Francesco Silvestri},
	year = 2017,
	title = {Locality-sensitive hashing of curves},
	journal = {CoRR},
	volume = {abs/1703.04040},
	url = {http://arxiv.org/abs/1703.04040},
	archiveprefix = {arXiv},
	eprint = {1703.04040},
	timestamp = {Tue, 18 Jul 2017 10:49:06 +0200},
	biburl = {http://dblp.org/rec/bib/journals/corr/DriemelS17},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{Sankararaman2013,
	author = {Sankararaman, Swaminathan and Agarwal, Pankaj K. and M{\o}lhave, Thomas and Pan, Jiangwei and Boedihardjo, Arnold P.},
	year = 2013,
	title = {Model-driven Matching and Segmentation of Trajectories},
	booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	location = {Orlando, Florida},
	publisher = {ACM},
	address = {New York, NY, USA},
	series = {SIGSPATIAL'13},
	pages = {234--243},
	doi = {10.1145/2525314.2525360},
	isbn = {978-1-4503-2521-9},
	url = {http://doi.acm.org/10.1145/2525314.2525360},
	numpages = 10,
	acmid = 2525360,
	keywords = {GPS trajectories, trajectory matching, trajectory segmentation}
}
@article{Mirzargar2014,
	author = {M. Mirzargar and R. T. Whitaker and R. M. Kirby},
	year = 2014,
	title = {Curve Boxplot: Generalization of Boxplot for Ensembles of Curves},
	month = {Dec},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	volume = 20,
	number = 12,
	pages = {2654--2663},
	doi = {10.1109/TVCG.2014.2346455},
	issn = {1077-2626},
	keywords = {computational geometry;data visualisation;boundary values;boxplot generalization;computational scientists;curve boxplot;curve ensembles;data depth;descriptive statistics;nonparametric method;rendering strategies;simulation science;visualization community;visualization strategies;Computational modeling;Curve fitting;Data visualization;Robustness;Shape analysis;Statistical analysis;Uncertainty visualization;boxplots;data depth;ensemble visualization;functional data;nonparametric statistic;order statistics;parametric curves;0}
}
@article{Raj2017,
	author = {Mukund Raj and Mahsa Mirzargar and Robert Ricci and Kirby, {Robert M.} and Whitaker, {Ross T.}},
	year = 2017,
	title = {Path Boxplots: A Method for Characterizing Uncertainty in Path Ensembles on a Graph},
	month = 4,
	journal = {Journal of Computational and Graphical Statistics},
	publisher = {American Statistical Association},
	volume = 26,
	number = 2,
	pages = {243--252},
	doi = {10.1080/10618600.2016.1209115},
	issn = {1061-8600},
	abstract = {Graphs are powerful and versatile data structures that can be used to represent a wide range of different types of information. In this article, we introduce a method to analyze and then visualize an important class of data described over a graph—namely, ensembles of paths. Analysis of such path ensembles is useful in a variety of applications, in diverse fields such as transportation, computer networks, and molecular dynamics. The proposed method generalizes the concept of band depth to an ensemble of paths on a graph, which provides a center-outward ordering on the paths. This ordering is, in turn, used to construct a generalization of the conventional boxplot or whisker plot, called a path boxplot, which applies to paths on a graph. The utility of path boxplot is demonstrated for several examples of path ensembles including paths defined over computer networks and roads. Supplementary materials for this article are available online.},
	keywords = {Data depth, Descriptive statistics, Ensemble, Graph, Paths}
}
@inbook{Kharrat2008,
	author = {Kharrat, Ahmed and Popa, Iulian Sandu and Zeitouni, Karine and Faiz, Sami},
	year = 2008,
	title = {Clustering Algorithm for Network Constraint Trajectories},
	booktitle = {Headway in Spatial Data Handling: 13th International Symposium on Spatial Data Handling},
	publisher = {Springer Berlin Heidelberg},
	address = {Berlin, Heidelberg},
	pages = {631--647},
	doi = {10.1007/978-3-540-68566-1_36},
	isbn = {978-3-540-68566-1},
	url = {https://doi.org/10.1007/978-3-540-68566-1_36},
	nothing = {Ruas, Anne and Gold, Christopher},
	abstract = {Spatial data mining is an active topic in spatial databases. This paper proposes a new clustering method for moving object trajectories databases. It applies specifically to trajectories that only lie on a predefined network. The proposed algorithm (NETSCAN) is inspired from the well-known density based algorithms. However, it takes advantage of the network constraint to estimate the object density. Indeed, NETSCAN first computes dense paths in the network based on the moving object count, then, it clusters the sub-trajectories which are similar to the dense paths. The user can adjust the clustering result by setting a density threshold for the dense paths, and a similarity threshold within the clusters. This paper describes the proposed method. An implementation is reported, along with experimental results that show the effectiveness of our approach and the flexibility allowed by the user parameters.}
}
@article{Zheng2015,
	author = {Zheng, Yu},
	year = 2015,
	title = {Trajectory Data Mining: An Overview},
	month = may,
	journal = {ACM Trans. Intell. Syst. Technol.},
	publisher = {ACM},
	address = {New York, NY, USA},
	volume = 6,
	number = 3,
	pages = {29:1--29:41},
	doi = {10.1145/2743025},
	issn = {2157-6904},
	url = {http://doi.acm.org/10.1145/2743025},
	issue_date = {May 2015},
	articleno = 29,
	numpages = 41,
	acmid = 2743025,
	keywords = {Spatiotemporal data mining, trajectory classification, trajectory compression, trajectory data mining, trajectory indexing and retrieval, trajectory outlier detection, trajectory pattern mining, trajectory uncertainty, urban computing}
}
@article{Shah12017,
	author = {Shah, Sohil Atul and Koltun, Vladlen},
	year = 2017,
	title = {Robust continuous clustering},
	journal = {Proceedings of the National Academy of Sciences},
	volume = 114,
	number = 37,
	pages = {9814--9819},
	doi = {10.1073/pnas.1700770114},
	url = {http://www.pnas.org/content/114/37/9814.abstract},
	abstract = {Clustering is a fundamental procedure in the analysis of scientific data. It is used ubiquitously across the sciences. Despite decades of research, existing clustering algorithms have limited effectiveness in high dimensions and often require tuning parameters for different domains and datasets. We present a clustering algorithm that achieves high accuracy across multiple domains and scales efficiently to high dimensions and large datasets. The presented algorithm optimizes a smooth continuous objective, which is based on robust statistics and allows heavily mixed clusters to be untangled. The continuous nature of the objective also allows clustering to be integrated as a module in end-to-end feature learning pipelines. We demonstrate this by extending the algorithm to perform joint clustering and dimensionality reduction by efficiently optimizing a continuous global objective. The presented approach is evaluated on large datasets of faces, hand-written digits, objects, newswire articles, sensor readings from the Space Shuttle, and protein expression levels. Our method achieves high accuracy across all datasets, outperforming the best prior algorithm by a factor of 3 in average rank.},
	eprint = {http://www.pnas.org/content/114/37/9814.full.pdf}
}
@article{Jaromczyk1982,
	author = {J. W. Jaromczyk and G. T. Toussaint},
	year = 1992,
	title = {Relative neighborhood graphs and their relatives},
	month = {Sep},
	journal = {Proceedings of the IEEE},
	volume = 80,
	number = 9,
	pages = {1502--1517},
	doi = {10.1109/5.163414},
	issn = {0018-9219},
	keywords = {computational geometry;computer vision;pattern recognition;spatial data structures;visual databases;computational morphology;computer vision;databases;neighborhood graphs;pattern classification;spatial analysis;Application software;Bibliographies;Biology computing;Computational geometry;Computer applications;Computer science;Computer vision;Morphology;Pattern analysis;Shape}
}
@inproceedings{Chakrabarti2006,
	author = {Chakrabarti, Deepayan and Kumar, Ravi and Tomkins, Andrew},
	year = 2006,
	title = {Evolutionary clustering},
	booktitle = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages = {554--560},
	organization = {ACM}
}
@inproceedings{Andrade2001,
	author = {Andrade, Diogo Vieira and de Figueiredo, Luiz Henrique},
	year = 2001,
	title = {Good approximations for the relative neighbourhood graph.},
	booktitle = {CCCG},
	pages = {25--28}
}
@book{greene2003,
	author = {Greene, W.H.},
	year = 2003,
	title = {Econometric Analysis},
	publisher = {Pearson Education},
	isbn = 9788177586848,
	url = {https://books.google.com/books?id=njAcXDlR5U8C}
}
@article{mcfadden1973,
	author = {McFadden, Daniel and others},
	year = 1973,
	title = {Conditional logit analysis of qualitative choice behavior},
	journal = {Frontiers in Econometrics},
	publisher = {Institute of Urban and Regional Development, University of California}
}
@phdthesis{novelli2015,
	author = {Novelli, Francesco},
	year = 2015,
	title = {Detection and Measurement of Sales Cannibalization in Information Technology Markets},
	school = {Technische Universit{\"a}t}
}
@article{draganska2004,
	author = {Draganska, Michaela and Jain, Dipak},
	year = 2004,
	title = {A likelihood approach to estimating market equilibrium models},
	journal = {Management Science},
	publisher = {INFORMS},
	volume = 50,
	number = 5,
	pages = {605--616}
}
@misc{shriver2015,
	author = {Shriver, Scott and Bollinger, Bryan},
	year = 2015,
	title = {A Structural Model of Channel Choice with Implications for Retail Entry}
}
@article{fisher2009,
	author = {Fisher, Marshall L and Vaidyanathan, Ramnath},
	year = 2009,
	title = {An algorithm and demand estimation procedure for retail assortment optimization},
	journal = {Philadelphia: The Wharton School}
}
@article{liu1990,
	author = {Liu, Regina Y and others},
	year = 1990,
	title = {On a notion of data depth based on random simplices},
	journal = {The Annals of Statistics},
	publisher = {Institute of Mathematical Statistics},
	volume = 18,
	number = 1,
	pages = {405--414}
}
@article{lopez2009,
	author = {L{\'o}pez-Pintado, Sara and Romo, Juan},
	year = 2009,
	title = {On the concept of depth for functional data},
	journal = {Journal of the American Statistical Association},
	publisher = {Taylor \& Francis},
	volume = 104,
	number = 486,
	pages = {718--734}
}
@article{rousseeuw1996,
	author = {Rousseeuw, Peter J and Ruts, Ida},
	year = 1996,
	title = {Algorithm AS 307: Bivariate location depth},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	publisher = {JSTOR},
	volume = 45,
	number = 4,
	pages = {516--526}
}
@inproceedings{cheng2001,
	author = {Cheng, Andrew Y and Ouyang, Ming},
	year = 2001,
	title = {On algorithms for simplicial depth.},
	booktitle = {CCCG},
	pages = {53--56}
}
@article{krishnan2006,
	author = {Krishnan, Suresh and Mustafa, Nabil H and Venkatasubramanian, Suresh},
	year = 2006,
	title = {Statistical data depth and the graphics hardware},
	journal = {DIMACS Series in Discrete Mathematics and Theoretical Computer Science},
	publisher = {AMERICAN MATHEMATICAL SOCIETY},
	volume = 72,
	pages = 223
}
@article{Zasenko2016,
	author = {Olga Zasenko and Tamon Stephen},
	year = 2016,
	title = {Algorithms for Colourful Simplicial Depth and Medians in the Plane},
	journal = {CoRR},
	volume = {abs/1608.07348},
	url = {http://arxiv.org/abs/1608.07348},
	archiveprefix = {arXiv},
	eprint = {1608.07348},
	timestamp = {Wed, 07 Jun 2017 14:42:05 +0200},
	biburl = {http://dblp.org/rec/bib/journals/corr/ZasenkoS16},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{Linderman2017,
	author = {George C. Linderman and Stefan Steinerberger},
	year = 2017,
	title = {Clustering with t-SNE, provably},
	journal = {CoRR},
	volume = {abs/1706.02582},
	url = {http://arxiv.org/abs/1706.02582},
	archiveprefix = {arXiv},
	eprint = {1706.02582},
	timestamp = {Mon, 03 Jul 2017 13:29:02 +0200},
	biburl = {http://dblp.org/rec/bib/journals/corr/LindermanS17},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{wattenberg2016how,
	author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
	year = 2016,
	title = {How to Use t-SNE Effectively},
	journal = {Distill},
	doi = {10.23915/distill.00002},
	url = {http://distill.pub/2016/misread-tsne}
}
@inproceedings{Lee_Verleysen2014,
	author = {J. A. Lee and M. Verleysen},
	year = 2014,
	title = {Two key properties of dimensionality reduction methods},
	month = {Dec},
	booktitle = {2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)},
	volume = {},
	number = {},
	pages = {163--170},
	doi = {10.1109/CIDM.2014.7008663},
	issn = {},
	keywords = {data reduction;data structures;neural nets;principal component analysis;DR;data representation;deep neural networks;dimensionality reduction;principal component analysis;Cost function;Covariance matrices;Force;Manifolds;Plastics;Principal component analysis;Vectors}
}
@inproceedings{simon1996,
	author = {Simon, Daniel R},
	year = 1996,
	title = {Anonymous communication and anonymous cash},
	booktitle = {Annual International Cryptology Conference},
	pages = {61--73},
	organization = {Springer}
}
@inproceedings{al2016,
	author = {Al-Mehairi, Yaared and Coecke, Bob and Lewis, Martha},
	year = 2016,
	title = {Categorical Compositional Cognition},
	booktitle = {International Symposium on Quantum Interaction},
	pages = {122--134},
	organization = {Springer}
}
@article{kartsaklis2013,
	author = {Kartsaklis, Dimitri and Sadrzadeh, Mehrnoosh and Pulman, Stephen and Coecke, Bob},
	year = 2013,
	title = {Reasoning about meaning in natural language with compact closed categories and frobenius algebras},
	journal = {Logic and Algebraic Structures in Quantum Computing},
	pages = 199
}
@article{berger2017cite2vec,
	author = {Berger, Matthew and McDonough, Katherine and Seversky, Lee M},
	year = 2017,
	title = {cite2vec: citation-driven document exploration via word embeddings},
	journal = {IEEE transactions on visualization and computer graphics},
	publisher = {IEEE},
	volume = 23,
	number = 1,
	pages = {691--700}
}
@article{lagarias2002beyond,
	author = {Lagarias, Jeffrey C and Mallows, Colin L and Wilks, Allan R},
	year = 2002,
	title = {Beyond the Descartes circle theorem},
	journal = {The American mathematical monthly},
	publisher = {JSTOR},
	volume = 109,
	number = 4,
	pages = {338--361}
}
@article{Gatys2015,
	author = {Leon A. Gatys and Alexander S. Ecker and Matthias Bethge},
	year = 2015,
	title = {A Neural Algorithm of Artistic Style},
	journal = {CoRR},
	volume = {abs/1508.06576},
	url = {http://arxiv.org/abs/1508.06576},
	archiveprefix = {arXiv},
	eprint = {1508.06576},
	timestamp = {Wed, 07 Jun 2017 14:41:58 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/GatysEB15a},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Haber2017,
	author = {Eldad Haber and Lars Ruthotto},
	year = 2017,
	title = {Stable Architectures for Deep Neural Networks},
	journal = {CoRR},
	volume = {abs/1705.03341},
	url = {http://arxiv.org/abs/1705.03341},
	archiveprefix = {arXiv},
	eprint = {1705.03341},
	timestamp = {Wed, 07 Jun 2017 14:40:21 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/HaberR17},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Chang2017,
	author = {Bo Chang and Lili Meng and Eldad Haber and Lars Ruthotto and David Begert and Elliot Holtham},
	year = 2017,
	title = {Reversible Architectures for Arbitrarily Deep Residual Neural Networks},
	journal = {CoRR},
	volume = {abs/1709.03698},
	url = {http://arxiv.org/abs/1709.03698},
	archiveprefix = {arXiv},
	eprint = {1709.03698},
	timestamp = {Thu, 05 Oct 2017 09:42:58 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1709-03698},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
                  
@inproceedings{szegedy2013,
  author       = {Christian Szegedy and
                  Wojciech Zaremba and
                  Ilya Sutskever and
                  Joan Bruna and
                  Dumitru Erhan and
                  Ian J. Goodfellow and
                  Rob Fergus},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Intriguing properties of neural networks},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6199},
  timestamp    = {Thu, 25 Jul 2019 14:35:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}                  
@article{Johnson2016,
	author = {Justin Johnson and Alexandre Alahi and Fei{-}Fei Li},
	year = 2016,
	title = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
	journal = {CoRR},
	volume = {abs/1603.08155},
	url = {http://arxiv.org/abs/1603.08155},
	archiveprefix = {arXiv},
	eprint = {1603.08155},
	timestamp = {Wed, 07 Jun 2017 14:40:26 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/JohnsonAL16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{prakash2018,
	author = {Aaditya Prakash and Nick Moran and Solomon Garber and Antonella DiLillo and James A. Storer},
	year = 2018,
	title = {Deflecting Adversarial Attacks with Pixel Deflection},
	journal = {CoRR},
	booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR}) Salt Lake City, UT, USA},
	volume = {abs/1801.08926},
	pages = {8571--8580},
	url = {http://arxiv.org/abs/1801.08926},
	archiveprefix = {arXiv},
	eprint = {1801.08926},
	timestamp = {Fri, 02 Feb 2018 14:20:25 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1801-08926},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{prakash_ecting_nodate,
	author = {Prakash, Aaditya and Moran, Nick and Garber, Solomon and DiLillo, Antonella and Storer, James and University, Brandeis},
	title = {Deflecting Adversarial Attacks with Pixel Deflection},
	pages = 17,
	abstract = {CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classiﬁcation accuracy is signiﬁcantly preserved in the presence of such adversarial manipulations. Image classiﬁers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.},
	language = {en},
	file = {Prakash et al. - Deflecting Adversarial Attacks with Pixel Deflection.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\QUNFRPBY\\Prakash et al. - Deflecting Adversarial Attacks with Pixel Deflection.pdf:application/pdf}
}
@article{haber_stable_2018,
	author = {Haber, Eldad and Ruthotto, Lars},
	year = 2018,
	title = {Stable architectures for deep neural networks},
	month = jan,
	journal = {Inverse Problems},
	volume = 34,
	number = 1,
	pages = {014004},
	doi = {10.1088/1361-6420/aa9a90},
	issn = {0266-5611, 1361-6420},
	url = {http://stacks.iop.org/0266-5611/34/i=1/a=014004?key=crossref.1cc46f347b817746f33b5329460be31b},
	urldate = {2018-04-25},
	abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g., classiﬁcation of text or images. While often oﬀering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Important issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper we propose new forward propagation techniques inspired by systems of Ordinary Diﬀerential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks.},
	language = {en},
	file = {Haber and Ruthotto - 2018 - Stable architectures for deep neural networks.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\VGE5DB2I\\Haber and Ruthotto - 2018 - Stable architectures for deep neural networks.pdf:application/pdf}
}
@article{gatys_neural_2016,
	author = {Gatys, Leon and Ecker, Alexander and Bethge, Matthias},
	year = 2016,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	month = sep,
	journal = {Journal of Vision},
	volume = 16,
	number = 12,
	pages = 326,
	doi = {10.1167/16.12.326},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/16.12.326},
	urldate = {2018-04-25},
	language = {en},
	file = {Gatys et al. - 2016 - A Neural Algorithm of Artistic Style.pdf:C\:\\Use'rs\\Nexus\\Zotero\\storage\\8TSSA4PG\\Gatys et al. - 2016 - A Neural Algorithm of Artistic Style.pdf:application/pdf}
}
@article{chang_reversible_2017,
	author = {Chang, Bo and Meng, Lili and Haber, Eldad and Ruthotto, Lars and Begert, David and Holtham, Elliot},
	year = 2017,
	title = {Reversible Architectures for Arbitrarily Deep Residual Neural Networks},
	month = {Nov},
	pages = 8,
	abstract = {Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memoryefﬁcient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efﬁcacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.},
	language = {en},
	file = {Chang et al. - Reversible Architectures for Arbitrarily Deep Resi.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\CS6F3WMH\\Chang et al. - Reversible Architectures for Arbitrarily Deep Resi.pdf:application/pdf}
}
@article{johnson_perceptual_2016,
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	year = 2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	month = mar,
	journal = {arXiv:1603.08155 [cs]},
	url = {http://arxiv.org/abs/1603.08155},
	urldate = {2018-04-25},
	note = {arXiv: 1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {arXiv\:1603.08155 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\8P957BLX\\Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\5XBNB97T\\1603.html:text/html}
}
@article{goodfellow_explaining_2014,
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year = 2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	month = dec,
	journal = {arXiv:1412.6572 [cs, stat]},
	booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA},
	url = {http://arxiv.org/abs/1412.6572},
	urldate = {2018-04-25},
	note = {arXiv: 1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1412.6572 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\4AJ5ZRYV\\Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\KB34UKC8\\1412.html:text/html},
	editor = {Yoshua Bengio and Yann LeCun}
}
@article{kurakin_adversarial_2016,
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	year = 2016,
	title = {Adversarial examples in the physical world},
	month = jul,
	journal = {arXiv:1607.02533 [cs, stat]},
	booktitle = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France},
	url = {http://arxiv.org/abs/1607.02533},
	urldate = {2018-04-25},
	note = {arXiv: 1607.02533},
	abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: 14 pages, 6 figures. Demo available at https://youtu.be/zQ\_uMenoBCk},
	file = {arXiv\:1607.02533 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\35I7YL6G\\Kurakin et al. - 2016 - Adversarial examples in the physical world.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\6BVECYWQ\\1607.html:text/html}
}
@article{papernot_cleverhans_2016,
	author = {Papernot, Nicolas and Carlini, Nicholas and Goodfellow, Ian and Feinman, Reuben and Faghri, Fartash and Matyasko, Alexander and Hambardzumyan, Karen and Juang, Yi-Lin and Kurakin, Alexey and Sheatsley, Ryan and Garg, Abhibhav and Lin, Yen-Chen},
	year = 2016,
	title = {cleverhans v2.0.0: an adversarial machine learning library},
	shorttitle = {cleverhans v2.0.0},
	month = oct,
	journal = {arXiv:1610.00768 [cs, stat]},
	url = {http://arxiv.org/abs/1610.00768},
	urldate = {2018-04-25},
	note = {arXiv: 1610.00768},
	abstract = {{\textbackslash}texttt\{cleverhans\} is a software library that provides standardized reference implementations of {\textbackslash}emph\{adversarial example\} construction techniques and {\textbackslash}emph\{adversarial training\}. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section{\textasciitilde}{\textbackslash}ref\{sec:introduction\} provides an overview of adversarial examples in machine learning and of the {\textbackslash}texttt\{cleverhans\} software. Section{\textasciitilde}{\textbackslash}ref\{sec:core\} presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section{\textasciitilde}{\textbackslash}ref\{sec:benchmark\} describes how to report benchmark results using the library. Section{\textasciitilde}{\textbackslash}ref\{sec:version\} describes the versioning system.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: Technical report for https://github.com/openai/cleverhans},
	file = {arXiv\:1610.00768 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\RTY5GJJN\\Papernot et al. - 2016 - cleverhans v2.0.0 an adversarial machine learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\GZ9CJGY3\\1610.html:text/html}
}
@article{papernot_practical_2016,
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	year = 2016,
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	month = feb,
	journal = {arXiv:1602.02697 [cs]},
	url = {http://arxiv.org/abs/1602.02697},
	urldate = {2018-04-25},
	note = {arXiv: 1602.02697},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning},
	annote = {Comment: Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE},
	file = {arXiv\:1602.02697 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\7E32ER7G\\Papernot et al. - 2016 - Practical Black-Box Attacks against Machine Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\VGUUWBMH\\1602.html:text/html}
}
@article{papernot_limitations_2015,
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	year = 2015,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	month = nov,
	journal = {arXiv:1511.07528 [cs, stat]},
	url = {http://arxiv.org/abs/1511.07528},
	urldate = {2018-04-25},
	note = {arXiv: 1511.07528},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted to the 1st IEEE European Symposium on Security \& Privacy, IEEE 2016. Saarbrucken, Germany},
	file = {arXiv\:1511.07528 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\SI9WW5F6\\Papernot et al. - 2015 - The Limitations of Deep Learning in Adversarial Se.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\VQ5FT6KW\\1511.html:text/html}
}
@article{moosavi-dezfooli_deepfool:_2015,
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	year = 2015,
	title = {{DeepFool}: a simple and accurate method to fool deep neural networks},
	shorttitle = {{DeepFool}},
	month = nov,
	journal = {arXiv:1511.04599 [cs]},
	url = {http://arxiv.org/abs/1511.04599},
	urldate = {2018-04-25},
	note = {arXiv: 1511.04599},
	abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016},
	file = {arXiv\:1511.04599 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\R4FMVV3B\\Moosavi-Dezfooli et al. - 2015 - DeepFool a simple and accurate method to fool dee.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\KZ58SXIR\\1511.html:text/html}
}
@article{carlini_towards_2016,
	author = {Carlini, Nicholas and Wagner, David},
	year = 2016,
	title = {Towards Evaluating the Robustness of Neural Networks},
	month = aug,
	journal = {arXiv:1608.04644 [cs]},
	booktitle = {2017 {IEEE} symposium on security and privacy ({SP})},
	pages = {39--57},
	url = {http://arxiv.org/abs/1608.04644},
	urldate = {2018-04-25},
	note = {arXiv: 1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input \$x\$ and any target classification \$t\$, it is possible to find a new input \$x'\$ that is similar to \$x\$ but classified as \$t\$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from \$95{\textbackslash}\%\$ to \$0.5{\textbackslash}\%\$. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with \$100{\textbackslash}\%\$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	file = {arXiv\:1608.04644 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\Q4T8UTKH\\Carlini and Wagner - 2016 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\PQ7PVTRJ\\1608.html:text/html},
	organization = {IEEE}
}
@article{madry_towards_2017,
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = 2017,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	month = jun,
	journal = {arXiv:1706.06083 [cs, stat]},
	booktitle = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada},
	url = {http://arxiv.org/abs/1706.06083},
	urldate = {2018-04-25},
	note = {arXiv: 1706.06083},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1706.06083 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\QL6LA2DS\\Madry et al. - 2017 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\6YSIR8L9\\1706.html:text/html}
}
@article{su_one_2017,
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	year = 2017,
	title = {One pixel attack for fooling deep neural networks},
	month = oct,
	journal = {arXiv:1710.08864 [cs, stat]},
	url = {http://arxiv.org/abs/1710.08864},
	urldate = {2018-04-25},
	note = {arXiv: 1710.08864},
	abstract = {Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution. It requires less adversarial information and can fool more types of networks. The results show that 70.97\% of the natural images can be perturbed to at least one target class by modifying just one pixel with 97.47\% confidence on average. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.08864 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\47WQZ5NK\\Su et al. - 2017 - One pixel attack for fooling deep neural networks.pdf:application/pdf}
}
@misc{aksh00,
	author = {Akshay Chawla},
	year = 2017,
	title = {{GTS}: {GNU} {Triangulated} {Surface} library},
	month = {November},
	howpublished = {https://github.com/akshaychawla/Adversarial-Examples-in-PyTorch}
}
@book{Bishop:2006:PRM:1162264,
	author = {Bishop, Christopher M.},
	year = 2006,
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	publisher = {Springer-Verlag New York, Inc.},
	address = {Secaucus, NJ, USA},
	isbn = {0387310738}
}
@misc{arden2018,
	author = {Arden Dertat},
	year = 2018,
	title = {Applied-{Deep}-{Learning}-with-{Keras}/{Part} 4 ({GPU}) - {Convolutional} {Neural} {Networks}.ipynb at master · ardendertat/{Applied}-{Deep}-{Learning}-with-{Keras} · {GitHub}},
	url = {https://github.com/ardendertat/Applied-Deep-Learning-with-Keras/blob/master/notebooks/Part%204%20%28GPU%29%20-%20Convolutional%20Neural%20Networks.ipynb},
	urldate = {2018-06-30}
}
@article{nair_rectified_nodate,
	author = {Nair, Vinod and Hinton, Geoffrey E},
	year = 2010,
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines}},
	booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
	pages = {807--814},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	language = {en},
	file = {Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\MEQHIX28\\Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:application/pdf}
}
@misc{goodfellow2013multidigit,
	author = {Ian J. Goodfellow and Yaroslav Bulatov and Julian Ibarz and Sacha Arnoud and Vinay Shet},
	year = 2013,
	title = {Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks},
	journal = {arXiv preprint, arXiv:1312.6082},
	eprint = {1312.6082},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{mohammad16,
	author = {Mohammad Rastegari and Vicente Ordonez and Joseph Redmon and Ali Farhadi},
	year = 2016,
	title = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
	journal = {CoRR},
	volume = {abs/1603.05279},
	url = {http://arxiv.org/abs/1603.05279},
	archiveprefix = {arXiv},
	eprint = {1603.05279},
	timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/RastegariORF16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{inevitable2018,
	author = {Ali Shafahi and W. Ronny Huang and Christoph Studer and Soheil Feizi and Tom Goldstein},
	year = 2018,
	title = {Are adversarial examples inevitable?},
	journal = {CoRR},
	booktitle = {International Conference on Learning Representations ({ICLR})},
	volume = {abs/1809.02104},
	url = {http://arxiv.org/abs/1809.02104},
	archiveprefix = {arXiv},
	eprint = {1809.02104},
	timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1809-02104},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{madry2018towards,
  author       = {Aleksander Madry and
                  Aleksandar Makelov and
                  Ludwig Schmidt and
                  Dimitris Tsipras and
                  Adrian Vladu},
  title        = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=rJzIBfZAb},
  timestamp    = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MadryMSTV18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}                  
@article{tsipras2018robustness,
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	year = 2018,
	title = {Robustness may be at odds with accuracy},
	journal = {stat},
	booktitle = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA},
	volume = 1050,
	pages = 11
}
@inproceedings{nguyen2015deep,
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	year = 2015,
	title = {Deep neural networks are easily fooled: High confidence predictions for unrecognizable images},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages = {427--436}
}
@article{simant2018,
	author = {Simant Dube},
	year = 2018,
	title = {High Dimensional Spaces, Deep Learning and Adversarial Examples},
	journal = {CoRR},
	volume = {abs/1801.00634},
	url = {http://arxiv.org/abs/1801.00634},
	archiveprefix = {arXiv},
	eprint = {1801.00634},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1801-00634},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{erichson2019,
	author = {N. Benjamin Erichson and Zhewei Yao and Michael W. Mahoney},
	year = 2019,
	title = {JumpReLU: {A} Retrofit Defense Strategy for Adversarial Attacks},
	journal = {CoRR},
	volume = {abs/1904.03750},
	url = {http://arxiv.org/abs/1904.03750},
	archiveprefix = {arXiv},
	eprint = {1904.03750},
	timestamp = {Thu, 25 Apr 2019 13:55:01 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1904-03750},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{frosst2018,
	author = {Nicholas Frosst and Sara Sabour and Geoffrey E. Hinton},
	year = 2018,
	title = {{DARCCC:} Detecting Adversaries by Reconstruction from Class Conditional Capsules},
	journal = {CoRR},
	volume = {abs/1811.06969},
	url = {http://arxiv.org/abs/1811.06969},
	archiveprefix = {arXiv},
	eprint = {1811.06969},
	timestamp = {Sun, 25 Nov 2018 18:57:12 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1811-06969},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
%%%%#######%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{volodmymyr2016,
	author = {Volodymyr Mnih and Adri{\`{a}} Puigdom{\`{e}}nech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
	year = 2016,
	title = {Asynchronous Methods for Deep Reinforcement Learning},
	journal = {CoRR},
	volume = {abs/1602.01783},
	url = {http://arxiv.org/abs/1602.01783},
	archiveprefix = {arXiv},
	eprint = {1602.01783},
	timestamp = {Wed, 07 Jun 2017 14:43:09 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/MnihBMGLHSK16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Penghang2018,
	author = {Penghang Yin and Shuai Zhang and Jiancheng Lyu and Stanley Osher and Yingyong Qi and Jack Xin},
	year = 2018,
	title = {Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks},
	journal = {CoRR},
	volume = {abs/1808.05240},
	url = {http://arxiv.org/abs/1808.05240},
	archiveprefix = {arXiv},
	eprint = {1808.05240},
	timestamp = {Fri, 21 Dec 2018 14:34:10 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1808-05240},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{selvaraju2017,
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year = 2017,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	shorttitle = {Grad-{CAM}},
	month = oct,
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	address = {Venice},
	pages = {618--626},
	doi = {10.1109/ICCV.2017.74},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237336/},
	urldate = {2019-05-16},
	abstract = {We propose a technique for producing â€˜visual explanationsâ€™ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach â€“ Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for â€˜dogâ€™ or even a caption), ï¬‚owing into the ï¬nal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. VQA) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing ï¬ne-grained visualizations to create a high-resolution class-discriminative visualization and apply it to image classiï¬cation, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classiï¬cation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a â€˜strongerâ€™ deep network from a â€˜weakerâ€™ one. Our code is available at https://github.com/ramprs/grad-cam/ and a demo is available on CloudCV [2]1. Video of the demo can be found at youtu.be/COjUB9Izk6E.},
	language = {en},
	file = {Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf:/home/bwbell/Zotero/storage/G7359QEK/Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf}
}
@inproceedings{Lee2018ASU,
	author = {Kimin Lee and Kibok Lee and Honglak Lee and Jinwoo Shin},
	year = 2018,
	title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
	booktitle = {NeurIPS}
}
@inproceedings{gao2018robust,
	author = {Gao, Rui and Xie, Liyan and Xie, Yao and Xu, Huan},
	year = 2018,
	title = {Robust hypothesis testing using wasserstein uncertainty sets},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {7902--7912}
}
@inproceedings{Zhou:2010:IYR:1879141.1879193,
	author = {Zhou, Renjie and Khemmarat, Samamon and Gao, Lixin},
	year = 2010,
	title = {The Impact of YouTube Recommendation System on Video Views},
	booktitle = {Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement},
	location = {Melbourne, Australia},
	publisher = {ACM},
	address = {New York, NY, USA},
	series = {IMC '10},
	pages = {404--410},
	doi = {10.1145/1879141.1879193},
	isbn = {978-1-4503-0483-2},
	url = {http://doi.acm.org/10.1145/1879141.1879193},
	numpages = 7,
	acmid = 1879193,
	keywords = {YouTube, recommendation system, video sharing site, view diversity, view sources}
}
@article{DBLP:journals/corr/abs-1811-10104,
	author = {Ben Hutchinson and Margaret Mitchell},
	year = 2018,
	title = {50 Years of Test (Un)fairness: Lessons for Machine Learning},
	journal = {CoRR},
	volume = {abs/1811.10104},
	url = {http://arxiv.org/abs/1811.10104},
	archiveprefix = {arXiv},
	eprint = {1811.10104},
	timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1811-10104},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/EvtimovEFKLPRS17,
	author = {Ivan Evtimov and Kevin Eykholt and Earlence Fernandes and Tadayoshi Kohno and Bo Li and Atul Prakash and Amir Rahmati and Dawn Song},
	year = 2017,
	title = {Robust Physical-World Attacks on Machine Learning Models},
	journal = {CoRR},
	volume = {abs/1707.08945},
	url = {http://arxiv.org/abs/1707.08945},
	archiveprefix = {arXiv},
	eprint = {1707.08945},
	timestamp = {Thu, 09 May 2019 13:10:56 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/EvtimovEFKLPRS17},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/BojarskiTDFFGJM16,
	author = {Mariusz Bojarski and Davide Del Testa and Daniel Dworakowski and Bernhard Firner and Beat Flepp and Prasoon Goyal and Lawrence D. Jackel and Mathew Monfort and Urs Muller and Jiakai Zhang and Xin Zhang and Jake Zhao and Karol Zieba},
	year = 2016,
	title = {End to End Learning for Self-Driving Cars},
	journal = {CoRR},
	volume = {abs/1604.07316},
	url = {http://arxiv.org/abs/1604.07316},
	archiveprefix = {arXiv},
	eprint = {1604.07316},
	timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/BojarskiTDFFGJM16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{wyly2008subprime,
	author = {Wyly, Elvin K and Moos, Markus and Foxcroft, Holly and Kabahizi, Emmanuel},
	year = 2008,
	title = {Subprime mortgage segmentation in the American urban system},
	journal = {Tijdschrift voor economische en sociale geografie},
	publisher = {Wiley Online Library},
	volume = 99,
	number = 1,
	pages = {3--23}
}
@article{rosenblatt1958perceptron,
	author = {Rosenblatt, Frank},
	year = 1958,
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	journal = {Psychological review},
	publisher = {American Psychological Association},
	volume = 65,
	number = 6,
	pages = 386
}
@book{ivakhnenko1965cybernetic,
	author = {Ivakhnenko, Alekse Grigorevich and Lapa, Valentin Grigor\'evich},
	year = 1965,
	title = {Cybernetic predicting devices},
	publisher = {CCM Information Corporation}
}
@article{SCHMIDHUBER201585,
	author = {JÃ¼rgen Schmidhuber},
	year = 2015,
	title = {Deep learning in neural networks: An overview},
	journal = {Neural Networks},
	volume = 61,
	pages = {85--117},
	doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}
@article{minsky1969perceptrons,
	author = {Minsky, Marvin and Papert, Seymour},
	year = 1969,
	title = {Perceptrons: Anlntroduction to computational geometry},
	journal = {MITPress, Cambridge, Massachusetts}
}
@article{werbos1974beyond,
	author = {Werbos, Paul J},
	year = 1974,
	title = {Beyond regression: New tools for prediction and analysis in the be havioral sciences/'PhD diss., Harvard Uni versity. Werbos, Paul J. 1988},
	journal = {Generalization of back propagation with application to a recurrent gas market method," Neural Networks},
	volume = 1,
	number = 4,
	pages = {339--356}
}
@article{mcclelland1986parallel,
	author = {McClelland, James L and Rumelhart, David E and PDP Research Group and others},
	year = 1986,
	title = {Parallel distributed processing},
	journal = {Explorations in the Microstructure of Cognition},
	publisher = {MIT Press Cambridge, Ma},
	volume = 2,
	pages = {216--271}
}
@article{lecun1998gradient,
	author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
	year = 1998,
	title = {Gradient-based learning applied to document recognition},
	journal = {Proceedings of the IEEE},
	publisher = {Taipei, Taiwan},
	volume = 86,
	number = 11,
	pages = {2278--2324}
}
@article{lecun1989backpropagation,
	author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
	year = 1989,
	title = {Backpropagation applied to handwritten zip code recognition},
	journal = {Neural computation},
	publisher = {MIT Press},
	volume = 1,
	number = 4,
	pages = {541--551}
}
@inproceedings{goodfellow2014generative,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = 2014,
	title = {Generative adversarial nets},
	booktitle = {Advances in neural information processing systems},
	pages = {2672--2680}
}
@article{malik1990preattentive,
	author = {Malik, Jitendra and Perona, Pietro},
	year = 1990,
	title = {Preattentive texture discrimination with early vision mechanisms},
	journal = {JOSA A},
	publisher = {Optical Society of America},
	volume = 7,
	number = 5,
	pages = {923--932}
}
@inproceedings{li2017convergence,
	author = {Yuanzhi {Li} and Yang {Yuan}},
	year = 2017,
	title = {Convergence Analysis of Two-layer Neural Networks with ReLU Activation},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {597--607},
	notes = {Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963519230}
}
@article{petersen2018optimal,
	author = {Petersen, Philipp and Voigtlaender, Felix},
	year = 2018,
	title = {Optimal approximation of piecewise smooth functions using deep ReLU neural networks},
	journal = {Neural Networks},
	publisher = {Elsevier},
	volume = 108,
	pages = {296--330}
}
@article{HardtRS15,
	author = {Moritz Hardt and Benjamin Recht and Yoram Singer},
	year = 2015,
	title = {Train faster, generalize better: Stability of stochastic gradient descent},
	journal = {CoRR},
	volume = {abs/1509.01240},
	url = {http://arxiv.org/abs/1509.01240},
	archiveprefix = {arXiv},
	eprint = {1509.01240},
	timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
	biburl = {https://dblp.org/rec/journals/corr/HardtRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{liu2015very,
	author = {Liu, Shuying and Deng, Weihong},
	year = 2015,
	title = {Very deep convolutional neural network based image classification using small training sample size},
	booktitle = {2015 3rd IAPR Asian conference on pattern recognition (ACPR)},
	pages = {730--734},
	organization = {IEEE}
}
@article{rumelhart1986learning,
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year = 1986,
	title = {Learning representations by back-propagating errors},
	journal = {nature},
	publisher = {Nature Publishing Group},
	volume = 323,
	number = 6088,
	pages = {533--536}
}
@inproceedings{lecun1988theoretical,
	author = {LeCun, Yann and Touresky, D and Hinton, G and Sejnowski, T},
	year = 1988,
	title = {A theoretical framework for back-propagation},
	booktitle = {Proceedings of the 1988 connectionist models summer school},
	volume = 1,
	pages = {21--28},
	organization = {CMU, Pittsburgh, Pa: Morgan Kaufmann}
}
@article{liu1989limited,
	author = {Liu, Dong C and Nocedal, Jorge},
	year = 1989,
	title = {On the limited memory BFGS method for large scale optimization},
	journal = {Mathematical programming},
	publisher = {Springer},
	volume = 45,
	number = {1-3},
	pages = {503--528}
}
@article{lecun1995convolutional,
	author = {LeCun, Yann and Bengio, Yoshua and others},
	year = 1995,
	title = {Convolutional networks for images, speech, and time series},
	journal = {The handbook of brain theory and neural networks},
	volume = 3361,
	number = 10,
	pages = 1995
}
@article{mcculloch1943logical,
	author = {McCulloch, Warren S and Pitts, Walter},
	year = 1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	journal = {The bulletin of mathematical biophysics},
	publisher = {Springer},
	volume = 5,
	number = 4,
	pages = {115--133}
}
@article{linnainmaa1970representation,
	author = {Linnainmaa, Seppo},
	year = 1970,
	title = {The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors},
	journal = {Master's Thesis (in Finnish), Univ. Helsinki},
	pages = {6--7}
}
@article{kak1993training,
	author = {Kak, Subhash},
	year = 1993,
	title = {On training feedforward neural networks},
	journal = {Pramana},
	publisher = {Springer},
	volume = 40,
	number = 1,
	pages = {35--42}
}
@article{simonyan2014very,
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = 2014,
	title = {Very deep convolutional networks for large-scale image recognition},
	journal = {arXiv preprint arXiv:1409.1556},
	booktitle = {International Conference on Learning Representations}
}
@article{wiyatno2018saliency,
	author = {Rey Wiyatno and Anqi Xu},
	year = 2018,
	title = {Maximal Jacobian-based Saliency Map Attack},
	journal = {CoRR},
	volume = {abs/1808.07945},
	url = {http://arxiv.org/abs/1808.07945},
	archiveprefix = {arXiv},
	eprint = {1808.07945},
	timestamp = {Sun, 02 Sep 2018 15:01:56 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1808-07945.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{Krause20,
	author = {Andreas Krause},
	year = 2020,
	title = {Introduction to Machine Learning},
	month = {August},
	publisher = {Learning and Adaptive Systems ETH}
}
@inproceedings{coates2011analysis,
	author = {Coates, Adam and Ng, Andrew and Lee, Honglak},
	year = 2011,
	title = {An analysis of single-layer networks in unsupervised feature learning},
	booktitle = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
	pages = {215--223},
	organization = {JMLR Workshop and Conference Proceedings}
}
@inproceedings{glorot2011deep,
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	year = 2011,
	title = {Deep sparse rectifier neural networks},
	booktitle = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
	pages = {315--323},
	organization = {JMLR Workshop and Conference Proceedings}
}
@article{collobert2011natural,
	author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	year = 2011,
	title = {Natural language processing (almost) from scratch},
	journal = {Journal of machine learning research},
	volume = 12,
	number = {ARTICLE},
	pages = {2493--2537}
}
@inproceedings{mikolov2010recurrent,
	author = {Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
	year = 2010,
	title = {Recurrent neural network based language model.},
	booktitle = {Interspeech},
	volume = 2,
	number = 3,
	pages = {1045--1048},
	organization = {Makuhari}
}
@article{vincent2010stacked,
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
	year = 2010,
	title = {Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
	journal = {Journal of machine learning research},
	volume = 11,
	number = 12
}
@inproceedings{boureau2010learning,
	author = {Boureau, Y-Lan and Bach, Francis and LeCun, Yann and Ponce, Jean},
	year = 2010,
	title = {Learning mid-level features for recognition},
	booktitle = {2010 IEEE computer society conference on computer vision and pattern recognition},
	pages = {2559--2566},
	organization = {IEEE}
}
@article{hinton2010practical,
	author = {Hinton, Geoffrey},
	year = 2010,
	title = {A practical guide to training restricted Boltzmann machines},
	journal = {Momentum},
	volume = 9,
	number = 1,
	pages = 926
}
@inproceedings{glorot2010understanding,
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = 2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	pages = {249--256},
	organization = {JMLR Workshop and Conference Proceedings}
}
@inproceedings{erhan2010does,
	author = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
	year = 2010,
	title = {Why does unsupervised pre-training help deep learning?},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	pages = {201--208},
	organization = {JMLR Workshop and Conference Proceedings}
}
@article{bengio2009learning,
	author = {Bengio, Yoshua and others},
	year = 2009,
	title = {Learning deep architectures for AI},
	journal = {Foundations and trends{\textregistered} in Machine Learning},
	publisher = {Now Publishers, Inc.},
	volume = 2,
	number = 1,
	pages = {1--127}
}
@inproceedings{lee2009convolutional,
	author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y},
	year = 2009,
	title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
	booktitle = {Proceedings of the 26th annual international conference on machine learning},
	pages = {609--616}
}
@misc{bengio2007greedy,
	author = {Bengio, Y and Lamblin, P and Popovici, D and Larochelle, H},
	year = 2007,
	title = {Greedy layer-wise training of deep networks. NIPS 19 (pp. 153--160)},
	publisher = {MIT Press}
}
@article{hinton2006reducing,
	author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
	year = 2006,
	title = {Reducing the dimensionality of data with neural networks},
	journal = {science},
	publisher = {American Association for the Advancement of Science},
	volume = 313,
	number = 5786,
	pages = {504--507}
}
@article{hinton2006fast,
	author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
	year = 2006,
	title = {A fast learning algorithm for deep belief nets},
	journal = {Neural computation},
	publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209},
	volume = 18,
	number = 7,
	pages = {1527--1554}
}
@article{hochreiter1997long,
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	year = 1997,
	title = {Long short-term memory},
	journal = {Neural computation},
	publisher = {MIT press},
	volume = 9,
	number = 8,
	pages = {1735--1780}
}
and an inexorable expansion of  pre-built and well-maintained
libraries for working with neural networks including the two most
famous: torch 2002 and tensorflow (released 2015)
@inproceedings{Collobert2002TorchAM,
	author = {Ronan Collobert and Samy Bengio and Johnny Mari{\'e}thoz},
	year = 2002,
	title = {Torch: a modular machine learning software library}
}
@incollection{pytorch2019,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = 2019,
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	pages = {8024--8035},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}
@misc{tensorflow2015-whitepaper,
	author = {Mart\'{i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
	year = 2015,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	note = {Software available from tensorflow.org}
}
@misc{vaswani2017attention,
	author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year = 2017,
	title = {Attention Is All You Need},
	eprint = {1706.03762},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{functorch,
	author = {Horace He, Richard Zou},
	year = 2021,
	title = {functorch: JAX-like composable function transforms for PyTorch},
	howpublished = {\url{https://github.com/pytorch/functorch}}
}

@inproceedings{bietti2019bias,
	author = {Bietti, Alberto and Mairal, Julien},
	year = 2019,
	title = {On the Inductive Bias of Neural Tangent Kernels},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 32,
	pages = {},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/c4ef9c39b300931b69a36fb3dbb8d60e-Paper.pdf},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
                  
@inproceedings{du2019graphntk,
	author = {Du, Simon S and Hou, Kangcheng and Salakhutdinov, Russ R and Poczos, Barnabas and Wang, Ruosong and Xu, Keyulu},
	year = 2019,
	title = {Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 32,
	pages = {},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/663fd3c5144fd10bd5ca6611a9a5b92d-Paper.pdf},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@article{halko2011finding,
  title={Finding structure with randomness: Probabilistic algorithms for constructing approximate matrix decompositions},
  author={Halko, Nathan and Martinsson, Per-Gunnar and Tropp, Joel A},
  journal={SIAM review},
  volume={53},
  number={2},
  pages={217--288},
  year={2011},
  publisher={SIAM}
}
@inproceedings{tancik2020fourierfeatures,
	author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
	year = 2020,
	title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 33,
	pages = {7537--7547},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/55053683268957697aa39fba6f231c68-Paper.pdf},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@article{hoover1982high,
	author = {Hoover, William G and Ladd, Anthony JC and Moran, Bill},
	year = 1982,
	title = {High-strain-rate plastic flow studied via nonequilibrium molecular dynamics},
	journal = {Physical Review Letters},
	publisher = {APS},
	volume = 48,
	number = 26,
	pages = 1818
}
@article{evans1983computer,
	author = {Evans, Denis J},
	year = 1983,
	title = {Computer ‘‘experiment’’for nonlinear thermodynamics of Couette flow},
	journal = {The Journal of Chemical Physics},
	publisher = {American Institute of Physics},
	volume = 78,
	number = 6,
	pages = {3297--3302}
}
@article{evans1990computer,
	author = {Evans, DJ and Morriss, GP},
	year = 1990,
	title = {Computer Phys. Rep. 1, 297 (1984). 25 DJ Evans and GP Morris},
	journal = {Statistical Mechanics of Nonequilibrium Liquids}
}
@inproceedings{chizat2020maxmargin,
	author = {Chizat, L\'ena\"ic  and Bach, Francis},
	year = 2020,
	title = {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
	month = {09--12 Jul},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	volume = 125,
	pages = {1305--1338},
	url = {https://proceedings.mlr.press/v125/chizat20a.html},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	pdf = {http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf},
	abstract = {Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.}
}
@article{snelson2005sparse,
	author = {Snelson, Edward and Ghahramani, Zoubin},
	year = 2005,
	title = {Sparse Gaussian processes using pseudo-inputs},
	journal = {Advances in neural information processing systems},
	volume = 18
}
@inproceedings{geifman2020similarity,
	author = {Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Ronen, Basri},
	year = 2020,
	title = {On the {Similarity} between the {Laplace} and {Neural} {Tangent} {Kernels}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	volume = 33,
	pages = {1451--1461},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html},
	urldate = {2023-05-16},
	file = {Full Text PDF:/Users/mgeyer/Zotero/storage/TMZRRUYZ/Geifman et al. - 2020 - On the Similarity between the Laplace and Neural T.pdf:application/pdf}
}
@misc{alemohammad2021recurrent,
	author = {Alemohammad, Sina and Wang, Zichao and Balestriero, Randall and Baraniuk, Richard},
	year = 2021,
	title = {The {Recurrent} {Neural} {Tangent} {Kernel}},
	month = jun,
	publisher = {arXiv},
	doi = {10.48550/arXiv.2006.10246},
	url = {http://arxiv.org/abs/2006.10246},
	urldate = {2023-05-16},
	note = {arXiv:2006.10246 [cs, stat]},
	abstract = {The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN). In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets.},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mgeyer/Zotero/storage/YW47W8N7/Alemohammad et al. - 2021 - The Recurrent Neural Tangent Kernel.pdf:application/pdf;arXiv.org Snapshot:/Users/mgeyer/Zotero/storage/8HB6QBKD/2006.html:text/html}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@techreport{mitchell80,
	author = {T. M. Mitchell},
	year = 1980,
	title = {The Need for Biases in Learning Generalizations},
	address = {New Brunswick, MA},
	institution = {Computer Science Department, Rutgers University}
}
@phdthesis{kearns89,
	author = {M. J. Kearns},
	year = 1989,
	title = {Computational Complexity of Machine Learning},
	school = {Department of Computer Science, Harvard University}
}
@book{MachineLearningI,
	year = 1983,
	title = {Machine Learning: An Artificial Intelligence Approach, Vol. I},
	publisher = {Tioga},
	address = {Palo Alto, CA},
	editor = {R. S. Michalski and J. G. Carbonell and T. M. Mitchell}
}
@book{DudaHart2nd,
	author = {R. O. Duda and P. E. Hart and D. G. Stork},
	year = 2000,
	title = {Pattern Classification},
	publisher = {John Wiley and Sons},
	edition = {2nd}
}
@misc{anonymous,
	author = {Author, N. N.},
	year = 2021,
	title = {Suppressed for Anonymity}
}
@incollection{Newell81,
	author = {A. Newell and P. S. Rosenbloom},
	year = 1981,
	title = {Mechanisms of Skill Acquisition and the Law of Practice},
	booktitle = {Cognitive Skills and Their Acquisition},
	publisher = {Lawrence Erlbaum Associates, Inc.},
	address = {Hillsdale, NJ},
	pages = {1--51},
	editor = {J. R. Anderson},
	chapter = 1
}
@article{Samuel59,
	author = {A. L. Samuel},
	year = 1959,
	title = {Some Studies in Machine Learning Using the Game of Checkers},
	journal = {IBM Journal of Research and Development},
	volume = 3,
	number = 3,
	pages = {211--229}
}
@article{wang2022pinns,
	author = {Sifan Wang and Xinling Yu and Paris Perdikaris},
	year = 2022,
	title = {When and why PINNs fail to train: A neural tangent kernel perspective},
	journal = {Journal of Computational Physics},
	volume = 449,
	pages = 110768,
	doi = {https://doi.org/10.1016/j.jcp.2021.110768},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
	keywords = {Physics-informed neural networks, Spectral bias, Multi-task learning, Gradient descent, Scientific machine learning},
	abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/PINNsNTK.}
}
@inproceedings{DBLP:conf/nips/BubeckS21,
	author = {S{\'{e}}bastien Bubeck and Mark Sellke},
	year = 2021,
	title = {A Universal Law of Robustness via Isoperimetry},
	booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual},
	pages = {28811--28822},
	url = {https://proceedings.neurips.cc/paper/2021/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html},
	editor = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann N. Dauphin and Percy Liang and Jennifer Wortman Vaughan},
	timestamp = {Tue, 03 May 2022 16:20:49 +0200},
	biburl = {https://dblp.org/rec/conf/nips/BubeckS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
                  
@article{neal1996priors,
	author = {Neal, Radford M and Neal, Radford M},
	year = 1996,
	title = {Priors for infinite networks},
	journal = {Bayesian learning for neural networks},
	publisher = {Springer},
	pages = {29--53}
}
@inproceedings{tramer2020adaptive,
	author = {Florian Tram{\`{e}}r and Nicholas Carlini and Wieland Brendel and Aleksander Madry},
	title = {On Adaptive Attacks to Adversarial Example Defenses},
	booktitle = {Advances in Neural Information Processing Systems 33 ({NeurIPS} 2020), virtual},
	editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin}
}
@inproceedings{tramer2018ensemble,
	author = {Florian Tram{\`{e}}r and Alexey Kurakin and Nicolas Papernot and Ian J. Goodfellow and Dan Boneh and Patrick D. McDaniel},
	title = {Ensemble Adversarial Training: Attacks and Defenses},
	booktitle = {6th International Conference on Learning Representations, ({ICLR} 2018), Vancouver, BC, Canada}
}
% dynamical clustering paper
@inproceedings{qin2020,
	author = {Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison W. Cottrell and Geoffrey E. Hinton},
	title = {Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},
	booktitle = {8th International Conference on Learning Representations, ({ICLR} 2020), Addis Ababa, Ethiopia}
}
%%%%#######%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{roth19aodds,
	author = {Roth, Kevin and Kilcher, Yannic and Hofmann, Thomas},
	year = 2019,
	title = {The Odds are Odd: A Statistical Test for Detecting Adversarial Examples},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning ({ICML})},
	volume = 97,
	pages = {5498--5507},
	editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
	abstract = {We investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.}
}
@article{hosseini2019odds,
	author = {Hossein Hosseini and Sreeram Kannan and Radha Poovendran},
	year = 2019,
	title = {Are Odds Really Odd? Bypassing Statistical Detection of Adversarial Examples},
	journal = {CoRR},
	volume = {abs/1907.12138},
	url = {http://arxiv.org/abs/1907.12138},
	archiveprefix = {arXiv},
	eprint = {1907.12138},
	timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1907-12138.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{kim2021torchattacks,
	author = {Hoki Kim},
	year = 2020,
	title = {Torchattacks : {A} Pytorch Repository for Adversarial Attacks},
	journal = {CoRR},
	volume = {abs/2010.01950}
}
@inproceedings{yu2019new,
	author = {Shengyuan Hu and Tao Yu and Chuan Guo and Wei{-}Lun Chao and Kilian Q. Weinberger},
	title = {A New Defense Against Adversarial Images: Turning a Weakness into a Strength},
	booktitle = {Advances in Neural Information Processing Systems 32 ({NeurIPS} 2019) Vancouver, BC, Canada},
	pages = {1633--1644},
	editor = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett},
	year = 2019
}
@inproceedings{aggsurprising,
	author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
	year = 2001,
	title = {On the Surprising Behavior of Distance Metrics in High Dimensional Space},
	booktitle = {Database Theory --- ICDT 2001},
	publisher = {Springer Berlin Heidelberg},
	address = {Berlin, Heidelberg},
	pages = {420--434},
	isbn = {978-3-540-44503-6},
	editor = {Van den Bussche, Jan and Vianu, Victor}
}
@article{MNIST,
	author = {LeCun, Yann and Cortes, Corinna},
	year = 2010,
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	added-at = {2010-06-28T21:16:30.000+0200},
	biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
	groups = {public},
	howpublished = {http://yann.lecun.com/exdb/mnist/},
	interhash = {21b9d0558bd66279df9452562df6e6f3},
	intrahash = {935bad99fa1f65e03c25b315aa3c1032},
	keywords = {MSc _checked character_recognition mnist network neural},
	lastchecked = {2016-01-14 14:24:11},
	timestamp = {2016-07-12T19:25:30.000+0200},
	username = {mhwombat}
}
@inproceedings{Imagenet-old,
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = 2009,
	title = {Imagenet: A large-scale hierarchical image database},
	booktitle = {2009 IEEE conference on computer vision and pattern recognition},
	pages = {248--255},
	organization = {IEEE}
}
@article{ILSVRC15,
	author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
	year = 2015,
	title = {{ImageNet Large Scale Visual Recognition Challenge}},
	journal = {International Journal of Computer Vision (IJCV)},
	volume = 115,
	number = 3,
	pages = {211--252},
	doi = {10.1007/s11263-015-0816-y}
}
@article{shamir2021,
	author = {Shamir, Adi},
	year = 2021,
	title = {A new theory of adversarial examples in machine learning (a non-technical extended abstract)},
	journal = {preprint}
}
@inproceedings{Fawzi2018empirical,
	author = {Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal and Soatto, Stefano},
	year = 2018,
	title = {Empirical Study of the Topology and Geometry of Deep Networks},
	month = {June},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@incollection{ledoux1999concentration,
	author = {Ledoux, Michel},
	year = 1999,
	title = {Concentration of measure and logarithmic Sobolev inequalities},
	booktitle = {Seminaire de probabilites XXXIII},
	publisher = {Springer},
	pages = {120--216}
}
@incollection{ledoux1996isoperimetry,
	author = {Ledoux, Michel},
	year = 1996,
	title = {Isoperimetry and Gaussian analysis},
	booktitle = {Lectures on probability theory and statistics},
	publisher = {Springer},
	pages = {165--294}
}
@article{wegner2021lecture,
	author = {Wegner, Sven-Ake},
	year = 2021,
	title = {Lecture notes on high-dimensional spaces},
	journal = {arXiv preprint arXiv:2101.05841}
}
@book{blum_hopcroft_kannan_2020,
	author = {Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
	year = 2020,
	title = {Foundations of Data Science},
	publisher = {Cambridge University Press},
	doi = {10.1017/9781108755528},
	place = {Cambridge}
}
@book{morvan,
	author = {Morvan, Jean-Marie},
	year = 2008,
	title = {Generalized Curvatures},
	publisher = {Springer Publishing Company, Incorporated},
	abstract = {The intent of this book is to set the modern foundations of the theory of generalized curvature measures. This subject has a long history, beginning with J. Steiner (1850), H. Weyl (1939), H. Federer (1959), P. Wintgen (1982), and continues today with young and brilliant mathematicians. In the last decades, a renewal of interest in mathematics as well as computer science has arisen (finding new applications in computer graphics, medical imaging, computational geometry, visualization ). Following a historical and didactic approach, the book introduces the mathematical background of the subject, beginning with curves and surfaces, going on with convex subsets, smooth submanifolds, subsets of positive reach, polyhedra and triangulations, and ending with surface reconstruction. We focus on the theory of normal cycle, which allows to compute and approximate curvature measures of a large class of smooth or discrete objects of the Euclidean space. We give explicit computations when the object is a 2 or 3 dimensional polyhedron. This book can serve as a textbook to any mathematician or computer scientist, engineer or researcher who is interested in the theory of curvature measures.}
}
@conference{crecchi2019,
	author = {Francesco Crecchi and Davide Bacciu and Battista Biggio},
	year = 2019,
	title = {Detecting Adversarial Examples through Nonlinear Dimensionality Reduction},
	booktitle = {27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning - ESANN {\textquoteright}19},
	pages = {483--488}
}
@inproceedings{alexnet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = 2012,
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = 25,
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger}
}
@inproceedings{dongMIFGSM,
	author = {Yinpeng Dong and Fangzhou Liao and Tianyu Pang and Hang Su and Jun Zhu and Xiaolin Hu and Jianguo Li},
	title = {Boosting Adversarial Attacks With Momentum},
	booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition, ({CVPR}) 2018, Salt Lake City, UT, USA, June 18-22, 2018},
	pages = {9185--9193}
}
@article{shamir2021dimpled,
  author       = {Adi Shamir and
                  Odelia Melamed and
                  Oriel BenShmuel},
  title        = {The Dimpled Manifold Model of Adversarial Examples in Machine Learning},
  journal      = {CoRR},
  volume       = {abs/2106.10151},
  year         = {2021},
  url          = {https://arxiv.org/abs/2106.10151},
  eprinttype    = {arXiv},
  eprint       = {2106.10151},
  timestamp    = {Tue, 29 Jun 2021 16:55:04 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2106-10151.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{stutz2019disentangling,
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	year = 2019,
	title = {Disentangling adversarial robustness and generalization},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages = {6976--6987}
}
@article{ganz2022perceptually,
	author = {Ganz, Roy and Kawar, Bahjat and Elad, Michael},
	year = 2022,
	title = {Do Perceptually Aligned Gradients Imply Adversarial Robustness?},
	journal = {arXiv preprint arXiv:2207.11378}
}
@article{baehrens2010explain,
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\"u}ller, Klaus-Robert},
	year = 2010,
	title = {How to explain individual classification decisions},
	journal = {The Journal of Machine Learning Research},
	publisher = {JMLR. org},
	volume = 11,
	pages = {1803--1831}
}
@article{adebayo2018sanity,
	author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	year = 2018,
	title = {Sanity checks for saliency maps},
	journal = {Advances in neural information processing systems},
	volume = 31
}
                  
@article{tjeng2017evaluating,
	author = {Tjeng, Vincent and Xiao, Kai and Tedrake, Russ},
	year = 2017,
	title = {Evaluating robustness of neural networks with mixed integer programming},
	journal = {arXiv preprint arXiv:1711.07356}
}
@article{singh2018fast,
	author = {Singh, Gagandeep and Gehr, Timon and Mirman, Matthew and P{\"u}schel, Markus and Vechev, Martin},
	year = 2018,
	title = {Fast and effective robustness certification},
	journal = {Advances in neural information processing systems},
	volume = 31
}
@inproceedings{xie2017aggregated,
	author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
	year = 2017,
	title = {Aggregated residual transformations for deep neural networks},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages = {1492--1500}
}
@article{akhtar2018threat,
	author = {Akhtar, Naveed and Mian, Ajmal},
	year = 2018,
	title = {Threat of adversarial attacks on deep learning in computer vision: A survey},
	journal = {Ieee Access},
	publisher = {IEEE},
	volume = 6,
	pages = {14410--14430}
}
@article{tramer2019adversarial,
	author = {Tramer, Florian and Boneh, Dan},
	year = 2019,
	title = {Adversarial training and robustness for multiple perturbations},
	journal = {Advances in Neural Information Processing Systems},
	volume = 32
}
@article{jo2017measuring,
	author = {Jo, Jason and Bengio, Yoshua},
	year = 2017,
	title = {Measuring the tendency of cnns to learn surface statistical regularities},
	journal = {arXiv preprint arXiv:1711.11561}
}
@article{geirhos2018imagenet,
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
	year = 2018,
	title = {ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
	journal = {arXiv preprint arXiv:1811.12231}
}
@article{kaur2019perceptually,
	author = {Kaur, Simran and Cohen, Jeremy and Lipton, Zachary C},
	year = 2019,
	title = {Are perceptually-aligned gradients a general property of robust classifiers?},
	journal = {arXiv preprint arXiv:1910.08640}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{vardi2022gradient,
	author = {Gal Vardi and Gilad Yehudai and Ohad Shamir},
	year = 2022,
	title = {Gradient Methods Provably Converge to Non-Robust Networks},
	eprint = {2202.04347},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{lecuyer2019certified,
	author = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
	year = 2019,
	title = {Certified robustness to adversarial examples with differential privacy},
	booktitle = {2019 IEEE Symposium on Security and Privacy (SP)},
	pages = {656--672},
	organization = {IEEE}
}
@article{li2019certified,
	author = {Li, Bai and Chen, Changyou and Wang, Wenlin and Carin, Lawrence},
	year = 2019,
	title = {Certified adversarial robustness with additive noise},
	journal = {Advances in neural information processing systems},
	volume = 32
}
@article{blum2020random,
	author = {Blum, Avrim and Dick, Travis and Manoj, Naren and Zhang, Hongyang},
	year = 2020,
	title = {Random smoothing might be unable to certify $L^{\infty}$ robustness for high-dimensional images},
	journal = {The Journal of Machine Learning Research},
	publisher = {JMLRORG},
	volume = 21,
	number = 1,
	pages = {8726--8746}
}
@inproceedings{kumar2020curse,
	author = {Kumar, Aounon and Levine, Alexander and Goldstein, Tom and Feizi, Soheil},
	year = 2020,
	title = {Curse of dimensionality on randomized smoothing for certifiable robustness},
	booktitle = {International Conference on Machine Learning},
	pages = {5458--5467},
	organization = {PMLR}
}
@inproceedings{yang2020randomized,
	author = {Yang, Greg and Duan, Tony and Hu, J Edward and Salman, Hadi and Razenshteyn, Ilya and Li, Jerry},
	year = 2020,
	title = {Randomized smoothing of all shapes and sizes},
	booktitle = {International Conference on Machine Learning},
	pages = {10693--10705},
	organization = {PMLR}
}
@inproceedings{cohen2019certified,
	author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	year = 2019,
	title = {Certified adversarial robustness via randomized smoothing},
	booktitle = {international conference on machine learning},
	pages = {1310--1320},
	organization = {PMLR}
}
@misc{blau2023classifier,
	author = {Tsachi Blau and Roy Ganz and Chaim Baskin and Michael Elad and Alex Bronstein},
	year = 2023,
	title = {Classifier Robustness Enhancement Via Test-Time Transformation},
	eprint = {2303.15409},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@inproceedings{nguyen-minh-luu-2022-textual,
	author = {Nguyen Minh, Dang  and Luu, Anh Tuan},
	year = 2022,
	title = {Textual Manifold-based Defense Against Natural Language Adversarial Examples},
	month = dec,
	booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	address = {Abu Dhabi, United Arab Emirates},
	pages = {6612--6625},
	url = {https://aclanthology.org/2022.emnlp-main.443},
	abstract = {Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this study, we find a similar phenomenon occurs in the contextualized embedding space of natural sentences induced by pretrained language models in which textual adversarial examples tend to have their embeddings diverge off the manifold of natural sentence embeddings. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that learns the embedding space manifold of the underlying language model and projects novel inputs back to the approximated structure before classification. Through extensive experiments, we find that our method consistently and significantly outperforms previous defenses under various attack settings while remaining unaffected to the clean accuracy. To the best of our knowledge, this is the first kind of manifold-based defense adapted to the NLP domain.}
}
@inproceedings{Osada_2023_WACV,
	author = {Osada, Genki and Takahashi, Tsubasa and Ahsan, Budrul and Nishide, Takashi},
	year = 2023,
	title = {Out-of-Distribution Detection With Reconstruction Error and Typicality-Based Penalty},
	month = {January},
	booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
	pages = {5551--5563}
}
@misc{melamed2023adversarial,
	author = {Odelia Melamed and Gilad Yehudai and Gal Vardi},
	year = 2023,
	title = {Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Data Manifolds},
	eprint = {2303.00783},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{magai2022topology,
	author = {German Magai and Anton Ayzenberg},
	year = 2022,
	title = {Topology and geometry of data manifold in deep learning},
	eprint = {2204.08624},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@techreport{aparne2022pca,
	author = {Aparne, Gupta and Banburski, Andrzej and Poggio, Tomaso},
	year = 2022,
	title = {PCA as a defense against some adversaries},
	institution = {Center for Brains, Minds and Machines (CBMM)}
}
@inproceedings{lu2022randommasking,
	author = {Lu, Zhiping and Hu, Hongchao and Huo, Shumin and Li, Shuyi},
	year = 2022,
	title = {MR2D: Multiple Random Masking Reconstruction Adversarial Detector},
	booktitle = {2022 10th International Conference on Information Systems and Computing Technology (ISCTech)},
	volume = {},
	number = {},
	pages = {61--67},
	doi = {10.1109/ISCTech58360.2022.00016}
}
@article{jin2022roby,
	author = {Haibo Jin and Jinyin Chen and Haibin Zheng and Zhen Wang and Jun Xiao and Shanqing Yu and Zhaoyan Ming},
	year = 2022,
	title = {ROBY: Evaluating the adversarial robustness of a deep model by its decision boundaries},
	journal = {Information Sciences},
	volume = 587,
	pages = {97--122},
	doi = {https://doi.org/10.1016/j.ins.2021.12.021},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521012421},
	keywords = {Robustness evaluation, Deep learning, Deep neural network, Decision boundaries},
	abstract = {With the successful applications of DNNs in many real-world tasks, model’s robustness has raised public concern. Recently the robustness of deep models is often evaluated by purposely generated adversarial samples, which is time-consuming and usually dependent on the specific attacks and model structures. Addressing the problem, we propose a generic evaluation metric ROBY, a novel attack-independent robustness measurement based on the model’s feature distribution. Without prior knowledge of adversarial samples, ROBY uses inter-class and intra-class statistics to capture the features in the latent space. Models with stronger robustness always have larger distances between classes and smaller distances in the same class. Comprehensive experiments have been conducted on ten state-of-the-art deep models and different datasets to verify ROBY’s effectiveness and efficiency. Compared with other evaluation metrics, ROBY better matches the robustness golden standard attack success rate (ASR), with significantly less computation cost. To the best of our knowledge, ROBY is the first light-weighted attack-independent robustness evaluation metric general to a wide range of deep models. The code of it can be downloaded at https://github.com/Allen-piexl/ROBY.}
}
@inproceedings{carmon2019unlabeled,
	author = {Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
	year = 2019,
	title = {Unlabeled Data Improves Adversarial Robustness},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 32,
	pages = {},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{taori2020shifts,
	author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
	year = 2020,
	title = {Measuring Robustness to Natural Distribution Shifts in Image Classification},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 33,
	pages = {18583--18599},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@inproceedings{Wang2020Improving,
	author = {Yisen Wang and Difan Zou and Jinfeng Yi and James Bailey and Xingjun Ma and Quanquan Gu},
	year = 2020,
	title = {Improving Adversarial Robustness Requires Revisiting Misclassified Examples},
	booktitle = {International Conference on Learning Representations},
	url = {https://openreview.net/forum?id=rklOg6EFwS}
}
@inproceedings{he2018decision,
	author = {Warren He and Bo Li and Dawn Song},
	year = 2018,
	title = {Decision Boundary Analysis of Adversarial Examples},
	booktitle = {International Conference on Learning Representations},
	url = {https://openreview.net/forum?id=BkpiPMbA-}
}
@misc{xu2023exploring,
	author = {Yuancheng Xu and Yanchao Sun and Micah Goldblum and Tom Goldstein and Furong Huang},
	year = 2023,
	title = {Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness},
	eprint = {2302.03015},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{chen2023aware,
	author = {Chen, Chen and Zhang, Jingfeng and Xu, Xilie and Lyu, Lingjuan and Chen, Chaochao and Hu, Tianlei and Chen, Gang},
	year = 2023,
	title = {Decision Boundary-Aware Data Augmentation for Adversarial Training},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	volume = 20,
	number = 3,
	pages = {1882--1894},
	doi = {10.1109/TDSC.2022.3165889}
}
                  

 @inproceedings{Ansuini_Laio_Macke_Zoccolan_2019, title={Intrinsic dimension of data representations in deep neural networks}, volume={32}, url={https://proceedings.neurips.cc/paper/2019/hash/cfcce0621b49c983991ead4c3d4d3b6b-Abstract.html}, abstractNote={Deep neural networks progressively transform their inputs across multiple processing layers. What are the geometrical properties of the representations learned by these networks? Here we study the intrinsic dimensionality (ID) of data
representations, i.e. the minimal number of parameters needed to describe a representation. We find that, in a trained network, the ID is orders of magnitude smaller than the number of units in each layer. Across layers, the ID first increases and then progressively decreases in the final layers. Remarkably, the ID of the last hidden layer predicts classification accuracy on the test set. These results can neither be found by linear dimensionality estimates (e.g., with principal component analysis), nor in representations that had been artificially linearized. They are neither found in untrained networks, nor in networks that are trained on randomized labels. This suggests that neural networks that can generalize are those that transform the data into low-dimensional, but not necessarily flat manifolds.}, booktitle={Advances in Neural Information Processing Systems}, publisher={Curran Associates, Inc.}, author={Ansuini, Alessio and Laio, Alessandro and Macke, Jakob H and Zoccolan, Davide}, year={2019} }
 @article{ceruti2012,
  author       = {Claudio Ceruti and
                  Simone Bassis and
                  Alessandro Rozza and
                  Gabriele Lombardi and
                  Elena Casiraghi and
                  Paola Campadelli},
  title        = {DANCo: Dimensionality from Angle and Norm Concentration},
  journal      = {CoRR},
  volume       = {abs/1206.3881},
  year         = {2012},
  url          = {http://arxiv.org/abs/1206.3881},
  eprinttype    = {arXiv},
  eprint       = {1206.3881},
  timestamp    = {Mon, 13 Aug 2018 16:47:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1206-3881.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
 @article{Costa_Hero_2004a, title={Geodesic entropic graphs for dimension and entropy estimation in manifold learning}, volume={52}, ISSN={1941-0476}, DOI={10.1109/TSP.2004.831130}, abstractNote={In the manifold learning problem, one seeks to discover a smooth low dimensional surface, i.e., a manifold embedded in a higher dimensional linear vector space, based on a set of measured sample points on the surface. In this paper, we consider the closely related problem of estimating the manifold’s intrinsic dimension and the intrinsic entropy of the sample points. Specifically, we view the sample points as realizations of an unknown multivariate density supported on an unknown smooth manifold. We introduce a novel geometric approach based on entropic graph methods. Although the theory presented applies to this general class of graphs, we focus on the geodesic-minimal-spanning-tree (GMST) to obtaining asymptotically consistent estimates of the manifold dimension and the Re/spl acute/nyi /spl alpha/-entropy of the sample density on the manifold. The GMST approach is striking in its simplicity and does not require reconstruction of the manifold or estimation of the multivariate density of the samples. The GMST method simply constructs a minimal spanning tree (MST) sequence using a geodesic edge matrix and uses the overall lengths of the MSTs to simultaneously estimate manifold dimension and entropy. We illustrate the GMST approach on standard synthetic manifolds as well as on real data sets consisting of images of faces.}, number={8}, journal={IEEE Transactions on Signal Processing}, author={Costa, J.A. and Hero, A.O.}, year={2004}, month={Aug}, pages={2210–2221} }
 @inproceedings{Costa_Hero_2004b, title={Learning intrinsic dimension and intrinsic entropy of high-dimensional datasets}, abstractNote={Populations of measurements of objects such as faces, genes or internet data traces, lie in lower dimensional manifolds of their high dimensional embedding spaces, e.g. face images, gene microarrays, or multivariate time series records. Knowing the intrinsic dimension and relative entropy of these manifolds is important for discovering structure, classifying differences, or performing dimensionality reduction (compression). In this paper we apply a new family of entropic graph methods to the estimation of intrinsic dimension and entropy of datasets supported on synthetic manifolds and of a high dimensional dataset of handwritten digits.}, booktitle={2004 12th European Signal Processing Conference}, author={Costa, Jose A. and Hero, Alfred O.}, year={2004}, month={Sep}, pages={369–372} }
 @inproceedings{Dalal_Triggs_2005, title={Histograms of oriented gradients for human detection}, volume={1}, ISSN={1063-6919}, DOI={10.1109/CVPR.2005.177}, abstractNote={We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.}, booktitle={2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR’05)}, author={Dalal, N. and Triggs, B.}, year={2005}, month={Jun}, pages={886–893 vol. 1} }
 @inproceedings{gong2019,
  author       = {Sixue Gong and
                  Vishnu Naresh Boddeti and
                  Anil K. Jain},
  title        = {On the Intrinsic Dimensionality of Image Representations},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
                  2019, Long Beach, CA, USA, June 16-20, 2019},
  pages        = {3987--3996},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2019},
  url          = {http://openaccess.thecvf.com/content\_CVPR\_2019/html/Gong\_On\_the\_Intrinsic\_Dimensionality\_of\_Image\_Representations\_CVPR\_2019\_paper.html},
  doi          = {10.1109/CVPR.2019.00411},
  timestamp    = {Mon, 30 Aug 2021 17:01:14 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/GongB019.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{khoury2018,
  author       = {Marc Khoury and
                  Dylan Hadfield{-}Menell},
  title        = {On the Geometry of Adversarial Examples},
  journal      = {CoRR},
  volume       = {abs/1811.00525},
  year         = {2018},
  url          = {http://arxiv.org/abs/1811.00525},
  eprinttype    = {arXiv},
  eprint       = {1811.00525},
  timestamp    = {Thu, 22 Nov 2018 17:58:30 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1811-00525.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
 @article{Khoury_Hadfield-Menell_2018, title={On the Geometry of Adversarial Examples}, url={http://arxiv.org/abs/1811.00525}, DOI={10.48550/arXiv.1811.00525}, abstractNote={Adversarial examples are a pervasive phenomenon of machine learning models where seemingly imperceptible perturbations to the input lead to misclassifications for otherwise statistically accurate models. We propose a geometric framework, drawing on tools from the manifold reconstruction literature, to analyze the high-dimensional geometry of adversarial examples. In particular, we highlight the importance of codimension: for low-dimensional data manifolds embedded in high-dimensional space there are many directions off the manifold in which to construct adversarial examples. Adversarial examples are a natural consequence of learning a decision boundary that classifies the low-dimensional data manifold well, but classifies points near the manifold incorrectly. Using our geometric framework we prove (1) a tradeoff between robustness under different norms, (2) that adversarial training in balls around the data is sample inefficient, and (3) sufficient sampling conditions under which nearest neighbor classifiers and ball-based adversarial training are robust.}, note={arXiv:1811.00525 [cs, stat]}, number={arXiv:1811.00525}, publisher={arXiv}, author={Khoury, Marc and Hadfield-Menell, Dylan}, year={2018}, month={Dec} }
 @inproceedings{Levina_Bickel_2004, title={Maximum Likelihood Estimation of Intrinsic Dimension}, volume={17}, url={https://proceedings.neurips.cc/paper_files/paper/2004/hash/74934548253bcab8490ebd74afed7031-Abstract.html}, abstractNote={We propose a new method for estimating intrinsic dimension of a dataset derived by applying the principle of maximum likelihood to the distances between close neighbors. We derive the estimator by a Poisson process approximation, assess its bias and variance theo- retically and by simulations, and apply it to a number of simulated and real datasets. We also show it has the best overall performance compared with two other intrinsic dimension estimators.}, booktitle={Advances in Neural Information Processing Systems}, publisher={MIT Press}, author={Levina, Elizaveta and Bickel, Peter}, year={2004} }
 @inproceedings{talwalker2008, title={Large-scale manifold learning}, ISSN={1063-6919}, DOI={10.1109/CVPR.2008.4587670}, abstractNote={This paper examines the problem of extracting low-dimensional manifold structure given millions of high-dimensional face images. Specifically, we address the computational challenges of nonlinear dimensionality reduction via Isomap and Laplacian Eigenmaps, using a graph containing about 18 million nodes and 65 million edges. Since most manifold learning techniques rely on spectral decomposition, we first analyze two approximate spectral decomposition techniques for large dense matrices (Nystrom and column-sampling), providing the first direct theoretical and empirical comparison between these techniques. We next show extensive experiments on learning low-dimensional embeddings for two large face datasets: CMU-PIE (35 thousand faces) and a web dataset (18 million faces). Our comparisons show that the Nystrom approximation is superior to the column-sampling method. Furthermore, approximate Isomap tends to perform better than Laplacian Eigenmaps on both clustering and classification with the labeled CMU-PIE dataset.}, booktitle={2008 IEEE Conference on Computer Vision and Pattern Recognition}, author={Talwalkar, Ameet and Kumar, Sanjiv and Rowley, Henry}, year={2008}, month={Jun}, pages={1–8} }
 @article{Turk_Pentland_1991, title={Eigenfaces for Recognition}, volume={3}, ISSN={0898-929X}, DOI={10.1162/jocn.1991.3.1.71}, abstractNote={We have developed a near-real-time computer system that can locate and track a subject’s head, and then recognize the person by comparing characteristics of the face to those of known individuals. The computational approach taken in this system is motivated by both physiology and information theory, as well as by the practical requirements of near-real-time performance and accuracy. Our approach treats the face recognition problem as an intrinsically two-dimensional (2-D) recognition problem rather than requiring recovery of three-dimensional geometry, taking advantage of the fact that faces are normally upright and thus may be described by a small set of 2-D characteristic views. The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as “eigenfaces,” because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.}, number={1}, journal={Journal of Cognitive Neuroscience}, author={Turk, Matthew and Pentland, Alex}, year={1991}, month={Jan}, pages={71–86} }
 @article{Zheng_He_Qiu_Wipf_2022, title={Learning Manifold Dimensions with Conditional Variational Autoencoders}, volume={35}, journal={Advances in Neural Information Processing Systems}, author={Zheng, Yijia and He, Tong and Qiu, Yixuan and Wipf, David P.}, year={2022}, month={Dec}, pages={34709–34721}, language={en} }
@article{baehrens2010explain,
  author       = {David Baehrens and
                  Timon Schroeter and
                  Stefan Harmeling and
                  Motoaki Kawanabe and
                  Katja Hansen and
                  Klaus{-}Robert M{\"{u}}ller},
  title        = {How to Explain Individual Classification Decisions},
  journal      = {J. Mach. Learn. Res.},
  volume       = {11},
  pages        = {1803--1831},
  year         = {2010},
  url          = {https://dl.acm.org/doi/10.5555/1756006.1859912},
  doi          = {10.5555/1756006.1859912},
  timestamp    = {Thu, 02 Jun 2022 13:58:57 +0200},
  biburl       = {https://dblp.org/rec/journals/jmlr/BaehrensSHKHM10.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{song2018pixeldefend,
  author       = {Yang Song and
                  Taesup Kim and
                  Sebastian Nowozin and
                  Stefano Ermon and
                  Nate Kushman},
  title        = {PixelDefend: Leveraging Generative Models to Understand and Defend
                  against Adversarial Examples},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=rJUYGxbCW},
  timestamp    = {Thu, 01 Oct 2020 17:52:58 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/SongKNEK18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{chen2021equivalence,
  title={On the equivalence between neural network and support vector machine},
  author={Chen, Yilan and Huang, Wei and Nguyen, Lam and Weng, Tsui-Wei},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={23478--23490},
  year={2021}
}

@incollection{pytorch,
title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
booktitle = {Advances in Neural Information Processing Systems 32},
pages = {8024--8035},
year = {2019},
publisher = {Curran Associates, Inc.},
}

@Misc{functorch,
  author =       {Horace He, Richard Zou},
  title =        {functorch: JAX-like composable function transforms for PyTorch},
  howpublished = {\url{https://github.com/pytorch/functorch}},
  year =         {2021}
}


@book{rasmussen2006gaussian,
  title={Gaussian processes for machine learning},
  author={Rasmussen, Carl Edward and Williams, Christopher KI and others},
  volume={1},
  year={2006},
  publisher={Springer}
}

@article{abdar2021uq,
title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
journal = {Information Fusion},
volume = {76},
pages = {243-297},
year = {2021},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2021.05.008},
url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.}
}
@inproceedings{tsipras2019robustness,
  author       = {Dimitris Tsipras and
                  Shibani Santurkar and
                  Logan Engstrom and
                  Alexander Turner and
                  Aleksander Madry},
  title        = {Robustness May Be at Odds with Accuracy},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=SyxAb30cY7},
  timestamp    = {Thu, 25 Jul 2019 14:26:02 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/TsiprasSETM19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@incollection{kindermans2019reliability,
  author       = {Pieter{-}Jan Kindermans and
                  Sara Hooker and
                  Julius Adebayo and
                  Maximilian Alber and
                  Kristof T. Sch{\"{u}}tt and
                  Sven D{\"{a}}hne and
                  Dumitru Erhan and
                  Been Kim},
  editor       = {Wojciech Samek and
                  Gr{\'{e}}goire Montavon and
                  Andrea Vedaldi and
                  Lars Kai Hansen and
                  Klaus{-}Robert M{\"{u}}ller},
  title        = {The (Un)reliability of Saliency Methods},
  booktitle    = {Explainable {AI:} Interpreting, Explaining and Visualizing Deep Learning},
  series       = {Lecture Notes in Computer Science},
  volume       = {11700},
  pages        = {267--280},
  publisher    = {Springer},
  year         = {2019},
  url          = {https://doi.org/10.1007/978-3-030-28954-6\_14},
  doi          = {10.1007/978-3-030-28954-6\_14},
  timestamp    = {Fri, 09 Apr 2021 18:19:01 +0200},
  biburl       = {https://dblp.org/rec/series/lncs/KindermansHAASDEK19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}




@article{burgess1996estimating,
  title={Estimating equivalent kernels For neural networks: A data perturbation approach},
  author={Burgess, A},
  journal={Advances in Neural Information Processing Systems},
  volume={9},
  year={1996}
}

@misc{incudini2022quantum,
      title={The Quantum Path Kernel: a Generalized Quantum Neural Tangent Kernel for Deep Quantum Machine Learning}, 
      author={Massimiliano Incudini and Michele Grossi and Antonio Mandarino and Sofia Vallecorsa and Alessandra Di Pierro and David Windridge},
      year={2022},
      eprint={2212.11826},
      archivePrefix={arXiv},
      primaryClass={quant-ph}
}

@article{hoover1982high,
  title={High-strain-rate plastic flow studied via nonequilibrium molecular dynamics},
  author={Hoover, William G and Ladd, Anthony JC and Moran, Bill},
  journal={Physical Review Letters},
  volume={48},
  number={26},
  pages={1818},
  year={1982},
  publisher={APS}
}

@article{evans1983computer,
  title={Computer ‘‘experiment’’for nonlinear thermodynamics of Couette flow},
  author={Evans, Denis J},
  journal={The Journal of Chemical Physics},
  volume={78},
  number={6},
  pages={3297--3302},
  year={1983},
  publisher={American Institute of Physics}
}

@article{evans1990computer,
  title={Computer Phys. Rep. 1, 297 (1984). 25 DJ Evans and GP Morris},
  author={Evans, DJ and Morriss, GP},
  journal={Statistical Mechanics of Nonequilibrium Liquids},
  year={1990}
}

@article{shah2021input,
  title={Do input gradients highlight discriminative features?},
  author={Shah, Harshay and Jain, Prateek and Netrapalli, Praneeth},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={2046--2059},
  year={2021}
}


@inproceedings{lin2020gradient,
  title={On gradient descent ascent for nonconvex-concave minimax problems},
  author={Lin, Tianyi and Jin, Chi and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={6083--6093},
  year={2020},
  organization={PMLR}
}

@misc{neuralode2018,
  doi = {10.48550/ARXIV.1806.07366},
  
  url = {https://arxiv.org/abs/1806.07366},
  
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Neural Ordinary Differential Equations},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}


@article{bilovs2021neural,
  title={Neural flows: Efficient alternative to neural ODEs},
  author={Bilo{\v{s}}, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and G{\"u}nnemann, Stephan},
  journal={Advances in Neural Information Processing Systems},
  volume={34},
  pages={21325--21337},
  year={2021}
}

@article{nose1990constant,
  title={Constant-temperature molecular dynamics},
  author={Nose, Shuichi},
  journal={Journal of Physics: Condensed Matter},
  volume={2},
  number={S},
  pages={SA115},
  year={1990},
  publisher={IOP Publishing}
}
@article{scherer2020kernel,
  title={Kernel-based machine learning for efficient simulations of molecular liquids},
  author={Scherer, Christoph and Scheid, Ren{\'e} and Andrienko, Denis and Bereau, Tristan},
  journal={Journal of chemical theory and computation},
  volume={16},
  number={5},
  pages={3194--3204},
  year={2020},
  publisher={ACS Publications}
}
@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={arXiv preprint arXiv:1312.6199},
  year={2013}
}

@article{gillette2022data,
  title={Data-driven geometric scale detection via Delaunay interpolation},
  author={Gillette, Andrew and Kur, Eugene},
  journal={arXiv preprint arXiv:2203.05685},
  year={2022}
}


@misc{lundberg2017unified,
	author = {Lundberg, Scott and Lee, Su-In},
	year = 2017,
	title = {A Unified Approach to Interpreting Model Predictions},
	publisher = {arXiv},
	doi = {10.48550/ARXIV.1705.07874},
	url = {https://arxiv.org/abs/1705.07874},
	copyright = {arXiv.org perpetual, non-exclusive license},
	keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}

@article{domingos2020,
  author    = {Pedro Domingos},
  title     = {Every Model Learned by Gradient Descent Is Approximately a Kernel
               Machine},
  journal   = {CoRR},
  volume    = {abs/2012.00152},
  year      = {2020},
  url       = {https://arxiv.org/abs/2012.00152},
  eprinttype = {arXiv},
  eprint    = {2012.00152},
  timestamp = {Fri, 04 Dec 2020 12:07:23 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2012-00152.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{domingos2020every,
  title={Every model learned by gradient descent is approximately a kernel machine},
  author={Domingos, Pedro},
  journal={arXiv preprint arXiv:2012.00152},
  year={2020}
}
@article{smola2002support,
  title={Support vector machines, regularization, optimization, and beyond},
  author={Smola, A},
  journal={Learning with Kernels},
  year={2002}
}
@article{cortes2009learning,
  title={Learning non-linear combinations of kernels},
  author={Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
  journal={Advances in neural information processing systems},
  volume={22},
  year={2009}
}
@misc{ghojogh2021,
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	year = 2021,
	title = {Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nyström Method, and Use of Kernels in Machine Learning: Tutorial and Survey},
	publisher = {arXiv},
	doi = {10.48550/ARXIV.2106.08443},
	url = {https://arxiv.org/abs/2106.08443},
	copyright = {arXiv.org perpetual, non-exclusive license},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Functional Analysis (math.FA), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics}
}
@inproceedings{zhao2005extracting,
  title={Extracting relations with integrated information using kernel methods},
  author={Zhao, Shubin and Grishman, Ralph},
  booktitle={Proceedings of the 43rd annual meeting of the association for computational linguistics (acl’05)},
  pages={419--426},
  year={2005}
}
@article{he2020bayesian,
  title={Bayesian deep ensembles via the neural tangent kernel},
  author={He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1010--1022},
  year={2020}
}



@article{chen2020generalized,
  author       = {Zixiang Chen and
                  Yuan Cao and
                  Quanquan Gu and
                  Tong Zhang},
  title        = {Mean-Field Analysis of Two-Layer Neural Networks: Non-Asymptotic Rates
                  and Generalization Bounds},
  journal      = {CoRR},
  volume       = {abs/2002.04026},
  year         = {2020},
  url          = {https://arxiv.org/abs/2002.04026},
  eprinttype    = {arXiv},
  eprint       = {2002.04026},
  timestamp    = {Wed, 13 Jan 2021 16:36:40 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2002-04026.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

@inproceedings{langley00,
 author    = {P. Langley},
 title     = {Crafting Papers on Machine Learning},
 year      = {2000},
 pages     = {1207--1216},
 editor    = {Pat Langley},
 booktitle     = {Proceedings of the 17th International Conference
              on Machine Learning (ICML 2000)},
 address   = {Stanford, CA},
 publisher = {Morgan Kaufmann}
}

@TechReport{mitchell80,
  author = 	 "T. M. Mitchell",
  title = 	 "The Need for Biases in Learning Generalizations",
  institution =  "Computer Science Department, Rutgers University",
  year = 	 "1980",
  address =	 "New Brunswick, MA",
}

@phdthesis{kearns89,
  author = {M. J. Kearns},
  title =  {Computational Complexity of Machine Learning},
  school = {Department of Computer Science, Harvard University},
  year =   {1989}
}

@Book{MachineLearningI,
  editor = 	 "R. S. Michalski and J. G. Carbonell and T.
		  M. Mitchell",
  title = 	 "Machine Learning: An Artificial Intelligence
		  Approach, Vol. I",
  publisher = 	 "Tioga",
  year = 	 "1983",
  address =	 "Palo Alto, CA"
}

@Book{DudaHart2nd,
  author =       "R. O. Duda and P. E. Hart and D. G. Stork",
  title =        "Pattern Classification",
  publisher =    "John Wiley and Sons",
  edition =      "2nd",
  year =         "2000"
}

@misc{anonymous,
  title= {Suppressed for Anonymity},
  author= {Author, N. N.},
  year= {2021}
}

@InCollection{Newell81,
  author =       "A. Newell and P. S. Rosenbloom",
  title =        "Mechanisms of Skill Acquisition and the Law of
                  Practice", 
  booktitle =    "Cognitive Skills and Their Acquisition",
  pages =        "1--51",
  publisher =    "Lawrence Erlbaum Associates, Inc.",
  year =         "1981",
  editor =       "J. R. Anderson",
  chapter =      "1",
  address =      "Hillsdale, NJ"
}


@Article{Samuel59,
  author = 	 "A. L. Samuel",
  title = 	 "Some Studies in Machine Learning Using the Game of
		  Checkers",
  journal =	 "IBM Journal of Research and Development",
  year =	 "1959",
  volume =	 "3",
  number =	 "3",
  pages =	 "211--229"
}

@article{wang2022pinns,
title = {When and why PINNs fail to train: A neural tangent kernel perspective},
journal = {Journal of Computational Physics},
volume = {449},
pages = {110768},
year = {2022},
issn = {0021-9991},
doi = {https://doi.org/10.1016/j.jcp.2021.110768},
url = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
author = {Sifan Wang and Xinling Yu and Paris Perdikaris},
keywords = {Physics-informed neural networks, Spectral bias, Multi-task learning, Gradient descent, Scientific machine learning},
abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/PINNsNTK.}
}





@inproceedings{DBLP:conf/iclr/LeeBNSPS18,
  author       = {Jaehoon Lee and
                  Yasaman Bahri and
                  Roman Novak and
                  Samuel S. Schoenholz and
                  Jeffrey Pennington and
                  Jascha Sohl{-}Dickstein},
  title        = {Deep Neural Networks as Gaussian Processes},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=B1EA-M-0Z},
  timestamp    = {Mon, 17 Jan 2022 07:47:59 +0100},
  biburl       = {https://dblp.org/rec/conf/iclr/LeeBNSPS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{eijkelboom_mathrmen_2023,
	title = {\${\textbackslash}mathrm\{E\}(n)\$ Equivariant Message Passing Simplicial Networks},
	url = {http://arxiv.org/abs/2305.07100},
	doi = {10.48550/arXiv.2305.07100},
	abstract = {This paper presents \${\textbackslash}mathrm\{E\}(n)\$ Equivariant Message Passing Simplicial Networks ({EMPSNs}), a novel approach to learning on geometric graphs and point clouds that is equivariant to rotations, translations, and reflections. {EMPSNs} can learn high-dimensional simplex features in graphs (e.g. triangles), and use the increase of geometric information of higher-dimensional simplices in an \${\textbackslash}mathrm\{E\}(n)\$ equivariant fashion. {EMPSNs} simultaneously generalize \${\textbackslash}mathrm\{E\}(n)\$ Equivariant Graph Neural Networks to a topologically more elaborate counterpart and provide an approach for including geometric information in Message Passing Simplicial Networks. The results indicate that {EMPSNs} can leverage the benefits of both approaches, leading to a general increase in performance when compared to either method. Furthermore, the results suggest that incorporating geometric information serves as an effective measure against over-smoothing in message passing networks, especially when operating on high-dimensional simplicial structures. Last, we show that {EMPSNs} are on par with state-of-the-art approaches for learning on geometric graphs.},
	number = {{arXiv}:2305.07100},
	publisher = {{arXiv}},
	author = {Eijkelboom, Floor and Hesselink, Rob and Bekkers, Erik},
	urldate = {2023-09-13},
	date = {2023-05-11},
	eprinttype = {arxiv},
	eprint = {2305.07100 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{liu_2d-shapley_2023,
	title = {2D-Shapley: A Framework for Fragmented Data Valuation},
	url = {http://arxiv.org/abs/2306.10473},
	doi = {10.48550/arXiv.2306.10473},
	shorttitle = {2D-Shapley},
	abstract = {Data valuation -- quantifying the contribution of individual data sources to certain predictive behaviors of a model -- is of great importance to enhancing the transparency of machine learning and designing incentive systems for data sharing. Existing work has focused on evaluating data sources with the shared feature or sample space. How to valuate fragmented data sources of which each only contains partial features and samples remains an open question. We start by presenting a method to calculate the counterfactual of removing a fragment from the aggregated data matrix. Based on the counterfactual calculation, we further propose 2D-Shapley, a theoretical framework for fragmented data valuation that uniquely satisfies some appealing axioms in the fragmented data context. 2D-Shapley empowers a range of new use cases, such as selecting useful data fragments, providing interpretation for sample-wise data values, and fine-grained data issue diagnosis.},
	number = {{arXiv}:2306.10473},
	publisher = {{arXiv}},
	author = {Liu, Zhihong and Just, Hoang Anh and Chang, Xiangyu and Chen, Xi and Jia, Ruoxi},
	urldate = {2023-09-13},
	date = {2023-07-26},
	eprinttype = {arxiv},
	eprint = {2306.10473 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{luo_closer_2023,
	title = {A Closer Look at Few-shot Classification Again},
	url = {http://arxiv.org/abs/2301.12246},
	doi = {10.48550/arXiv.2301.12246},
	abstract = {Few-shot classification consists of a training phase where a model is learned on a relatively large dataset and an adaptation phase where the learned model is adapted to previously-unseen tasks with limited labeled samples. In this paper, we empirically prove that the training algorithm and the adaptation algorithm can be completely disentangled, which allows algorithm analysis and design to be done individually for each phase. Our meta-analysis for each phase reveals several interesting insights that may help better understand key aspects of few-shot classification and connections with other fields such as visual representation learning and transfer learning. We hope the insights and research challenges revealed in this paper can inspire future work in related directions. Code and pre-trained models (in {PyTorch}) are available at https://github.com/Frankluox/{CloserLookAgainFewShot}.},
	number = {{arXiv}:2301.12246},
	publisher = {{arXiv}},
	author = {Luo, Xu and Wu, Hao and Zhang, Ji and Gao, Lianli and Xu, Jing and Song, Jingkuan},
	urldate = {2023-08-02},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2301.12246 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{sun_critical_2023,
	title = {A Critical Revisit of Adversarial Robustness in 3D Point Cloud Recognition with Diffusion-Driven Purification},
	url = {https://openreview.net/forum?id=dwn6o2pYJp},
	abstract = {3D point clouds serve as a crucial data representation in numerous real-world applications such as autonomous driving, robotics, and medical imaging. While the advancements in deep learning have spurred the utilization of 3D point clouds, deep models are notoriously vulnerable to adversarial attacks. Various defense solutions have been proposed to build robust models against adversarial attacks. In this work, we pinpoint a major limitation of the leading empirical defense, adversarial training, when applied to 3D point cloud models: gradient obfuscation, which significantly hampers robustness against potent attacks. To bridge the gap, we propose {PointDP}, a purification strategy that leverages diffusion models to defend against 3D adversarial attacks. Since {PointDP} does not rely on predefined adversarial examples for training, it can defend against a variety of threats. We conduct a comprehensive evaluation of {PointDP} across six representative 3D point cloud architectures, employing sixteen strong and adaptive attacks to manifest its foundational robustness. Our evaluation shows that {PointDP} achieves significantly better (i.e., 12.6\%-40.3\%) adversarial robustness than state-of-the-art methods under strong attacks bounded by different \${\textbackslash}ell\_p\$ norms.},
	author = {Sun, Jiachen and Wang, Jiongxiao and Nie, Weili and Yu, Zhiding and Mao, Zhuoqing and Xiao, Chaowei},
	urldate = {2023-09-13},
	date = {2023-06-15},
	langid = {english},
}

@misc{slumbers_game-theoretic_2023,
	title = {A Game-Theoretic Framework for Managing Risk in Multi-Agent Systems},
	url = {http://arxiv.org/abs/2205.15434},
	doi = {10.48550/arXiv.2205.15434},
	abstract = {In order for agents in multi-agent systems ({MAS}) to be safe, they need to take into account the risks posed by the actions of other agents. However, the dominant paradigm in game theory ({GT}) assumes that agents are not affected by risk from other agents and only strive to maximise their expected utility. For example, in hybrid human-{AI} driving systems, it is necessary to limit large deviations in reward resulting from car crashes. Although there are equilibrium concepts in game theory that take into account risk aversion, they either assume that agents are risk-neutral with respect to the uncertainty caused by the actions of other agents, or they are not guaranteed to exist. We introduce a new {GT}-based Risk-Averse Equilibrium ({RAE}) that always produces a solution that minimises the potential variance in reward accounting for the strategy of other agents. Theoretically and empirically, we show {RAE} shares many properties with a Nash Equilibrium ({NE}), establishing convergence properties and generalising to risk-dominant {NE} in certain cases. To tackle large-scale problems, we extend {RAE} to the {PSRO} multi-agent reinforcement learning ({MARL}) framework. We empirically demonstrate the minimum reward variance benefits of {RAE} in matrix games with high-risk outcomes. Results on {MARL} experiments show {RAE} generalises to risk-dominant {NE} in a trust dilemma game and that it reduces instances of crashing by 7x in an autonomous driving setting versus the best performing baseline.},
	number = {{arXiv}:2205.15434},
	publisher = {{arXiv}},
	author = {Slumbers, Oliver and Mguni, David Henry and {McAleer}, Stephen Marcus and Blumberg, Stefano B. and Wang, Jun and Yang, Yaodong},
	urldate = {2023-09-13},
	date = {2023-03-02},
	eprinttype = {arxiv},
	eprint = {2205.15434 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{wu_law_2023,
	title = {A Law of Robustness beyond Isoperimetry},
	url = {http://arxiv.org/abs/2202.11592},
	doi = {10.48550/arXiv.2202.11592},
	abstract = {We study the robust interpolation problem of arbitrary data distributions supported on a bounded space and propose a two-fold law of robustness. Robust interpolation refers to the problem of interpolating \$n\$ noisy training data points in \${\textbackslash}mathbb\{R\}{\textasciicircum}d\$ by a Lipschitz function. Although this problem has been well understood when the samples are drawn from an isoperimetry distribution, much remains unknown concerning its performance under generic or even the worst-case distributions. We prove a Lipschitzness lower bound \${\textbackslash}Omega({\textbackslash}sqrt\{n/p\})\$ of the interpolating neural network with \$p\$ parameters on arbitrary data distributions. With this result, we validate the law of robustness conjecture in prior work by Bubeck, Li, and Nagaraj on two-layer neural networks with polynomial weights. We then extend our result to arbitrary interpolating approximators and prove a Lipschitzness lower bound \${\textbackslash}Omega(n{\textasciicircum}\{1/d\})\$ for robust interpolation. Our results demonstrate a two-fold law of robustness: i) we show the potential benefit of overparametrization for smooth data interpolation when \$n={\textbackslash}mathrm\{poly\}(d)\$, and ii) we disprove the potential existence of an \$O(1)\$-Lipschitz robust interpolating function when \$n={\textbackslash}exp({\textbackslash}omega(d))\$.},
	number = {{arXiv}:2202.11592},
	publisher = {{arXiv}},
	author = {Wu, Yihan and Huang, Heng and Zhang, Hongyang},
	urldate = {2023-08-08},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2202.11592 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{munn_margin-based_nodate,
	title = {A margin-based multiclass generalization  bound via geometric complexity},
	abstract = {There has been considerable effort to better understand the generalization capabilities of deep neural networks both as a means to unlock a theoretical understanding of their success as well as providing directions for further improvements. In this paper we investigate margin-based multiclass generalization bounds for neural networks which rely on a recent complexity measure, the geometric complexity, developed for neural networks and which measures the variability of the model function (Dherin et al., 2022). We derive a new upper bound on the generalization error which scales with the marginnormalized geometric complexity of the network and which holds for a broad family of data distributions and model classes. Our generalization bound is empirically investigated for a {ResNet}18 model trained with {SGD} on the {CIFAR}10 and {CIFAR}100 datasets with both original and random labels.},
	author = {Munn, Michael and Dherin, Benoit and Gonzalvo, Javier},
	langid = {english},
}

@misc{andriushchenko_modern_2023,
	title = {A Modern Look at the Relationship between Sharpness and Generalization},
	url = {http://arxiv.org/abs/2302.07011},
	doi = {10.48550/arXiv.2302.07011},
	abstract = {Sharpness of minima is a promising quantity that can correlate with generalization in deep networks and, when optimized during training, can improve generalization. However, standard sharpness is not invariant under reparametrizations of neural networks, and, to fix this, reparametrization-invariant sharpness definitions have been proposed, most prominently adaptive sharpness (Kwon et al., 2021). But does it really capture generalization in modern practical settings? We comprehensively explore this question in a detailed study of various definitions of adaptive sharpness in settings ranging from training from scratch on {ImageNet} and {CIFAR}-10 to fine-tuning {CLIP} on {ImageNet} and {BERT} on {MNLI}. We focus mostly on transformers for which little is known in terms of sharpness despite their widespread usage. Overall, we observe that sharpness does not correlate well with generalization but rather with some training parameters like the learning rate that can be positively or negatively correlated with generalization depending on the setup. Interestingly, in multiple cases, we observe a consistent negative correlation of sharpness with out-of-distribution error implying that sharper minima can generalize better. Finally, we illustrate on a simple model that the right sharpness measure is highly data-dependent, and that we do not understand well this aspect for realistic data distributions. The code of our experiments is available at https://github.com/tml-epfl/sharpness-vs-generalization.},
	number = {{arXiv}:2302.07011},
	publisher = {{arXiv}},
	author = {Andriushchenko, Maksym and Croce, Francesco and Müller, Maximilian and Hein, Matthias and Flammarion, Nicolas},
	urldate = {2023-08-02},
	date = {2023-06-07},
	eprinttype = {arxiv},
	eprint = {2302.07011 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{rugamer_new_2023,
	title = {A New {PHO}-rmula for Improved Performance of Semi-Structured Networks},
	url = {http://arxiv.org/abs/2306.00522},
	doi = {10.48550/arXiv.2306.00522},
	abstract = {Recent advances to combine structured regression models and deep neural networks for better interpretability, more expressiveness, and statistically valid uncertainty quantification demonstrate the versatility of semi-structured neural networks ({SSNs}). We show that techniques to properly identify the contributions of the different model components in {SSNs}, however, lead to suboptimal network estimation, slower convergence, and degenerated or erroneous predictions. In order to solve these problems while preserving favorable model properties, we propose a non-invasive post-hoc orthogonalization ({PHO}) that guarantees identifiability of model components and provides better estimation and prediction quality. Our theoretical findings are supported by numerical experiments, a benchmark comparison as well as a real-world application to {COVID}-19 infections.},
	number = {{arXiv}:2306.00522},
	publisher = {{arXiv}},
	author = {Rügamer, David},
	urldate = {2023-09-13},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2306.00522 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wang_adversarial_2023,
	title = {Adversarial Policies Beat Superhuman Go {AIs}},
	url = {http://arxiv.org/abs/2211.00241},
	doi = {10.48550/arXiv.2211.00241},
	abstract = {We attack the state-of-the-art Go-playing {AI} system {KataGo} by training adversarial policies against it, achieving a {\textgreater}97\% win rate against {KataGo} running at superhuman settings. Our adversaries do not win by playing Go well. Instead, they trick {KataGo} into making serious blunders. Our attack transfers zero-shot to other superhuman Go-playing {AIs}, and is comprehensible to the extent that human experts can implement it without algorithmic assistance to consistently beat superhuman {AIs}. The core vulnerability uncovered by our attack persists even in {KataGo} agents adversarially trained to defend against our attack. Our results demonstrate that even superhuman {AI} systems may harbor surprising failure modes. Example games are available https://goattack.far.ai/.},
	number = {{arXiv}:2211.00241},
	publisher = {{arXiv}},
	author = {Wang, Tony T. and Gleave, Adam and Tseng, Tom and Pelrine, Kellin and Belrose, Nora and Miller, Joseph and Dennis, Michael D. and Duan, Yawen and Pogrebniak, Viktor and Levine, Sergey and Russell, Stuart},
	urldate = {2023-09-13},
	date = {2023-07-13},
	eprinttype = {arxiv},
	eprint = {2211.00241 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
}

@misc{glockler_adversarial_2023,
	title = {Adversarial robustness of amortized Bayesian inference},
	url = {http://arxiv.org/abs/2305.14984},
	doi = {10.48550/arXiv.2305.14984},
	abstract = {Bayesian inference usually requires running potentially costly inference procedures separately for every new observation. In contrast, the idea of amortized Bayesian inference is to initially invest computational cost in training an inference network on simulated data, which can subsequently be used to rapidly perform inference (i.e., to return estimates of posterior distributions) for new observations. This approach has been applied to many real-world models in the sciences and engineering, but it is unclear how robust the approach is to adversarial perturbations in the observed data. Here, we study the adversarial robustness of amortized Bayesian inference, focusing on simulation-based estimation of multi-dimensional posterior distributions. We show that almost unrecognizable, targeted perturbations of the observations can lead to drastic changes in the predicted posterior and highly unrealistic posterior predictive samples, across several benchmark tasks and a real-world example from neuroscience. We propose a computationally efficient regularization scheme based on penalizing the Fisher information of the conditional density estimator, and show how it improves the adversarial robustness of amortized Bayesian inference.},
	number = {{arXiv}:2305.14984},
	publisher = {{arXiv}},
	author = {Glöckler, Manuel and Deistler, Michael and Macke, Jakob H.},
	urldate = {2023-09-13},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2305.14984 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{raj_algorithmic_2023,
	title = {Algorithmic Stability of Heavy-Tailed {SGD} with General Loss Functions},
	url = {http://arxiv.org/abs/2301.11885},
	doi = {10.48550/arXiv.2301.11885},
	abstract = {Heavy-tail phenomena in stochastic gradient descent ({SGD}) have been reported in several empirical studies. Experimental evidence in previous works suggests a strong interplay between the heaviness of the tails and generalization behavior of {SGD}. To address this empirical phenomena theoretically, several works have made strong topological and statistical assumptions to link the generalization error to heavy tails. Very recently, new generalization bounds have been proven, indicating a non-monotonic relationship between the generalization error and heavy tails, which is more pertinent to the reported empirical observations. While these bounds do not require additional topological assumptions given that {SGD} can be modeled using a heavy-tailed stochastic differential equation ({SDE}), they can only apply to simple quadratic problems. In this paper, we build on this line of research and develop generalization bounds for a more general class of objective functions, which includes non-convex functions as well. Our approach is based on developing Wasserstein stability bounds for heavy-tailed {SDEs} and their discretizations, which we then convert to generalization bounds. Our results do not require any nontrivial assumptions; yet, they shed more light to the empirical observations, thanks to the generality of the loss functions.},
	number = {{arXiv}:2301.11885},
	publisher = {{arXiv}},
	author = {Raj, Anant and Zhu, Lingjiong and Gürbüzbalaban, Mert and Şimşekli, Umut},
	urldate = {2023-09-13},
	date = {2023-01-30},
	eprinttype = {arxiv},
	eprint = {2301.11885 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{duan_are_2023,
	title = {Are Equivariant Equilibrium Approximators Beneficial?},
	url = {http://arxiv.org/abs/2301.11481},
	doi = {10.48550/arXiv.2301.11481},
	abstract = {Recently, remarkable progress has been made by approximating Nash equilibrium ({NE}), correlated equilibrium ({CE}), and coarse correlated equilibrium ({CCE}) through function approximation that trains a neural network to predict equilibria from game representations. Furthermore, equivariant architectures are widely adopted in designing such equilibrium approximators in normal-form games. In this paper, we theoretically characterize benefits and limitations of equivariant equilibrium approximators. For the benefits, we show that they enjoy better generalizability than general ones and can achieve better approximations when the payoff distribution is permutation-invariant. For the limitations, we discuss their drawbacks in terms of equilibrium selection and social welfare. Together, our results help to understand the role of equivariance in equilibrium approximators.},
	number = {{arXiv}:2301.11481},
	publisher = {{arXiv}},
	author = {Duan, Zhijian and Ma, Yunxuan and Deng, Xiaotie},
	urldate = {2023-08-02},
	date = {2023-04-27},
	eprinttype = {arxiv},
	eprint = {2301.11481 [cs]},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Computer Science - Multiagent Systems},
}

@misc{ziomek_are_2023,
	title = {Are Random Decompositions all we need in High Dimensional Bayesian Optimisation?},
	url = {http://arxiv.org/abs/2301.12844},
	doi = {10.48550/arXiv.2301.12844},
	abstract = {Learning decompositions of expensive-to-evaluate black-box functions promises to scale Bayesian optimisation ({BO}) to high-dimensional problems. However, the success of these techniques depends on finding proper decompositions that accurately represent the black-box. While previous works learn those decompositions based on data, we investigate data-independent decomposition sampling rules in this paper. We find that data-driven learners of decompositions can be easily misled towards local decompositions that do not hold globally across the search space. Then, we formally show that a random tree-based decomposition sampler exhibits favourable theoretical guarantees that effectively trade off maximal information gain and functional mismatch between the actual black-box and its surrogate as provided by the decomposition. Those results motivate the development of the random decomposition upper-confidence bound algorithm ({RDUCB}) that is straightforward to implement - (almost) plug-and-play - and, surprisingly, yields significant empirical gains compared to the previous state-of-the-art on a comprehensive set of benchmarks. We also confirm the plug-and-play nature of our modelling component by integrating our method with {HEBO}, showing improved practical gains in the highest dimensional tasks from Bayesmark.},
	number = {{arXiv}:2301.12844},
	publisher = {{arXiv}},
	author = {Ziomek, Juliusz and Bou-Ammar, Haitham},
	urldate = {2023-08-02},
	date = {2023-05-29},
	eprinttype = {arxiv},
	eprint = {2301.12844 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{janeiro_are_2023,
	title = {Are Visual Recognition Models Robust to Image Compression?},
	url = {http://arxiv.org/abs/2304.04518},
	doi = {10.48550/arXiv.2304.04518},
	abstract = {Reducing the data footprint of visual content via image compression is essential to reduce storage requirements, but also to reduce the bandwidth and latency requirements for transmission. In particular, the use of compressed images allows for faster transfer of data, and faster response times for visual recognition in edge devices that rely on cloud-based services. In this paper, we first analyze the impact of image compression using traditional codecs, as well as recent state-of-the-art neural compression approaches, on three visual recognition tasks: image classification, object detection, and semantic segmentation. We consider a wide range of compression levels, ranging from 0.1 to 2 bits-per-pixel (bpp). We find that for all three tasks, the recognition ability is significantly impacted when using strong compression. For example, for segmentation {mIoU} is reduced from 44.5 to 30.5 {mIoU} when compressing to 0.1 bpp using the best compression model we evaluated. Second, we test to what extent this performance drop can be ascribed to a loss of relevant information in the compressed image, or to a lack of generalization of visual recognition models to images with compression artefacts. We find that to a large extent the performance loss is due to the latter: by finetuning the recognition models on compressed training images, most of the performance loss is recovered. For example, bringing segmentation accuracy back up to 42 {mIoU}, i.e. recovering 82\% of the original drop in accuracy.},
	number = {{arXiv}:2304.04518},
	publisher = {{arXiv}},
	author = {Janeiro, João Maria and Frolov, Stanislav and El-Nouby, Alaaeldin and Verbeek, Jakob},
	urldate = {2023-09-13},
	date = {2023-04-10},
	eprinttype = {arxiv},
	eprint = {2304.04518 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{wang_better_2023,
	title = {Better Diffusion Models Further Improve Adversarial Training},
	url = {http://arxiv.org/abs/2302.04638},
	doi = {10.48550/arXiv.2302.04638},
	abstract = {It has been recognized that the data generated by the denoising diffusion probabilistic model ({DDPM}) improves adversarial training. After two years of rapid development in diffusion models, a question naturally arises: can better diffusion models further improve adversarial training? This paper gives an affirmative answer by employing the most recent diffusion model which has higher efficiency (\${\textbackslash}sim 20\$ sampling steps) and image quality (lower {FID} score) compared with {DDPM}. Our adversarially trained models achieve state-of-the-art performance on {RobustBench} using only generated data (no external datasets). Under the \${\textbackslash}ell\_{\textbackslash}infty\$-norm threat model with \${\textbackslash}epsilon=8/255\$, our models achieve \$70.69{\textbackslash}\%\$ and \$42.67{\textbackslash}\%\$ robust accuracy on {CIFAR}-10 and {CIFAR}-100, respectively, i.e. improving upon previous state-of-the-art models by \$+4.58{\textbackslash}\%\$ and \$+8.03{\textbackslash}\%\$. Under the \${\textbackslash}ell\_2\$-norm threat model with \${\textbackslash}epsilon=128/255\$, our models achieve \$84.86{\textbackslash}\%\$ on {CIFAR}-10 (\$+4.44{\textbackslash}\%\$). These results also beat previous works that use external data. We also provide compelling results on the {SVHN} and {TinyImageNet} datasets. Our code is available at https://github.com/wzekai99/{DM}-Improves-{AT}.},
	number = {{arXiv}:2302.04638},
	publisher = {{arXiv}},
	author = {Wang, Zekai and Pang, Tianyu and Du, Chao and Lin, Min and Liu, Weiwei and Yan, Shuicheng},
	urldate = {2023-09-13},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2302.04638 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{hu_beyond_2023,
	title = {Beyond Lipschitz Smoothness: A Tighter Analysis for Nonconvex Optimization},
	url = {https://openreview.net/forum?id=ehfsxFKgNu&referrer=%5Bthe%20profile%20of%20Xidong%20Wu%5D(%2Fprofile%3Fid%3D~Xidong_Wu1)},
	shorttitle = {Beyond Lipschitz Smoothness},
	abstract = {Negative and positive curvatures affect optimization in different ways. However, a lot of existing optimization theories are based on the Lipschitz smoothness assumption, which cannot differentiate between the two. In this paper, we propose to use two separate assumptions for positive and negative curvatures, so that we can study the different implications of the two. We analyze the Lookahead and Local {SGD} methods as concrete examples. Both of them require multiple copies of model parameters and communication among them for every certain period of time in order to prevent divergence. We show that the minimum communication frequency is inversely proportional to the negative curvature, and when the negative curvature becomes zero, we recover the existing theory results for convex optimization. Finally, both experimentally and theoretically, we demonstrate that modern neural networks have highly unbalanced positive/negative curvatures. Thus, an analysis based on separate positive and negative curvatures is more pertinent.},
	author = {Hu, Zhengmian and Wu, Xidong and Huang, Heng},
	urldate = {2023-09-13},
	date = {2023-06-15},
	langid = {english},
}

@misc{bombari_beyond_2023,
	title = {Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels},
	url = {http://arxiv.org/abs/2302.01629},
	doi = {10.48550/arXiv.2302.01629},
	shorttitle = {Beyond the Universal Law of Robustness},
	abstract = {Machine learning models are vulnerable to adversarial perturbations, and a thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon through the lens of over-parameterization: interpolating smoothly the data requires significantly more parameters than simply memorizing it. However, this "universal" law provides only a necessary condition for robustness, and it is unable to discriminate between models. In this paper, we address these gaps by focusing on empirical risk minimization in two prototypical settings, namely, random features and the neural tangent kernel ({NTK}). We prove that, for random features, the model is not robust for any degree of over-parameterization, even when the necessary condition coming from the universal law of robustness is satisfied. In contrast, for even activations, the {NTK} model meets the universal lower bound, and it is robust as soon as the necessary condition on over-parameterization is fulfilled. This also addresses a conjecture in prior work by Bubeck, Li and Nagaraj. Our analysis decouples the effect of the kernel of the model from an "interaction matrix", which describes the interaction with the test data and captures the effect of the activation. Our theoretical results are corroborated by numerical evidence on both synthetic and standard datasets ({MNIST}, {CIFAR}-10).},
	number = {{arXiv}:2302.01629},
	publisher = {{arXiv}},
	author = {Bombari, Simone and Kiyani, Shayan and Mondelli, Marco},
	urldate = {2023-08-02},
	date = {2023-05-27},
	eprinttype = {arxiv},
	eprint = {2302.01629 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bombari_beyond_2023-1,
	title = {Beyond the Universal Law of Robustness: Sharper Laws for Random Features and Neural Tangent Kernels},
	url = {https://proceedings.mlr.press/v202/bombari23a.html},
	shorttitle = {Beyond the Universal Law of Robustness},
	abstract = {Machine learning models are vulnerable to adversarial perturbations, and a thought-provoking paper by Bubeck and Sellke has analyzed this phenomenon through the lens of over-parameterization: interpolating smoothly the data requires significantly more parameters than simply memorizing it. However, this "universal" law provides only a necessary condition for robustness, and it is unable to discriminate between models. In this paper, we address these gaps by focusing on empirical risk minimization in two prototypical settings, namely, random features and the neural tangent kernel ({NTK}). We prove that, for random features, the model is not robust for any degree of over-parameterization, even when the necessary condition coming from the universal law of robustness is satisfied. In contrast, for even activations, the {NTK} model meets the universal lower bound, and it is robust as soon as the necessary condition on over-parameterization is fulfilled. This also addresses a conjecture in prior work by Bubeck, Li and Nagaraj. Our analysis decouples the effect of the kernel of the model from an "interaction matrix", which describes the interaction with the test data and captures the effect of the activation. Our theoretical results are corroborated by numerical evidence on both synthetic and standard datasets ({MNIST}, {CIFAR}-10).},
	eventtitle = {International Conference on Machine Learning},
	pages = {2738--2776},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Bombari, Simone and Kiyani, Shayan and Mondelli, Marco},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{adams_bnn-dp_2023,
	title = {{BNN}-{DP}: Robustness Certification of Bayesian Neural Networks via Dynamic Programming},
	url = {http://arxiv.org/abs/2306.10742},
	doi = {10.48550/arXiv.2306.10742},
	shorttitle = {{BNN}-{DP}},
	abstract = {In this paper, we introduce {BNN}-{DP}, an efficient algorithmic framework for analysis of adversarial robustness of Bayesian Neural Networks ({BNNs}). Given a compact set of input points \$T{\textbackslash}subset {\textbackslash}mathbb\{R\}{\textasciicircum}n\$, {BNN}-{DP} computes lower and upper bounds on the {BNN}'s predictions for all the points in \$T\$. The framework is based on an interpretation of {BNNs} as stochastic dynamical systems, which enables the use of Dynamic Programming ({DP}) algorithms to bound the prediction range along the layers of the network. Specifically, the method uses bound propagation techniques and convex relaxations to derive a backward recursion procedure to over-approximate the prediction range of the {BNN} with piecewise affine functions. The algorithm is general and can handle both regression and classification tasks. On a set of experiments on various regression and classification tasks and {BNN} architectures, we show that {BNN}-{DP} outperforms state-of-the-art methods by up to four orders of magnitude in both tightness of the bounds and computational efficiency.},
	number = {{arXiv}:2306.10742},
	publisher = {{arXiv}},
	author = {Adams, Steven and Patane, Andrea and Lahijanian, Morteza and Laurenti, Luca},
	urldate = {2023-08-02},
	date = {2023-06-19},
	eprinttype = {arxiv},
	eprint = {2306.10742 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{pearce-crump_brauers_2023,
	title = {Brauer's Group Equivariant Neural Networks},
	url = {http://arxiv.org/abs/2212.08630},
	doi = {10.48550/arXiv.2212.08630},
	abstract = {We provide a full characterisation of all of the possible group equivariant neural networks whose layers are some tensor power of \${\textbackslash}mathbb\{R\}{\textasciicircum}\{n\}\$ for three symmetry groups that are missing from the machine learning literature: \$O(n)\$, the orthogonal group; \${SO}(n)\$, the special orthogonal group; and \$Sp(n)\$, the symplectic group. In particular, we find a spanning set of matrices for the learnable, linear, equivariant layer functions between such tensor power spaces in the standard basis of \${\textbackslash}mathbb\{R\}{\textasciicircum}\{n\}\$ when the group is \$O(n)\$ or \${SO}(n)\$, and in the symplectic basis of \${\textbackslash}mathbb\{R\}{\textasciicircum}\{n\}\$ when the group is \$Sp(n)\$.},
	number = {{arXiv}:2212.08630},
	publisher = {{arXiv}},
	author = {Pearce-Crump, Edward},
	urldate = {2023-08-02},
	date = {2023-06-18},
	eprinttype = {arxiv},
	eprint = {2212.08630 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Combinatorics, Mathematics - Representation Theory, Statistics - Machine Learning},
}

@article{boccato_breaking_2023,
	title = {Breaking the Structure of Multilayer Perceptrons with Complex Topologies},
	url = {https://openreview.net/forum?id=reX37gos3p},
	abstract = {Recent advances in neural network ({NN}) architectures have demonstrated that complex topologies possess the potential to surpass the performance of conventional feedforward networks. Nonetheless, previous studies investigating the relationship between network topology and model performance have yielded inconsistent results, complicating their applicability in contexts beyond those scrutinized. In this study, we examine the utility of directed acyclic graphs ({DAGs}) for modeling intricate relationships among neurons within {NNs}. We introduce a novel algorithm for the efficient training of {DAG}-based networks and assess their performance relative to multilayer perceptrons ({MLPs}). Through experimentation on synthetic datasets featuring varying levels of difficulty and noise, we observe that complex networks founded on pertinent graphs outperform {MLPs} in terms of accuracy, particularly within high-difficulty scenarios. Additionally, we explore the theoretical underpinnings of these observations and explore the potential trade-offs associated with employing complex networks. Our research offers valuable insights into the capabilities and constraints of complex {NN} architectures, thus contributing to the ongoing pursuit of designing more potent and efficient deep learning models.},
	author = {Boccato, Tommaso and Ferrante, Matteo and Duggento, Andrea and Toschi, Nicola},
	urldate = {2023-09-13},
	date = {2023-06-18},
	langid = {english},
}

@misc{nguyen_building_2023,
	title = {Building Neural Networks on Matrix Manifolds: A Gyrovector Space Approach},
	url = {http://arxiv.org/abs/2305.04560},
	doi = {10.48550/arXiv.2305.04560},
	shorttitle = {Building Neural Networks on Matrix Manifolds},
	abstract = {Matrix manifolds, such as manifolds of Symmetric Positive Definite ({SPD}) matrices and Grassmann manifolds, appear in many applications. Recently, by applying the theory of gyrogroups and gyrovector spaces that is a powerful framework for studying hyperbolic geometry, some works have attempted to build principled generalizations of Euclidean neural networks on matrix manifolds. However, due to the lack of many concepts in gyrovector spaces for the considered manifolds, e.g., the inner product and gyroangles, techniques and mathematical tools provided by these works are still limited compared to those developed for studying hyperbolic geometry. In this paper, we generalize some notions in gyrovector spaces for {SPD} and Grassmann manifolds, and propose new models and layers for building neural networks on these manifolds. We show the effectiveness of our approach in two applications, i.e., human action recognition and knowledge graph completion.},
	number = {{arXiv}:2305.04560},
	publisher = {{arXiv}},
	author = {Nguyen, Xuan Son and Yang, Shuo},
	urldate = {2023-09-13},
	date = {2023-06-05},
        year = {2023},
	eprinttype = {arxiv},
	eprint = {2305.04560 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{maini_can_2023,
	title = {Can Neural Network Memorization Be Localized?},
	url = {http://arxiv.org/abs/2307.09542},
	doi = {10.48550/arXiv.2307.09542},
	abstract = {Recent efforts at explaining the interplay of memorization and generalization in deep overparametrized networks have posited that neural networks \${\textbackslash}textit\{memorize\}\$ "hard" examples in the final few layers of the model. Memorization refers to the ability to correctly predict on \${\textbackslash}textit\{atypical\}\$ examples of the training set. In this work, we show that rather than being confined to individual layers, memorization is a phenomenon confined to a small set of neurons in various layers of the model. First, via three experimental sources of converging evidence, we find that most layers are redundant for the memorization of examples and the layers that contribute to example memorization are, in general, not the final layers. The three sources are \${\textbackslash}textit\{gradient accounting\}\$ (measuring the contribution to the gradient norms from memorized and clean examples), \${\textbackslash}textit\{layer rewinding\}\$ (replacing specific model weights of a converged model with previous training checkpoints), and \${\textbackslash}textit\{retraining\}\$ (training rewound layers only on clean examples). Second, we ask a more generic question: can memorization be localized \${\textbackslash}textit\{anywhere\}\$ in a model? We discover that memorization is often confined to a small number of neurons or channels (around 5) of the model. Based on these insights we propose a new form of dropout -- \${\textbackslash}textit\{example-tied dropout\}\$ that enables us to direct the memorization of examples to an apriori determined set of neurons. By dropping out these neurons, we are able to reduce the accuracy on memorized examples from \$100{\textbackslash}\%{\textbackslash}to3{\textbackslash}\%\$, while also reducing the generalization gap.},
	number = {{arXiv}:2307.09542},
	publisher = {{arXiv}},
	author = {Maini, Pratyush and Mozer, Michael C. and Sedghi, Hanie and Lipton, Zachary C. and Kolter, J. Zico and Zhang, Chiyuan},
	urldate = {2023-09-13},
	date = {2023-07-18},
	eprinttype = {arxiv},
	eprint = {2307.09542 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{bennouna_certified_2023,
	title = {Certified Robust Neural Networks: Generalization and Corruption Resistance},
	url = {http://arxiv.org/abs/2303.02251},
	doi = {10.48550/arXiv.2303.02251},
	shorttitle = {Certified Robust Neural Networks},
	abstract = {Recent work have demonstrated that robustness (to "corruption") can be at odds with generalization. Adversarial training, for instance, aims to reduce the problematic susceptibility of modern neural networks to small data perturbations. Surprisingly, overfitting is a major concern in adversarial training despite being mostly absent in standard training. We provide here theoretical evidence for this peculiar "robust overfitting" phenomenon. Subsequently, we advance a novel distributionally robust loss function bridging robustness and generalization. We demonstrate both theoretically as well as empirically the loss to enjoy a certified level of robustness against two common types of corruption--data evasion and poisoning attacks--while ensuring guaranteed generalization. We show through careful numerical experiments that our resulting holistic robust ({HR}) training procedure yields {SOTA} performance. Finally, we indicate that {HR} training can be interpreted as a direct extension of adversarial training and comes with a negligible additional computational burden. A ready-to-use python library implementing our algorithm is available at https://github.com/{RyanLucas}3/{HR}\_Neural\_Networks.},
	number = {{arXiv}:2303.02251},
	publisher = {{arXiv}},
	author = {Bennouna, Amine and Lucas, Ryan and Van Parys, Bart},
	urldate = {2023-09-13},
	date = {2023-05-18},
	eprinttype = {arxiv},
	eprint = {2303.02251 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{jurewicz_clustering_2022,
	title = {Clustering and Ordering Variable-Sized Sets: The Catalog Problem},
	url = {https://openreview.net/forum?id=xgFfr5IIuXP},
	shorttitle = {Clustering and Ordering Variable-Sized Sets},
	abstract = {Prediction of a varying number of ordered clusters from sets of any cardinality is a challenging task for neural networks, combining elements of set representation, clustering and learning to order. This task arises in many diverse areas, ranging from medical triage, through multi-channel signal analysis for petroleum exploration to product catalog structure prediction. This paper focuses on the latter, which exemplifies a number of challenges inherent to adaptive ordered clustering, referred to further as the eponymous Catalog Problem. These include learning variable cluster constraints, exhibiting relational reasoning and managing combinatorial complexity. Despite progress in both neural clustering and set-to-sequence methods, no joint, fully differentiable model exists to-date. We develop such a modular architecture, referred to further as Neural Ordered Clusters ({NOC}), enhance it with a specific mechanism for learning cluster-level cardinality constraints, and provide a robust comparison of its performance in relation to alternative models. We test our method on three datasets, including synthetic catalog structures and {PROCAT}, a dataset of real-world catalogs consisting of over 1.5M products, achieving state-of-the-art results on a new, more challenging formulation of the underlying problem, which has not been addressed before. Additionally, we examine the network's ability to learn higher-order interactions and investigate its capacity to learn both compositional and structural rulesets.},
	author = {Jurewicz, Mateusz Maria and Taylor, Graham W. and Derczynski, Leon},
	urldate = {2023-09-13},
	date = {2022-09-29},
	langid = {english},
}

@misc{montanari_compressing_2023,
	title = {Compressing Tabular Data via Latent Variable Estimation},
	url = {http://arxiv.org/abs/2302.09780},
	doi = {10.48550/arXiv.2302.09780},
	abstract = {Data used for analytics and machine learning often take the form of tables with categorical entries. We introduce a family of lossless compression algorithms for such data that proceed in four steps: \$(i)\$ Estimate latent variables associated to rows and columns; \$(ii)\$ Partition the table in blocks according to the row/column latents; \$(iii)\$ Apply a sequential (e.g. Lempel-Ziv) coder to each of the blocks; \$(iv)\$ Append a compressed encoding of the latents. We evaluate it on several benchmark datasets, and study optimal compression in a probabilistic model for that tabular data, whereby latent values are independent and table entries are conditionally independent given the latent values. We prove that the model has a well defined entropy rate and satisfies an asymptotic equipartition property. We also prove that classical compression schemes such as Lempel-Ziv and finite-state encoders do not achieve this rate. On the other hand, the latent estimation strategy outlined above achieves the optimal rate.},
	number = {{arXiv}:2302.09780},
	publisher = {{arXiv}},
	author = {Montanari, Andrea and Weiner, Eric},
	urldate = {2023-08-02},
	date = {2023-02-20},
	eprinttype = {arxiv},
	eprint = {2302.09780 [cs, math]},
	keywords = {Computer Science - Information Theory},
}

@misc{liang_conformal_2023,
	title = {Conformal inference is (almost) free for neural networks trained with early stopping},
	url = {http://arxiv.org/abs/2301.11556},
	doi = {10.48550/arXiv.2301.11556},
	abstract = {Early stopping based on hold-out data is a popular regularization technique designed to mitigate overfitting and increase the predictive accuracy of neural networks. Models trained with early stopping often provide relatively accurate predictions, but they generally still lack precise statistical guarantees unless they are further calibrated using independent hold-out data. This paper addresses the above limitation with conformalized early stopping: a novel method that combines early stopping with conformal calibration while efficiently recycling the same hold-out data. This leads to models that are both accurate and able to provide exact predictive inferences without multiple data splits nor overly conservative adjustments. Practical implementations are developed for different learning tasks -- outlier detection, multi-class classification, regression -- and their competitive performance is demonstrated on real data.},
	number = {{arXiv}:2301.11556},
	publisher = {{arXiv}},
	author = {Liang, Ziyi and Zhou, Yanfei and Sesia, Matteo},
	urldate = {2023-09-13},
	date = {2023-06-26},
	eprinttype = {arxiv},
	eprint = {2301.11556 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@inproceedings{liang_consistency_2023,
	title = {Consistency of Multiple Kernel Clustering},
	url = {https://proceedings.mlr.press/v202/liang23b.html},
	abstract = {Consistency plays an important role in learning theory. However, in multiple kernel clustering ({MKC}), the consistency of kernel weights has not been sufficiently investigated. In this work, we fill this gap with a non-asymptotic analysis on the consistency of kernel weights of a novel method termed {SimpleMKKM}. Under the assumptions of the eigenvalue gap, we give an infinity norm bound as ˜(��/��√)O{\textasciitilde}(k/n){\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}(k/{\textbackslash}sqrt\{n\}), where ��kk is the number of clusters and ��nn is the number of samples. On this basis, we establish an upper bound for the excess clustering risk. Moreover, we study the difference of the kernel weights learned from ��nn samples and ��rr points sampled without replacement, and derive its upper bound as ˜(��⋅1/��−1/��‾‾‾‾‾‾‾‾‾√)O{\textasciitilde}(k⋅1/r−1/n){\textbackslash}widetilde\{{\textbackslash}mathcal\{O\}\}(k{\textbackslash}cdot{\textbackslash}sqrt\{1/r-1/n\}). Based on the above results, we propose a novel strategy with Nyström method to enable {SimpleMKKM} to handle large-scale datasets with a theoretical learning guarantee. Finally, extensive experiments are conducted to verify the theoretical results and the effectiveness of the proposed large-scale strategy.},
	eventtitle = {International Conference on Machine Learning},
	pages = {20650--20676},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Liang, Weixuan and Liu, Xinwang and Liu, Yong and Ma, Chuan and Zhao, Yunping and Liu, Zhe and Zhu, En},
	urldate = {2023-08-08},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{runje_constrained_2023,
	title = {Constrained Monotonic Neural Networks},
	url = {http://arxiv.org/abs/2205.11775},
	doi = {10.48550/arXiv.2205.11775},
	abstract = {Wider adoption of neural networks in many critical domains such as finance and healthcare is being hindered by the need to explain their predictions and to impose additional constraints on them. Monotonicity constraint is one of the most requested properties in real-world scenarios and is the focus of this paper. One of the oldest ways to construct a monotonic fully connected neural network is to constrain signs on its weights. Unfortunately, this construction does not work with popular non-saturated activation functions as it can only approximate convex functions. We show this shortcoming can be fixed by constructing two additional activation functions from a typical unsaturated monotonic activation function and employing each of them on the part of neurons. Our experiments show this approach of building monotonic neural networks has better accuracy when compared to other state-of-the-art methods, while being the simplest one in the sense of having the least number of parameters, and not requiring any modifications to the learning procedure or post-learning steps. Finally, we prove it can approximate any continuous monotone function on a compact subset of \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$.},
	number = {{arXiv}:2205.11775},
	publisher = {{arXiv}},
	author = {Runje, Davor and Shankaranarayana, Sharath M.},
	urldate = {2023-08-08},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2205.11775 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@article{shin_context_2023,
	title = {Context Consistency Regularization for Label Sparsity in Time Series},
	url = {https://openreview.net/forum?id=EvGOdASdHi},
	abstract = {Labels are typically sparse in real-world time series due to the high annotation cost. Recently, consistency regularization techniques have been used to generate artificial labels from unlabeled augmented instances. To fully exploit the sequential characteristic of time series in consistency regularization, we propose a novel method of data augmentation called *context-attached augmentation*, which adds preceding and succeeding instances to a target instance to form its augmented instance. Unlike the existing augmentation techniques that modify a target instance by directly perturbing its attributes, the context-attached augmentation generates instances augmented with varying contexts while maintaining the target instance. Based on our augmentation method, we propose a *context consistency regularization* framework, which first adds different contexts to a target instance sampled from a given time series and then shares unitary reliability-based cross-window labels across the augmented instances to maintain consistency. We demonstrate that the proposed framework outperforms the existing state-of-the-art consistency regularization frameworks through comprehensive experiments on real-world time-series datasets.},
	author = {Shin, Yooju and Yoon, Susik and Song, Hwanjun and Park, Dongmin and Kim, Byunghyun and Lee, Jae-Gil and Lee, Byung Suk},
	urldate = {2023-08-03},
	date = {2023-06-15},
	langid = {english},
}

@misc{hao_coupled_2023,
	title = {Coupled Variational Autoencoder},
	url = {http://arxiv.org/abs/2306.02565},
	doi = {10.48550/arXiv.2306.02565},
	abstract = {Variational auto-encoders are powerful probabilistic models in generative tasks but suffer from generating low-quality samples which are caused by the holes in the prior. We propose the Coupled Variational Auto-Encoder (C-{VAE}), which formulates the {VAE} problem as one of Optimal Transport ({OT}) between the prior and data distributions. The C-{VAE} allows greater flexibility in priors and natural resolution of the prior hole problem by enforcing coupling between the prior and the data distribution and enables flexible optimization through the primal, dual, and semi-dual formulations of entropic {OT}. Simulations on synthetic and real data show that the C-{VAE} outperforms alternatives including {VAE}, {WAE}, and {InfoVAE} in fidelity to the data, quality of the latent representation, and in quality of generated samples.},
	number = {{arXiv}:2306.02565},
	publisher = {{arXiv}},
	author = {Hao, Xiaoran and Shafto, Patrick},
	urldate = {2023-08-02},
	date = {2023-06-04},
	eprinttype = {arxiv},
	eprint = {2306.02565 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{good1963maximum,
  title={Maximum entropy for hypothesis formulation, especially for multidimensional contingency tables},
  author={Good, Irving J},
  journal={The Annals of Mathematical Statistics},
  volume={34},
  number={3},
  pages={911--934},
  year={1963},
  publisher={Institute of Mathematical Statistics}
}
@misc{mao_cross-entropy_2023,
	title = {Cross-Entropy Loss Functions: Theoretical Analysis and Applications},
	url = {http://arxiv.org/abs/2304.07288},
	doi = {10.48550/arXiv.2304.07288},
	shorttitle = {Cross-Entropy Loss Functions},
	abstract = {Cross-entropy is a widely used loss function in applications. It coincides with the logistic loss applied to the outputs of a neural network, when the softmax is used. But, what guarantees can we rely on when using cross-entropy as a surrogate loss? We present a theoretical analysis of a broad family of loss functions, comp-sum losses, that includes cross-entropy (or logistic loss), generalized cross-entropy, the mean absolute error and other cross-entropy-like loss functions. We give the first \$H\$-consistency bounds for these loss functions. These are non-asymptotic guarantees that upper bound the zero-one loss estimation error in terms of the estimation error of a surrogate loss, for the specific hypothesis set \$H\$ used. We further show that our bounds are tight. These bounds depend on quantities called minimizability gaps. To make them more explicit, we give a specific analysis of these gaps for comp-sum losses. We also introduce a new family of loss functions, smooth adversarial comp-sum losses, that are derived from their comp-sum counterparts by adding in a related smooth term. We show that these loss functions are beneficial in the adversarial setting by proving that they admit \$H\$-consistency bounds. This leads to new adversarial robustness algorithms that consist of minimizing a regularized smooth adversarial comp-sum loss. While our main purpose is a theoretical analysis, we also present an extensive empirical analysis comparing comp-sum losses. We further report the results of a series of experiments demonstrating that our adversarial robustness algorithms outperform the current state-of-the-art, while also achieving a superior non-adversarial accuracy.},
	number = {{arXiv}:2304.07288},
	publisher = {{arXiv}},
	author = {Mao, Anqi and Mohri, Mehryar and Zhong, Yutao},
	urldate = {2023-08-02},
	date = {2023-06-19},
	eprinttype = {arxiv},
	eprint = {2304.07288 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{kaufman_data_2023,
	title = {Data Representations’ Study of Latent Image Manifolds},
	url = {https://proceedings.mlr.press/v202/kaufman23a.html},
	abstract = {Deep neural networks have been demonstrated to achieve phenomenal success in many domains, and yet their inner mechanisms are not well understood. In this paper, we investigate the curvature of image manifolds, i.e., the manifold deviation from being flat in its principal directions. We find that state-of-the-art trained convolutional neural networks for image classification have a characteristic curvature profile along layers: an initial steep increase, followed by a long phase of a plateau, and followed by another increase. In contrast, this behavior does not appear in untrained networks in which the curvature flattens. We also show that the curvature gap between the last two layers has a strong correlation with the generalization capability of the network. Moreover, we find that the intrinsic dimension of latent codes is not necessarily indicative of curvature. Finally, we observe that common regularization methods such as mixup yield flatter representations when compared to other methods. Our experiments show consistent results over a variety of deep learning architectures and multiple data sets.},
	eventtitle = {International Conference on Machine Learning},
	pages = {15928--15945},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Kaufman, Ilya and Azencot, Omri},
	urldate = {2023-08-02},
        year = {2023},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@article{sinha_deepemd_2023,
	title = {{DeepEMD}: A Transformer-based Fast Estimation of the Earth Mover's Distance},
	url = {https://openreview.net/forum?id=MST20kIvhA},
	shorttitle = {{DeepEMD}},
	abstract = {The Earth Mover's Distance ({EMD}) is the measure of choice to assess similarity between point clouds. However the computational cost of standard algorithms to compute it makes it prohibitive as a training loss, and the standard approach is to use a surrogate such as the Chamfer distance. We propose instead to use a deep model dubbed {DeepEMD} to directly get an estimate of the {EMD}. We formulate casting the prediction of the bipartite matching as that of an attention matrix, from which we get an accurate estimate of both the {EMD}, and its gradient. Experiments demonstrate not only the accuracy of this model, in particular even when test and train data are from different origins. Moreover, in our experiments, the model performs accurately when processing point clouds which are several times larger than those seen during training. Computation-wise, while the complexity of the exact Hungarian algorithm is \$O(N{\textasciicircum}3)\$, {DeepEMD} scales as \$O(N{\textasciicircum}2)\$, where \$N\$ is the total number of points. This leads to a \$100{\textbackslash}times\$ wall-clock speed-up with \$1024\$ points. {DeepEMD} also achieves better performance than the standard Sinkhorn algorithm, with about \$40{\textbackslash}times\$ speed-up. The availability of gradients allows {DeepEMD} to be used for training a {VAE}, leading to a model with lower reconstruction {EMD} than a standard baseline trained with Chamfer distance.},
	author = {Sinha, Atul Kumar and Fleuret, François},
	urldate = {2023-09-13},
	date = {2023-06-18},
	langid = {english},
}

@misc{sipka_differentiable_2023,
	title = {Differentiable Simulations for Enhanced Sampling of Rare Events},
	url = {http://arxiv.org/abs/2301.03480},
	doi = {10.48550/arXiv.2301.03480},
	abstract = {Simulating rare events, such as the transformation of a reactant into a product in a chemical reaction typically requires enhanced sampling techniques that rely on heuristically chosen collective variables ({CVs}). We propose using differentiable simulations ({DiffSim}) for the discovery and enhanced sampling of chemical transformations without a need to resort to preselected {CVs}, using only a distance metric. Reaction path discovery and estimation of the biasing potential that enhances the sampling are merged into a single end-to-end problem that is solved by path-integral optimization. This is achieved by introducing multiple improvements over standard {DiffSim} such as partial backpropagation and graph mini-batching making {DiffSim} training stable and efficient. The potential of {DiffSim} is demonstrated in the successful discovery of transition paths for the Muller-Brown model potential as well as a benchmark chemical system - alanine dipeptide.},
	number = {{arXiv}:2301.03480},
	publisher = {{arXiv}},
	author = {Šípka, Martin and Dietschreit, Johannes C. B. and Grajciar, Lukáš and Gómez-Bombarelli, Rafael},
	urldate = {2023-08-02},
	date = {2023-01-27},
	eprinttype = {arxiv},
	eprint = {2301.03480 [physics]},
	keywords = {Computer Science - Machine Learning, Physics - Chemical Physics},
}

@inproceedings{sipka_differentiable_2023-1,
	title = {Differentiable Simulations for Enhanced Sampling of Rare Events},
	url = {https://proceedings.mlr.press/v202/sipka23a.html},
	abstract = {Simulating rare events, such as the transformation of a reactant into a product in a chemical reaction typically requires enhanced sampling techniques that rely on heuristically chosen collective variables ({CVs}). We propose using differentiable simulations ({DiffSim}) for the discovery and enhanced sampling of chemical transformations without a need to resort to preselected {CVs}, using only a distance metric. Reaction path discovery and estimation of the biasing potential that enhances the sampling are merged into a single end-to-end problem that is solved by path-integral optimization. This is achieved by introducing multiple improvements over standard {DiffSim} such as partial backpropagation and graph mini-batching making {DiffSim} training stable and efficient. The potential of {DiffSim} is demonstrated in the successful discovery of transition paths for the Muller-Brown model potential as well as a benchmark chemical system - alanine dipeptide.},
	eventtitle = {International Conference on Machine Learning},
	pages = {31990--32007},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Sipka, Martin and Dietschreit, Johannes C. B. and Grajciar, Lukáš and Gomez-Bombarelli, Rafael},
	urldate = {2023-08-01},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{somepalli_diffusion_2022,
	title = {Diffusion Art or Digital Forgery? Investigating Data Replication in Diffusion Models},
	url = {http://arxiv.org/abs/2212.03860},
	doi = {10.48550/arXiv.2212.03860},
	shorttitle = {Diffusion Art or Digital Forgery?},
	abstract = {Cutting-edge diffusion models produce images with high quality and customizability, enabling them to be used for commercial art and graphic design purposes. But do diffusion models create unique works of art, or are they replicating content directly from their training sets? In this work, we study image retrieval frameworks that enable us to compare generated images with training samples and detect when content has been replicated. Applying our frameworks to diffusion models trained on multiple datasets including Oxford flowers, Celeb-A, {ImageNet}, and {LAION}, we discuss how factors such as training set size impact rates of content replication. We also identify cases where diffusion models, including the popular Stable Diffusion model, blatantly copy from their training data.},
	number = {{arXiv}:2212.03860},
	publisher = {{arXiv}},
	author = {Somepalli, Gowthami and Singla, Vasu and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	urldate = {2023-09-13},
	date = {2022-12-12},
	eprinttype = {arxiv},
	eprint = {2212.03860 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Computers and Society, Computer Science - Machine Learning},
}

@online{wang_direct_2023,
	title = {Direct Parameterization of Lipschitz-Bounded Deep Networks},
	url = {https://arxiv.org/abs/2301.11526v3},
	abstract = {This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed \${\textbackslash}ell{\textasciicircum}2\$ Lipschitz bounds, i.e. limited sensitivity to input perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program ({SDP}). We provide a ``direct'' parameterization, i.e., a smooth mapping from \${\textbackslash}mathbb R{\textasciicircum}N\$ onto the set of weights satisfying the {SDP}-based bound. Moreover, our parameterization is complete, i.e. a neural network satisfies the {SDP} bound if and only if it can be represented via our parameterization. This enables training using standard gradient methods, without any inner approximation or computationally intensive tasks (e.g. projections or barrier terms) for the {SDP} constraint. The new parameterization can equivalently be thought of as either a new layer type (the {\textbackslash}textit\{sandwich layer\}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. A comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on both empirical and certified robust accuracy. Code is available at {\textbackslash}url\{https://github.com/acfr/{LBDN}\}.},
	titleaddon = {{arXiv}.org},
	author = {Wang, Ruigang and Manchester, Ian R.},
	urldate = {2023-08-03},
	date = {2023-01-27},
	langid = {english},
}

@misc{wang_direct_2023-1,
	title = {Direct Parameterization of Lipschitz-Bounded Deep Networks},
	url = {http://arxiv.org/abs/2301.11526},
	doi = {10.48550/arXiv.2301.11526},
	abstract = {This paper introduces a new parameterization of deep neural networks (both fully-connected and convolutional) with guaranteed \${\textbackslash}ell{\textasciicircum}2\$ Lipschitz bounds, i.e. limited sensitivity to input perturbations. The Lipschitz guarantees are equivalent to the tightest-known bounds based on certification via a semidefinite program ({SDP}). We provide a ``direct'' parameterization, i.e., a smooth mapping from \${\textbackslash}mathbb R{\textasciicircum}N\$ onto the set of weights satisfying the {SDP}-based bound. Moreover, our parameterization is complete, i.e. a neural network satisfies the {SDP} bound if and only if it can be represented via our parameterization. This enables training using standard gradient methods, without any inner approximation or computationally intensive tasks (e.g. projections or barrier terms) for the {SDP} constraint. The new parameterization can equivalently be thought of as either a new layer type (the {\textbackslash}textit\{sandwich layer\}), or a novel parameterization of standard feedforward networks with parameter sharing between neighbouring layers. A comprehensive set of experiments on image classification shows that sandwich layers outperform previous approaches on both empirical and certified robust accuracy. Code is available at {\textbackslash}url\{https://github.com/acfr/{LBDN}\}.},
	number = {{arXiv}:2301.11526},
	publisher = {{arXiv}},
	author = {Wang, Ruigang and Manchester, Ian R.},
	urldate = {2023-08-03},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2301.11526 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{ganz_perceptually_2023,
	title = {Do Perceptually Aligned Gradients Imply Adversarial Robustness?},
	url = {http://arxiv.org/abs/2207.11378},
	abstract = {Adversarially robust classifiers possess a trait that non-robust models do not -- Perceptually Aligned Gradients ({PAG}). Their gradients with respect to the input align well with human perception. Several works have identified {PAG} as a byproduct of robust training, but none have considered it as a standalone phenomenon nor studied its own implications. In this work, we focus on this trait and test whether {\textbackslash}emph\{Perceptually Aligned Gradients imply Robustness\}. To this end, we develop a novel objective to directly promote {PAG} in training classifiers and examine whether models with such gradients are more robust to adversarial attacks. Extensive experiments on multiple datasets and architectures validate that models with aligned gradients exhibit significant robustness, exposing the surprising bidirectional connection between {PAG} and robustness. Lastly, we show that better gradient alignment leads to increased robustness and harness this observation to boost the robustness of existing adversarial training techniques.},
	number = {{arXiv}:2207.11378},
	publisher = {{arXiv}},
	author = {Ganz, Roy and Kawar, Bahjat and Elad, Michael},
	urldate = {2023-08-01},
	date = {2023-02-01},
	eprinttype = {arxiv},
	eprint = {2207.11378 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@article{srinivas2023,
  author       = {Suraj Srinivas and
                  Sebastian Bordt and
                  Hima Lakkaraju},
  title        = {Which Models have Perceptually-Aligned Gradients? An Explanation via
                  Off-Manifold Robustness},
  journal      = {CoRR},
  volume       = {abs/2305.19101},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2305.19101},
  doi          = {10.48550/arXiv.2305.19101},
  eprinttype    = {arXiv},
  eprint       = {2305.19101},
  timestamp    = {Wed, 07 Jun 2023 14:31:13 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2305-19101.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{ivgi_dog_2023,
	title = {{DoG} is {SGD}'s Best Friend: A Parameter-Free Dynamic Step Size Schedule},
	url = {http://arxiv.org/abs/2302.12022},
	doi = {10.48550/arXiv.2302.12022},
	shorttitle = {{DoG} is {SGD}'s Best Friend},
	abstract = {We propose a tuning-free dynamic {SGD} step size formula, which we call Distance over Gradients ({DoG}). The {DoG} step sizes depend on simple empirical quantities (distance from the initial point and norms of gradients) and have no ``learning rate'' parameter. Theoretically, we show that a slight variation of the {DoG} formula enjoys strong parameter-free convergence guarantees for stochastic convex optimization assuming only {\textbackslash}emph\{locally bounded\} stochastic gradients. Empirically, we consider a broad range of vision and language transfer learning tasks, and show that {DoG}'s performance is close to that of {SGD} with tuned learning rate. We also propose a per-layer variant of {DoG} that generally outperforms tuned {SGD}, approaching the performance of tuned Adam. A {PyTorch} implementation is available at https://github.com/formll/dog},
	number = {{arXiv}:2302.12022},
	publisher = {{arXiv}},
	author = {Ivgi, Maor and Hinder, Oliver and Carmon, Yair},
	urldate = {2023-09-13},
	date = {2023-07-16},
	eprinttype = {arxiv},
	eprint = {2302.12022 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{lai_ds-1000_2022,
	title = {{DS}-1000: A Natural and Reliable Benchmark for Data Science Code Generation},
	url = {http://arxiv.org/abs/2211.11501},
	doi = {10.48550/arXiv.2211.11501},
	shorttitle = {{DS}-1000},
	abstract = {We introduce {DS}-1000, a code generation benchmark with a thousand data science problems spanning seven Python libraries, such as {NumPy} and Pandas. Compared to prior works, {DS}-1000 incorporates three core features. First, our problems reflect diverse, realistic, and practical use cases since we collected them from {StackOverflow}. Second, our automatic evaluation is highly specific (reliable) -- across all Codex-002-predicted solutions that our evaluation accept, only 1.8\% of them are incorrect; we achieve this with multi-criteria metrics, checking both functional correctness by running test cases and surface-form constraints by restricting {API} usages or keywords. Finally, we proactively defend against memorization by slightly modifying our problems to be different from the original {StackOverflow} source; consequently, models cannot answer them correctly by memorizing the solutions from pre-training. The current best public system (Codex-002) achieves 43.3\% accuracy, leaving ample room for improvement. We release our benchmark at https://ds1000-code-gen.github.io.},
	number = {{arXiv}:2211.11501},
	publisher = {{arXiv}},
	author = {Lai, Yuhang and Li, Chengxi and Wang, Yiming and Zhang, Tianyi and Zhong, Ruiqi and Zettlemoyer, Luke and Yih, Scott Wen-tau and Fried, Daniel and Wang, Sida and Yu, Tao},
	urldate = {2023-08-01},
	date = {2022-11-18},
	eprinttype = {arxiv},
	eprint = {2211.11501 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
}

@inproceedings{wu_effective_2023,
	title = {Effective Neural Topic Modeling with Embedding Clustering Regularization},
	url = {https://proceedings.mlr.press/v202/wu23c.html},
	abstract = {Topic models have been prevalent for decades with various applications. However, existing topic models commonly suffer from the notorious topic collapsing: discovered topics semantically collapse towards each other, leading to highly repetitive topics, insufficient topic discovery, and damaged model interpretability. In this paper, we propose a new neural topic model, Embedding Clustering Regularization Topic Model ({ECRTM}). Besides the existing reconstruction error, we propose a novel Embedding Clustering Regularization ({ECR}), which forces each topic embedding to be the center of a separately aggregated word embedding cluster in the semantic space. This enables each produced topic to contain distinct word semantics, which alleviates topic collapsing. Regularized by {ECR}, our {ECRTM} generates diverse and coherent topics together with high-quality topic distributions of documents. Extensive experiments on benchmark datasets demonstrate that {ECRTM} effectively addresses the topic collapsing issue and consistently surpasses state-of-the-art baselines in terms of topic quality, topic distributions of documents, and downstream classification tasks.},
	eventtitle = {International Conference on Machine Learning},
	pages = {37335--37357},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Wu, Xiaobao and Dong, Xinshuai and Nguyen, Thong Thanh and Luu, Anh Tuan},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{daneshmand_efficient_2023,
	title = {Efficient displacement convex optimization with particle gradient descent},
	url = {https://proceedings.mlr.press/v202/daneshmand23a.html},
	abstract = {Particle gradient descent, which uses particles to represent a probability measure and performs gradient descent on particles in parallel, is widely used to optimize functions of probability measures. This paper considers particle gradient descent with a finite number of particles and establishes its theoretical guarantees to optimize functions that are displacement convex in measures. Concretely, for Lipschitz displacement convex functions defined on probability over \$R{\textasciicircum}d\$, we prove that \$O(1/{\textbackslash}epsilon{\textasciicircum}2)\$ particles and \$O(d/{\textbackslash}epsilon{\textasciicircum}4)\$ iterations are sufficient to find the \${\textbackslash}epsilon\$-optimal solutions. We further provide improved complexity bounds for optimizing smooth displacement convex functions. An application of our results proves the conjecture of no optimization-barrier up to permutation invariance, proposed by Entezari et al. (2022), for specific two-layer neural networks with two-dimensional inputs uniformly drawn from unit circle.},
	eventtitle = {International Conference on Machine Learning},
	pages = {6836--6854},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Daneshmand, Hadi and Lee, Jason D. and Jin, Chi},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{choromanski_efficient_2023,
	title = {Efficient Graph Field Integrators Meet Point Clouds},
	url = {http://arxiv.org/abs/2302.00942},
	doi = {10.48550/arXiv.2302.00942},
	abstract = {We present two new classes of algorithms for efficient field integration on graphs encoding point clouds. The first class, {SeparatorFactorization}({SF}), leverages the bounded genus of point cloud mesh graphs, while the second class, {RFDiffusion}({RFD}), uses popular epsilon-nearest-neighbor graph representations for point clouds. Both can be viewed as providing the functionality of Fast Multipole Methods ({FMMs}), which have had a tremendous impact on efficient integration, but for non-Euclidean spaces. We focus on geometries induced by distributions of walk lengths between points (e.g., shortest-path distance). We provide an extensive theoretical analysis of our algorithms, obtaining new results in structural graph theory as a byproduct. We also perform exhaustive empirical evaluation, including on-surface interpolation for rigid and deformable objects (particularly for mesh-dynamics modeling), Wasserstein distance computations for point clouds, and the Gromov-Wasserstein variant.},
	number = {{arXiv}:2302.00942},
	publisher = {{arXiv}},
	author = {Choromanski, Krzysztof and Sehanobish, Arijit and Lin, Han and Zhao, Yunfan and Berger, Eli and Parshakova, Tetiana and Pan, Alvin and Watkins, David and Zhang, Tianyi and Likhosherstov, Valerii and Chowdhury, Somnath Basu Roy and Dubey, Avinava and Jain, Deepali and Sarlos, Tamas and Chaturvedi, Snigdha and Weller, Adrian},
	urldate = {2023-09-13},
	date = {2023-06-20},
	eprinttype = {arxiv},
	eprint = {2302.00942 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{khayatkhoei_emergent_2023,
	title = {Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions},
	url = {http://arxiv.org/abs/2306.09618},
	doi = {10.48550/arXiv.2306.09618},
	abstract = {Precision and Recall are two prominent metrics of generative performance, which were proposed to separately measure the fidelity and diversity of generative models. Given their central role in comparing and improving generative models, understanding their limitations are crucially important. To that end, in this work, we identify a critical flaw in the common approximation of these metrics using k-nearest-neighbors, namely, that the very interpretations of fidelity and diversity that are assigned to Precision and Recall can fail in high dimensions, resulting in very misleading conclusions. Specifically, we empirically and theoretically show that as the number of dimensions grows, two model distributions with supports at equal point-wise distance from the support of the real distribution, can have vastly different Precision and Recall regardless of their respective distributions, hence an emergent asymmetry in high dimensions. Based on our theoretical insights, we then provide simple yet effective modifications to these metrics to construct symmetric metrics regardless of the number of dimensions. Finally, we provide experiments on real-world datasets to illustrate that the identified flaw is not merely a pathological case, and that our proposed metrics are effective in alleviating its impact.},
	number = {{arXiv}:2306.09618},
	publisher = {{arXiv}},
	author = {Khayatkhoei, Mahyar and {AbdAlmageed}, Wael},
	urldate = {2023-08-02},
	date = {2023-07-18},
	eprinttype = {arxiv},
	eprint = {2306.09618 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{saha_end--end_2023,
	title = {End-to-end Differentiable Clustering with Associative Memories},
	url = {http://arxiv.org/abs/2306.03209},
	doi = {10.48550/arXiv.2306.03209},
	abstract = {Clustering is a widely used unsupervised learning technique involving an intensive discrete optimization problem. Associative Memory models or {AMs} are differentiable neural networks defining a recursive dynamical system, which have been integrated with various deep learning architectures. We uncover a novel connection between the {AM} dynamics and the inherent discrete assignment necessary in clustering to propose a novel unconstrained continuous relaxation of the discrete clustering problem, enabling end-to-end differentiable clustering with {AM}, dubbed {ClAM}. Leveraging the pattern completion ability of {AMs}, we further develop a novel self-supervised clustering loss. Our evaluations on varied datasets demonstrate that {ClAM} benefits from the self-supervision, and significantly improves upon both the traditional Lloyd's k-means algorithm, and more recent continuous clustering relaxations (by upto 60\% in terms of the Silhouette Coefficient).},
	number = {{arXiv}:2306.03209},
	publisher = {{arXiv}},
	author = {Saha, Bishwajit and Krotov, Dmitry and Zaki, Mohammed J. and Ram, Parikshit},
	urldate = {2023-08-02},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.03209 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{taniguchi_end--end_2023,
	title = {End-to-end Training of Deep Boltzmann Machines by Unbiased Contrastive Divergence with Local Mode Initialization},
	url = {http://arxiv.org/abs/2305.19684},
	doi = {10.48550/arXiv.2305.19684},
	abstract = {We address the problem of biased gradient estimation in deep Boltzmann machines ({DBMs}). The existing method to obtain an unbiased estimator uses a maximal coupling based on a Gibbs sampler, but when the state is high-dimensional, it takes a long time to converge. In this study, we propose to use a coupling based on the Metropolis-Hastings ({MH}) and to initialize the state around a local mode of the target distribution. Because of the propensity of {MH} to reject proposals, the coupling tends to converge in only one step with a high probability, leading to high efficiency. We find that our method allows {DBMs} to be trained in an end-to-end fashion without greedy pretraining. We also propose some practical techniques to further improve the performance of {DBMs}. We empirically demonstrate that our training algorithm enables {DBMs} to show comparable generative performance to other deep generative models, achieving the {FID} score of 10.33 for {MNIST}.},
	number = {{arXiv}:2305.19684},
	publisher = {{arXiv}},
	author = {Taniguchi, Shohei and Suzuki, Masahiro and Iwasawa, Yusuke and Matsuo, Yutaka},
	urldate = {2023-08-02},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2305.19684 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@online{noauthor_equivariant_nodate,
	title = {Equivariant Representation Learning with Equivariant Convolutional Kernel Networks {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=xIOmDCSzMv},
	urldate = {2023-09-13},
}

                  
@article{yang2022capacity,
  title={On the capacity of deep generative networks for approximating distributions},
  author={Yang, Yunfei and Li, Zhen and Wang, Yang},
  journal={Neural networks},
  volume={145},
  pages={144--154},
  year={2022},
  publisher={Elsevier}
}
                  
@article{lu2020universal,
  title={A universal approximation theorem of deep neural networks for expressing probability distributions},
  author={Lu, Yulong and Lu, Jianfeng},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={3094--3105},
  year={2020}
}
@inproceedings{yang_estimating_2023,
	title = {Estimating the Rate-Distortion Function by Wasserstein Gradient Descent},
	url = {https://openreview.net/forum?id=5pt5Btjr8W},
	abstract = {In the theory of lossy compression, the rate-distortion function \$R(D)\$ of a given data source characterizes the fundamental limit of compression performance by any algorithm. We propose a method to estimate \$R(D)\$ in the continuous setting based on Wasserstein gradient descent. While the classic Blahut--Arimoto algorithm only optimizes probability weights over the support points of its initialization, our method leverages optimal transport theory and learns the support of the optimal reproduction distribution by moving particles. This makes it more suitable for high dimensional continuous problems. Our method complements state-of-the-art neural network-based methods in rate-distortion estimation, achieving comparable or improved results with less tuning and computation effort. In addition, we can derive its convergence and finite-sample properties analytically. Our study also applies to maximum likelihood deconvolution and regularized Kantorovich estimation, as those tasks boil down to mathematically equivalent minimization problems.},
	eventtitle = {{ICML} 2023 Workshop Neural Compression: From Information Theory to Applications},
	author = {Yang, Yibo and Eckstein, Stephan and Nutz, Marcel and Mandt, Stephan},
	urldate = {2023-09-13},
	date = {2023-07-11},
	langid = {english},
}

@inproceedings{senetaire_explainability_2023,
	title = {Explainability as statistical inference},
	url = {https://proceedings.mlr.press/v202/senetaire23a.html},
	abstract = {A wide variety of model explanation approaches have been proposed in recent years, all guided by very different rationales and heuristics. In this paper, we take a new route and cast interpretability as a statistical inference problem. We propose a general deep probabilistic model designed to produce interpretable predictions. The model’s parameters can be learned via maximum likelihood, and the method can be adapted to any predictor network architecture, and any type of prediction problem. Our model is akin to amortized interpretability methods, where a neural network is used as a selector to allow for fast interpretation at inference time. Several popular interpretability methods are shown to be particular cases of regularized maximum likelihood for our general model. Using our framework, we identify imputation as a common issue of these models. We propose new datasets with ground truth selection which allow for the evaluation of the features importance map and show experimentally that multiple imputation provides more reasonable interpretations.},
	eventtitle = {International Conference on Machine Learning},
	pages = {30584--30612},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Senetaire, Hugo Henri Joseph and Garreau, Damien and Frellsen, Jes and Mattei, Pierre-Alexandre},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{tonin_extending_2023,
	title = {Extending Kernel {PCA} through Dualization: Sparsity, Robustness and Fast Algorithms},
	url = {https://proceedings.mlr.press/v202/tonin23a.html},
	shorttitle = {Extending Kernel {PCA} through Dualization},
	abstract = {The goal of this paper is to revisit Kernel Principal Component Analysis ({KPCA}) through dualization of a difference of convex functions. This allows to naturally extend {KPCA} to multiple objective functions and leads to efficient gradient-based algorithms avoiding the expensive {SVD} of the Gram matrix. Particularly, we consider objective functions that can be written as Moreau envelopes, demonstrating how to promote robustness and sparsity within the same framework. The proposed method is evaluated on synthetic and realworld benchmarks, showing significant speedup in {KPCA} training time as well as highlighting the benefits in terms of robustness and sparsity.},
	eventtitle = {International Conference on Machine Learning},
	pages = {34379--34393},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Tonin, Francesco and Lambert, Alex and Patrinos, Panagiotis and Suykens, Johan},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@article{peltonen_fair_2023,
	title = {Fair Neighbor Embedding},
	url = {https://openreview.net/forum?id=g0zYQRWmFR},
	abstract = {We consider fairness in dimensionality reduction. Nonlinear dimensionality reduction yields low dimensional representations that let users visualize and explore high-dimensional data. However, traditional dimensionality reduction may yield biased visualizations overemphasizing relationships of societal phenomena to sensitive attributes or protected groups. We introduce a framework of fair neighbor embedding, the Fair Neighbor Retrieval Visualizer, which formulates fair nonlinear dimensionality reduction as an information retrieval task whose performance and fairness are quantified by information retrieval criteria. The method optimizes low-dimensional embeddings that preserve high-dimensional data neighborhoods without yielding biased association of such neighborhoods to protected groups. In experiments the method yields fair visualizations outperforming previous methods.},
	author = {Peltonen, Jaakko and Xu, Wen and Nummenmaa, Timo and Nummenmaa, Jyrki},
	urldate = {2023-08-08},
	date = {2023-06-15},
	langid = {english},
}

@misc{zhou_fast_2023,
	title = {Fast Online Node Labeling for Very Large Graphs},
	url = {http://arxiv.org/abs/2305.16257},
	doi = {10.48550/arXiv.2305.16257},
	abstract = {This paper studies the online node classification problem under a transductive learning setting. Current methods either invert a graph kernel matrix with \${\textbackslash}mathcal\{O\}(n{\textasciicircum}3)\$ runtime and \${\textbackslash}mathcal\{O\}(n{\textasciicircum}2)\$ space complexity or sample a large volume of random spanning trees, thus are difficult to scale to large graphs. In this work, we propose an improvement based on the {\textbackslash}textit\{online relaxation\} technique introduced by a series of works (Rakhlin et al.,2012; Rakhlin and Sridharan, 2015; 2017). We first prove an effective regret \${\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{n{\textasciicircum}\{1+{\textbackslash}gamma\}\})\$ when suitable parameterized graph kernels are chosen, then propose an approximate algorithm {FastONL} enjoying \${\textbackslash}mathcal\{O\}(k{\textbackslash}sqrt\{n{\textasciicircum}\{1+{\textbackslash}gamma\}\})\$ regret based on this relaxation. The key of {FastONL} is a {\textbackslash}textit\{generalized local push\} method that effectively approximates inverse matrix columns and applies to a series of popular kernels. Furthermore, the per-prediction cost is \${\textbackslash}mathcal\{O\}({\textbackslash}text\{vol\}(\{{\textbackslash}mathcal\{S\}\}){\textbackslash}log 1/{\textbackslash}epsilon)\$ locally dependent on the graph with linear memory cost. Experiments show that our scalable method enjoys a better tradeoff between local and global consistency.},
	number = {{arXiv}:2305.16257},
	publisher = {{arXiv}},
	author = {Zhou, Baojian and Sun, Yifan and Babanezhad, Reza},
	urldate = {2023-08-02},
	date = {2023-05-28},
	eprinttype = {arxiv},
	eprint = {2305.16257 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Spectral Theory},
}

@misc{sander_fast_2023,
	title = {Fast, Differentiable and Sparse Top-k: a Convex Analysis Perspective},
	url = {http://arxiv.org/abs/2302.01425},
	doi = {10.48550/arXiv.2302.01425},
	shorttitle = {Fast, Differentiable and Sparse Top-k},
	abstract = {The top-k operator returns a sparse vector, where the non-zero values correspond to the k largest values of the input. Unfortunately, because it is a discontinuous function, it is difficult to incorporate in neural networks trained end-to-end with backpropagation. Recent works have considered differentiable relaxations, based either on regularization or perturbation techniques. However, to date, no approach is fully differentiable and sparse. In this paper, we propose new differentiable and sparse top-k operators. We view the top-k operator as a linear program over the permutahedron, the convex hull of permutations. We then introduce a p-norm regularization term to smooth out the operator, and show that its computation can be reduced to isotonic optimization. Our framework is significantly more general than the existing one and allows for example to express top-k operators that select values in magnitude. On the algorithmic side, in addition to pool adjacent violator ({PAV}) algorithms, we propose a new {GPU}/{TPU}-friendly Dykstra algorithm to solve isotonic optimization problems. We successfully use our operators to prune weights in neural networks, to fine-tune vision transformers, and as a router in sparse mixture of experts.},
	number = {{arXiv}:2302.01425},
	publisher = {{arXiv}},
	author = {Sander, Michael E. and Puigcerver, Joan and Djolonga, Josip and Peyré, Gabriel and Blondel, Mathieu},
	urldate = {2023-08-03},
	date = {2023-06-04},
	eprinttype = {arxiv},
	eprint = {2302.01425 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{wu_finite-sample_2023,
	title = {Finite-Sample Analysis of Learning High-Dimensional Single {ReLU} Neuron},
	url = {http://arxiv.org/abs/2303.02255},
	doi = {10.48550/arXiv.2303.02255},
	abstract = {This paper considers the problem of learning a single {ReLU} neuron with squared loss (a.k.a., {ReLU} regression) in the overparameterized regime, where the input dimension can exceed the number of samples. We analyze a Perceptron-type algorithm called {GLM}-tron (Kakade et al., 2011) and provide its dimension-free risk upper bounds for high-dimensional {ReLU} regression in both well-specified and misspecified settings. Our risk bounds recover several existing results as special cases. Moreover, in the well-specified setting, we provide an instance-wise matching risk lower bound for {GLM}-tron. Our upper and lower risk bounds provide a sharp characterization of the high-dimensional {ReLU} regression problems that can be learned via {GLM}-tron. On the other hand, we provide some negative results for stochastic gradient descent ({SGD}) for {ReLU} regression with symmetric Bernoulli data: if the model is well-specified, the excess risk of {SGD} is provably no better than that of {GLM}-tron ignoring constant factors, for each problem instance; and in the noiseless case, {GLM}-tron can achieve a small risk while {SGD} unavoidably suffers from a constant risk in expectation. These results together suggest that {GLM}-tron might be preferable to {SGD} for high-dimensional {ReLU} regression.},
	number = {{arXiv}:2303.02255},
	publisher = {{arXiv}},
	author = {Wu, Jingfeng and Zou, Difan and Chen, Zixiang and Braverman, Vladimir and Gu, Quanquan and Kakade, Sham M.},
	urldate = {2023-09-13},
	date = {2023-06-26},
	eprinttype = {arxiv},
	eprint = {2303.02255 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{diao_forward-backward_2023,
	title = {Forward-backward Gaussian variational inference via {JKO} in the Bures-Wasserstein Space},
	url = {http://arxiv.org/abs/2304.05398},
	doi = {10.48550/arXiv.2304.05398},
	abstract = {Variational inference ({VI}) seeks to approximate a target distribution \${\textbackslash}pi\$ by an element of a tractable family of distributions. Of key interest in statistics and machine learning is Gaussian {VI}, which approximates \${\textbackslash}pi\$ by minimizing the Kullback-Leibler ({KL}) divergence to \${\textbackslash}pi\$ over the space of Gaussians. In this work, we develop the (Stochastic) Forward-Backward Gaussian Variational Inference ({FB}-{GVI}) algorithm to solve Gaussian {VI}. Our approach exploits the composite structure of the {KL} divergence, which can be written as the sum of a smooth term (the potential) and a non-smooth term (the entropy) over the Bures-Wasserstein ({BW}) space of Gaussians endowed with the Wasserstein distance. For our proposed algorithm, we obtain state-of-the-art convergence guarantees when \${\textbackslash}pi\$ is log-smooth and log-concave, as well as the first convergence guarantees to first-order stationary solutions when \${\textbackslash}pi\$ is only log-smooth.},
	number = {{arXiv}:2304.05398},
	publisher = {{arXiv}},
	author = {Diao, Michael and Balasubramanian, Krishnakumar and Chewi, Sinho and Salim, Adil},
	urldate = {2023-09-13},
	date = {2023-04-10},
	eprinttype = {arxiv},
	eprint = {2304.05398 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Statistics Theory},
}

@misc{tran_fully_2023,
	title = {Fully Bayesian Autoencoders with Latent Sparse Gaussian Processes},
	url = {http://arxiv.org/abs/2302.04534},
	doi = {10.48550/arXiv.2302.04534},
	abstract = {Autoencoders and their variants are among the most widely used models in representation learning and generative modeling. However, autoencoder-based models usually assume that the learned representations are i.i.d. and fail to capture the correlations between the data samples. To address this issue, we propose a novel Sparse Gaussian Process Bayesian Autoencoder ({SGPBAE}) model in which we impose fully Bayesian sparse Gaussian Process priors on the latent space of a Bayesian Autoencoder. We perform posterior estimation for this model via stochastic gradient Hamiltonian Monte Carlo. We evaluate our approach qualitatively and quantitatively on a wide range of representation learning and generative modeling tasks and show that our approach consistently outperforms multiple alternatives relying on Variational Autoencoders.},
	number = {{arXiv}:2302.04534},
	publisher = {{arXiv}},
	author = {Tran, Ba-Hien and Shahbaba, Babak and Mandt, Stephan and Filippone, Maurizio},
	urldate = {2023-09-13},
	date = {2023-02-09},
	eprinttype = {arxiv},
	eprint = {2302.04534 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zeng_generative_2023,
	title = {Generative Graph Dictionary Learning},
	url = {https://proceedings.mlr.press/v202/zeng23c.html},
	abstract = {Dictionary learning, which approximates data samples by a set of shared atoms, is a fundamental task in representation learning. However, dictionary learning over graphs, namely graph dictionary learning ({GDL}), is much more challenging than vectorial data as graphs lie in disparate metric spaces. The sparse literature on {GDL} formulates the problem from the reconstructive view and often learns linear graph embeddings with a high computational cost. In this paper, we propose a Fused Gromov-Wasserstein ({FGW}) Mixture Model named {FraMe} to address the {GDL} problem from the generative view. Equipped with the graph generation function based on the radial basis function kernel and {FGW} distance, {FraMe} generates nonlinear embedding spaces, which, as we theoretically proved, provide a good approximation of the original graph spaces. A fast solution is further proposed on top of the expectation-maximization algorithm with guaranteed convergence. Extensive experiments demonstrate the effectiveness of the obtained node and graph embeddings, and our algorithm achieves significant improvements over the state-of-the-art methods.},
	eventtitle = {International Conference on Machine Learning},
	pages = {40749--40769},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Zeng, Zhichen and Zhu, Ruike and Xia, Yinglong and Zeng, Hanqing and Tong, Hanghang},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{kandpal_git-theta_2023,
	title = {Git-Theta: A Git Extension for Collaborative Development of Machine Learning Models},
	url = {http://arxiv.org/abs/2306.04529},
	doi = {10.48550/arXiv.2306.04529},
	shorttitle = {Git-Theta},
	abstract = {Currently, most machine learning models are trained by centralized teams and are rarely updated. In contrast, open-source software development involves the iterative development of a shared artifact through distributed collaboration using a version control system. In the interest of enabling collaborative and continual improvement of machine learning models, we introduce Git-Theta, a version control system for machine learning models. Git-Theta is an extension to Git, the most widely used version control software, that allows fine-grained tracking of changes to model parameters alongside code and other artifacts. Unlike existing version control systems that treat a model checkpoint as a blob of data, Git-Theta leverages the structure of checkpoints to support communication-efficient updates, automatic model merges, and meaningful reporting about the difference between two versions of a model. In addition, Git-Theta includes a plug-in system that enables users to easily add support for new functionality. In this paper, we introduce Git-Theta's design and features and include an example use-case of Git-Theta where a pre-trained model is continually adapted and modified. We publicly release Git-Theta in hopes of kickstarting a new era of collaborative model development.},
	number = {{arXiv}:2306.04529},
	publisher = {{arXiv}},
	author = {Kandpal, Nikhil and Lester, Brian and Muqeeth, Mohammed and Mascarenhas, Anisha and Evans, Monty and Baskaran, Vishal and Huang, Tenghao and Liu, Haokun and Raffel, Colin},
	urldate = {2023-08-02},
	date = {2023-06-07},
	eprinttype = {arxiv},
	eprint = {2306.04529 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Software Engineering},
}

@misc{shilton_gradient_2023,
	title = {Gradient Descent in Neural Networks as Sequential Learning in {RKBS}},
	url = {http://arxiv.org/abs/2302.00205},
	doi = {10.48550/arXiv.2302.00205},
	abstract = {The study of Neural Tangent Kernels ({NTKs}) has provided much needed insight into convergence and generalization properties of neural networks in the over-parametrized (wide) limit by approximating the network using a first-order Taylor expansion with respect to its weights in the neighborhood of their initialization values. This allows neural network training to be analyzed from the perspective of reproducing kernel Hilbert spaces ({RKHS}), which is informative in the over-parametrized regime, but a poor approximation for narrower networks as the weights change more during training. Our goal is to extend beyond the limits of {NTK} toward a more general theory. We construct an exact power-series representation of the neural network in a finite neighborhood of the initial weights as an inner product of two feature maps, respectively from data and weight-step space, to feature space, allowing neural network training to be analyzed from the perspective of reproducing kernel \{{\textbackslash}em Banach\} space ({RKBS}). We prove that, regardless of width, the training sequence produced by gradient descent can be exactly replicated by regularized sequential learning in {RKBS}. Using this, we present novel bound on uniform convergence where the iterations count and learning rate play a central role, giving new theoretical insight into neural network training.},
	number = {{arXiv}:2302.00205},
	publisher = {{arXiv}},
	author = {Shilton, Alistair and Gupta, Sunil and Rana, Santu and Venkatesh, Svetha},
	urldate = {2023-09-13},
	date = {2023-01-31},
	eprinttype = {arxiv},
	eprint = {2302.00205 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}
@article{zhang2009reproducing,
  title={Reproducing Kernel Banach Spaces for Machine Learning.},
  author={Zhang, Haizhang and Xu, Yuesheng and Zhang, Jun},
  journal={Journal of Machine Learning Research},
  volume={10},
  number={12},
  year={2009}
}
@misc{xin_gril_2023,
	title = {{GRIL}: A \$2\$-parameter Persistence Based Vectorization for Machine Learning},
	url = {http://arxiv.org/abs/2304.04970},
	doi = {10.48550/arXiv.2304.04970},
	shorttitle = {{GRIL}},
	abstract = {\$1\$-parameter persistent homology, a cornerstone in Topological Data Analysis ({TDA}), studies the evolution of topological features such as connected components and cycles hidden in data. It has been applied to enhance the representation power of deep learning models, such as Graph Neural Networks ({GNNs}). To enrich the representations of topological features, here we propose to study \$2\$-parameter persistence modules induced by bi-filtration functions. In order to incorporate these representations into machine learning models, we introduce a novel vector representation called Generalized Rank Invariant Landscape ({GRIL}) for \$2\$-parameter persistence modules. We show that this vector representation is \$1\$-Lipschitz stable and differentiable with respect to underlying filtration functions and can be easily integrated into machine learning models to augment encoding topological features. We present an algorithm to compute the vector representation efficiently. We also test our methods on synthetic and benchmark graph datasets, and compare the results with previous vector representations of \$1\$-parameter and \$2\$-parameter persistence modules. Further, we augment {GNNs} with {GRIL} features and observe an increase in performance indicating that {GRIL} can capture additional features enriching {GNNs}. We make the complete code for the proposed method available at https://github.com/soham0209/mpml-graph.},
	number = {{arXiv}:2304.04970},
	publisher = {{arXiv}},
	author = {Xin, Cheng and Mukherjee, Soham and Samaga, Shreyas N. and Dey, Tamal K.},
	urldate = {2023-09-13},
	date = {2023-06-30},
	eprinttype = {arxiv},
	eprint = {2304.04970 [cs, math]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computational Geometry, Computer Science - Machine Learning, Mathematics - Algebraic Topology},
}

@misc{grigsby_hidden_2023,
	title = {Hidden symmetries of {ReLU} networks},
	url = {http://arxiv.org/abs/2306.06179},
	doi = {10.48550/arXiv.2306.06179},
	abstract = {The parameter space for any fixed architecture of feedforward {ReLU} neural networks serves as a proxy during training for the associated class of functions - but how faithful is this representation? It is known that many different parameter settings can determine the same function. Moreover, the degree of this redundancy is inhomogeneous: for some networks, the only symmetries are permutation of neurons in a layer and positive scaling of parameters at a neuron, while other networks admit additional hidden symmetries. In this work, we prove that, for any network architecture where no layer is narrower than the input, there exist parameter settings with no hidden symmetries. We also describe a number of mechanisms through which hidden symmetries can arise, and empirically approximate the functional dimension of different network architectures at initialization. These experiments indicate that the probability that a network has no hidden symmetries decreases towards 0 as depth increases, while increasing towards 1 as width and input dimension increase.},
	number = {{arXiv}:2306.06179},
	publisher = {{arXiv}},
	author = {Grigsby, J. Elisenda and Lindsey, Kathryn and Rolnick, David},
	urldate = {2023-09-13},
	date = {2023-06-09},
	eprinttype = {arxiv},
	eprint = {2306.06179 [cs, math]},
	keywords = {57R70, 57Q99, 52B70, 52C35, Computer Science - Machine Learning, I.2.6, Mathematics - Combinatorics, Mathematics - Geometric Topology},
}

@misc{bonet_hyperbolic_2023,
	title = {Hyperbolic Sliced-Wasserstein via Geodesic and Horospherical Projections},
	url = {http://arxiv.org/abs/2211.10066},
	doi = {10.48550/arXiv.2211.10066},
	abstract = {It has been shown beneficial for many types of data which present an underlying hierarchical structure to be embedded in hyperbolic spaces. Consequently, many tools of machine learning were extended to such spaces, but only few discrepancies to compare probability distributions defined over those spaces exist. Among the possible candidates, optimal transport distances are well defined on such Riemannian manifolds and enjoy strong theoretical properties, but suffer from high computational cost. On Euclidean spaces, sliced-Wasserstein distances, which leverage a closed-form of the Wasserstein distance in one dimension, are more computationally efficient, but are not readily available on hyperbolic spaces. In this work, we propose to derive novel hyperbolic sliced-Wasserstein discrepancies. These constructions use projections on the underlying geodesics either along horospheres or geodesics. We study and compare them on different tasks where hyperbolic representations are relevant, such as sampling or image classification.},
	number = {{arXiv}:2211.10066},
	publisher = {{arXiv}},
	author = {Bonet, Clément and Chapel, Laetitia and Drumetz, Lucas and Courty, Nicolas},
	urldate = {2023-09-13},
	date = {2023-06-26},
	eprinttype = {arxiv},
	eprint = {2211.10066 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@inproceedings{lee_implicit_2023,
	title = {Implicit Jacobian regularization weighted with impurity of probability output},
	url = {https://proceedings.mlr.press/v202/lee23q.html},
	abstract = {The success of deep learning is greatly attributed to stochastic gradient descent ({SGD}), yet it remains unclear how {SGD} finds well-generalized models. We demonstrate that {SGD} has an implicit regularization effect on the logit-weight Jacobian norm of neural networks. This regularization effect is weighted with the impurity of the probability output, and thus it is active in a certain phase of training. Moreover, based on these findings, we propose a novel optimization method that explicitly regularizes the Jacobian norm, which leads to similar performance as other state-of-the-art sharpness-aware optimization methods.},
	eventtitle = {International Conference on Machine Learning},
	pages = {19141--19184},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Lee, Sungyoon and Park, Jinseong and Lee, Jaewook},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{zhou_implicit_2023,
	title = {Implicit Regularization Leads to Benign Overfitting for Sparse Linear Regression},
	url = {http://arxiv.org/abs/2302.00257},
	doi = {10.48550/arXiv.2302.00257},
	abstract = {In deep learning, often the training process finds an interpolator (a solution with 0 training loss), but the test loss is still low. This phenomenon, known as benign overfitting, is a major mystery that received a lot of recent attention. One common mechanism for benign overfitting is implicit regularization, where the training process leads to additional properties for the interpolator, often characterized by minimizing certain norms. However, even for a simple sparse linear regression problem \$y = {\textbackslash}beta{\textasciicircum}\{*{\textbackslash}top\} x +{\textbackslash}xi\$ with sparse \${\textbackslash}beta{\textasciicircum}*\$, neither minimum \${\textbackslash}ell\_1\$ or \${\textbackslash}ell\_2\$ norm interpolator gives the optimal test loss. In this work, we give a different parametrization of the model which leads to a new implicit regularization effect that combines the benefit of \${\textbackslash}ell\_1\$ and \${\textbackslash}ell\_2\$ interpolators. We show that training our new model via gradient descent leads to an interpolator with near-optimal test loss. Our result is based on careful analysis of the training dynamics and provides another example of implicit regularization effect that goes beyond norm minimization.},
	number = {{arXiv}:2302.00257},
	publisher = {{arXiv}},
	author = {Zhou, Mo and Ge, Rong},
	urldate = {2023-09-13},
	date = {2023-05-25},
	eprinttype = {arxiv},
	eprint = {2302.00257 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{zheng_improved_2023,
	title = {Improved Techniques for Maximum Likelihood Estimation for Diffusion {ODEs}},
	url = {http://arxiv.org/abs/2305.03935},
	doi = {10.48550/arXiv.2305.03935},
	abstract = {Diffusion models have exhibited excellent performance in various domains. The probability flow ordinary differential equation ({ODE}) of diffusion models (i.e., diffusion {ODEs}) is a particular case of continuous normalizing flows ({CNFs}), which enables deterministic inference and exact likelihood evaluation. However, the likelihood estimation results by diffusion {ODEs} are still far from those of the state-of-the-art likelihood-based generative models. In this work, we propose several improved techniques for maximum likelihood estimation for diffusion {ODEs}, including both training and evaluation perspectives. For training, we propose velocity parameterization and explore variance reduction techniques for faster convergence. We also derive an error-bounded high-order flow matching objective for finetuning, which improves the {ODE} likelihood and smooths its trajectory. For evaluation, we propose a novel training-free truncated-normal dequantization to fill the training-evaluation gap commonly existing in diffusion {ODEs}. Building upon these techniques, we achieve state-of-the-art likelihood estimation results on image datasets (2.56 on {CIFAR}-10, 3.43/3.69 on {ImageNet}-32) without variational dequantization or data augmentation.},
	number = {{arXiv}:2305.03935},
	publisher = {{arXiv}},
	author = {Zheng, Kaiwen and Lu, Cheng and Chen, Jianfei and Zhu, Jun},
	urldate = {2023-08-02},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2305.03935 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@online{yang_improving_2023,
	title = {Improving Adversarial Robustness of {DEQs} with Explicit Regulations Along the Neural Dynamics},
	url = {https://arxiv.org/abs/2306.01435v1},
	abstract = {Deep equilibrium ({DEQ}) models replace the multiple-layer stacking of conventional deep networks with a fixed-point iteration of a single-layer transformation. Having been demonstrated to be competitive in a variety of real-world scenarios, the adversarial robustness of general {DEQs} becomes increasingly crucial for their reliable deployment. Existing works improve the robustness of general {DEQ} models with the widely-used adversarial training ({AT}) framework, but they fail to exploit the structural uniquenesses of {DEQ} models. To this end, we interpret {DEQs} through the lens of neural dynamics and find that {AT} under-regulates intermediate states. Besides, the intermediate states typically provide predictions with a high prediction entropy. Informed by the correlation between the entropy of dynamical systems and their stability properties, we propose reducing prediction entropy by progressively updating inputs along the neural dynamics. During {AT}, we also utilize random intermediate states to compute the loss function. Our methods regulate the neural dynamics of {DEQ} models in this manner. Extensive experiments demonstrate that our methods substantially increase the robustness of {DEQ} models and even outperform the strong deep network baselines.},
	titleaddon = {{arXiv}.org},
	author = {Yang, Zonghan and Li, Peng and Pang, Tianyu and Liu, Yang},
	urldate = {2023-08-03},
	date = {2023-06-02},
	langid = {english},
}

@misc{yang_improving_2023-1,
	title = {Improving Adversarial Robustness of {DEQs} with Explicit Regulations Along the Neural Dynamics},
	url = {http://arxiv.org/abs/2306.01435},
	doi = {10.48550/arXiv.2306.01435},
	abstract = {Deep equilibrium ({DEQ}) models replace the multiple-layer stacking of conventional deep networks with a fixed-point iteration of a single-layer transformation. Having been demonstrated to be competitive in a variety of real-world scenarios, the adversarial robustness of general {DEQs} becomes increasingly crucial for their reliable deployment. Existing works improve the robustness of general {DEQ} models with the widely-used adversarial training ({AT}) framework, but they fail to exploit the structural uniquenesses of {DEQ} models. To this end, we interpret {DEQs} through the lens of neural dynamics and find that {AT} under-regulates intermediate states. Besides, the intermediate states typically provide predictions with a high prediction entropy. Informed by the correlation between the entropy of dynamical systems and their stability properties, we propose reducing prediction entropy by progressively updating inputs along the neural dynamics. During {AT}, we also utilize random intermediate states to compute the loss function. Our methods regulate the neural dynamics of {DEQ} models in this manner. Extensive experiments demonstrate that our methods substantially increase the robustness of {DEQ} models and even outperform the strong deep network baselines.},
	number = {{arXiv}:2306.01435},
	publisher = {{arXiv}},
	author = {Yang, Zonghan and Li, Peng and Pang, Tianyu and Liu, Yang},
	urldate = {2023-08-08},
	date = {2023-06-02},
	eprinttype = {arxiv},
	eprint = {2306.01435 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{straitouri_improving_2023,
	title = {Improving Expert Predictions with Conformal Prediction},
	url = {http://arxiv.org/abs/2201.12006},
	doi = {10.48550/arXiv.2201.12006},
	abstract = {Automated decision support systems promise to help human experts solve multiclass classification tasks more efficiently and accurately. However, existing systems typically require experts to understand when to cede agency to the system or when to exercise their own agency. Otherwise, the experts may be better off solving the classification tasks on their own. In this work, we develop an automated decision support system that, by design, does not require experts to understand when to trust the system to improve performance. Rather than providing (single) label predictions and letting experts decide when to trust these predictions, our system provides sets of label predictions constructed using conformal prediction\${\textbackslash}unicode\{x2014\}\$prediction sets\${\textbackslash}unicode\{x2014\}\$and forcefully asks experts to predict labels from these sets. By using conformal prediction, our system can precisely trade-off the probability that the true label is not in the prediction set, which determines how frequently our system will mislead the experts, and the size of the prediction set, which determines the difficulty of the classification task the experts need to solve using our system. In addition, we develop an efficient and near-optimal search method to find the conformal predictor under which the experts benefit the most from using our system. Simulation experiments using synthetic and real expert predictions demonstrate that our system may help experts make more accurate predictions and is robust to the accuracy of the classifier the conformal predictor relies on.},
	number = {{arXiv}:2201.12006},
	publisher = {{arXiv}},
	author = {Straitouri, Eleni and Wang, Lequn and Okati, Nastaran and Rodriguez, Manuel Gomez},
	urldate = {2023-08-02},
	date = {2023-06-30},
	eprinttype = {arxiv},
	eprint = {2201.12006 [cs, stat]},
	keywords = {Computer Science - Computers and Society, Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhu_interpolation_2023,
	title = {Interpolation for Robust Learning: Data Augmentation on Wasserstein Geodesics},
	url = {https://proceedings.mlr.press/v202/zhu23i.html},
	shorttitle = {Interpolation for Robust Learning},
	abstract = {We propose to study and promote the robustness of a model as per its performance on a continuous geodesic interpolation of subpopulations, e.g., a class of samples in a classification problem. Specifically, (1) we augment the data by finding the worst-case Wasserstein barycenter on the geodesic connecting subpopulation distributions. (2) we regularize the model for smoother performance on the continuous geodesic path connecting subpopulation distributions. (3) Additionally, we provide a theoretical guarantee of robustness improvement and investigate how the geodesic location and the sample size contribute, respectively. Experimental validations of the proposed strategy on four datasets including {CIFAR}-100 and {ImageNet}, establish the efficacy of our method, e.g., our method improves the baselines’ certifiable robustness on {CIFAR}10 upto 7.7\%, with 16.8\% on empirical robustness on {CIFAR}-100. Our work provides a new perspective of model robustness through the lens of Wasserstein geodesic-based interpolation with a practical off-the-shelf strategy that can be combined with existing robust training methods.},
	eventtitle = {International Conference on Machine Learning},
	pages = {43129--43157},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Zhu, Jiacheng and Qiu, Jielin and Guha, Aritra and Yang, Zhuolin and Nguyen, Xuanlong and Li, Bo and Zhao, Ding},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@article{park_kernel_2023,
	title = {Kernel Sufficient Dimension Reduction and Variable Selection for Compositional Data via Amalgamation},
	url = {https://openreview.net/forum?id=Q6Y1cnHMee},
	abstract = {Compositional data with a large number of components and an abundance of zeros are frequently observed in many fields recently. Analyzing such sparse high-dimensional compositional data naturally calls for dimension reduction or, more preferably, variable selection. Most existing approaches lack interpretability or cannot handle zeros properly, as they rely on a log-ratio transformation. We approach this problem with sufficient dimension reduction ({SDR}), one of the most studied dimension reduction frameworks in statistics. Characterized by the conditional independence of the data to the response on the found subspace, the {SDR} framework has been effective for both linear and nonlinear dimension reduction problems. This work proposes a compositional {SDR} that can handle zeros naturally while incorporating the nonlinear nature and spurious negative correlations among components rigorously. A critical consideration of sub-composition versus amalgamation for compositional variable selection is discussed. The proposed compositional {SDR} is shown to be statistically consistent in constructing a sub-simplex consisting of true signal variables. Simulation and real microbiome data are used to demonstrate the performance of the proposed {SDR} compared to existing state-of-art approaches.},
	author = {Park, Junyoung and Ahn, Jeongyoun and Park, Cheolwoo},
	urldate = {2023-08-08},
	date = {2023-06-15},
	langid = {english},
}

@misc{zhu_label_2023,
	title = {Label Distributionally Robust Losses for Multi-class Classification: Consistency, Robustness and Adaptivity},
	url = {http://arxiv.org/abs/2112.14869},
	doi = {10.48550/arXiv.2112.14869},
	shorttitle = {Label Distributionally Robust Losses for Multi-class Classification},
	abstract = {We study a family of loss functions named label-distributionally robust ({LDR}) losses for multi-class classification that are formulated from distributionally robust optimization ({DRO}) perspective, where the uncertainty in the given label information are modeled and captured by taking the worse case of distributional weights. The benefits of this perspective are several fold: (i) it provides a unified framework to explain the classical cross-entropy ({CE}) loss and {SVM} loss and their variants, (ii) it includes a special family corresponding to the temperature-scaled {CE} loss, which is widely adopted but poorly understood; (iii) it allows us to achieve adaptivity to the uncertainty degree of label information at an instance level. Our contributions include: (1) we study both consistency and robustness by establishing top-\$k\$ (\${\textbackslash}forall k{\textbackslash}geq 1\$) consistency of {LDR} losses for multi-class classification, and a negative result that a top-\$1\$ consistent and symmetric robust loss cannot achieve top-\$k\$ consistency simultaneously for all \$k{\textbackslash}geq 2\$; (2) we propose a new adaptive {LDR} loss that automatically adapts the individualized temperature parameter to the noise degree of class label of each instance; (3) we demonstrate stable and competitive performance for the proposed adaptive {LDR} loss on 7 benchmark datasets under 6 noisy label and 1 clean settings against 13 loss functions, and on one real-world noisy dataset. The code is open-sourced at {\textbackslash}url\{https://github.com/Optimization-{AI}/{ICML}2023\_LDR\}.},
	number = {{arXiv}:2112.14869},
	publisher = {{arXiv}},
	author = {Zhu, Dixian and Ying, Yiming and Yang, Tianbao},
	urldate = {2023-08-03},
	date = {2023-06-28},
	eprinttype = {arxiv},
	eprint = {2112.14869 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{van_der_linden_learned_2023,
	title = {Learned Gridification for Efficient Point Cloud Processing},
	url = {http://arxiv.org/abs/2307.14354},
	doi = {10.48550/arXiv.2307.14354},
	abstract = {Neural operations that rely on neighborhood information are much more expensive when deployed on point clouds than on grid data due to the irregular distances between points in a point cloud. In a grid, on the other hand, we can compute the kernel only once and reuse it for all query positions. As a result, operations that rely on neighborhood information scale much worse for point clouds than for grid data, specially for large inputs and large neighborhoods. In this work, we address the scalability issue of point cloud methods by tackling its root cause: the irregularity of the data. We propose learnable gridification as the first step in a point cloud processing pipeline to transform the point cloud into a compact, regular grid. Thanks to gridification, subsequent layers can use operations defined on regular grids, e.g., Conv3D, which scale much better than native point cloud methods. We then extend gridification to point cloud to point cloud tasks, e.g., segmentation, by adding a learnable de-gridification step at the end of the point cloud processing pipeline to map the compact, regular grid back to its original point cloud form. Through theoretical and empirical analysis, we show that gridified networks scale better in terms of memory and time than networks directly applied on raw point cloud data, while being able to achieve competitive results. Our code is publicly available at https://github.com/computri/gridifier.},
	number = {{arXiv}:2307.14354},
	publisher = {{arXiv}},
	author = {van der Linden, Putri A. and Romero, David W. and Bekkers, Erik J.},
	urldate = {2023-09-13},
	date = {2023-07-22},
	eprinttype = {arxiv},
	eprint = {2307.14354 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{gabel_learning_2023,
	title = {Learning Lie Group Symmetry Transformations with Neural Networks},
	url = {http://arxiv.org/abs/2307.01583},
	doi = {10.48550/arXiv.2307.01583},
	abstract = {The problem of detecting and quantifying the presence of symmetries in datasets is useful for model selection, generative modeling, and data analysis, amongst others. While existing methods for hard-coding transformations in neural networks require prior knowledge of the symmetries of the task at hand, this work focuses on discovering and characterizing unknown symmetries present in the dataset, namely, Lie group symmetry transformations beyond the traditional ones usually considered in the field (rotation, scaling, and translation). Specifically, we consider a scenario in which a dataset has been transformed by a one-parameter subgroup of transformations with different parameter values for each data point. Our goal is to characterize the transformation group and the distribution of the parameter values. The results showcase the effectiveness of the approach in both these settings.},
	number = {{arXiv}:2307.01583},
	publisher = {{arXiv}},
	author = {Gabel, Alex and Klein, Victoria and Valperga, Riccardo and Lamb, Jeroen S. W. and Webster, Kevin and Quax, Rick and Gavves, Efstratios},
	urldate = {2023-09-13},
	date = {2023-07-04},
	eprinttype = {arxiv},
	eprint = {2307.01583 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@article{hannouch_learning_nodate,
	title = {Learning to See Topological Properties in 4D Using Convolutional Neural Networks},
	abstract = {Topology describes the essential structure of a space, and in 4D, a larger variety of topologically distinct manifolds can be embedded versus 2D or 3D. The present study investigates an end-to-end visual approach, which couples data generation software and convolutional neural networks ({CNNs}) to estimate the topology of 4D data. A synthetic 4D training data set is generated with the use of several manifolds, and then labelled with their associated Betti numbers by using techniques from algebraic topology. Several approaches to implementing a 4D convolution layer are compared. Experiments demonstrate that already a basic {CNN} can be trained to provide estimates for the Betti numbers associated with the number of one-, two-, and three-dimensional holes in the data. Some of the intricacies of topological data analysis in the 4D setting are also put on view, including aspects of persistent homology.},
	author = {Hannouch, Khalil Mathieu and Chalup, Stephan},
	langid = {english},
}

@article{kang_leveraging_2023,
	title = {Leveraging Proxy of Training Data for Test-Time Adaptation},
	url = {https://openreview.net/forum?id=VVGNInOAm9},
	abstract = {We consider test-time adaptation ({TTA}), the task of adapting a trained model to an arbitrary test domain using unlabeled input data on-the-fly during testing. A common practice of {TTA} is to disregard data used in training due to large memory demand and privacy leakage. However, the training data are the only source of supervision. This motivates us to investigate a proper way of using them while minimizing the side effects. To this end, we propose two lightweight yet informative proxies of the training data and a {TTA} method fully exploiting them. One of the proxies is composed of a small number of images synthesized (hence, less privacy-sensitive) by data condensation which minimizes their domain-specificity to capture a general underlying structure over a wide spectrum of domains. Then, in {TTA}, they are translated into labeled test data by stylizing them to match styles of unlabeled test samples. This enables virtually supervised test-time training. The other proxy is inter-class relations of training data, which are transferred to target model during {TTA}. On four public benchmarks, our method outperforms the state-of-the-art ones at remarkably less computation and memory.},
	author = {Kang, Juwon and Kim, Nayeong and Kwon, Donghyeon and Ok, Jungseul and Kwak, Suha},
	urldate = {2023-09-13},
	date = {2023-06-15},
	langid = {english},
}

@article{akhound-sadegh_lie_2023,
	title = {Lie Point Symmetry and Physics Informed Networks},
	url = {https://openreview.net/forum?id=VlUf77e9cR&referrer=%5Bthe%20profile%20of%20Max%20Welling%5D(%2Fprofile%3Fid%3D~Max_Welling1)},
	abstract = {Physics-informed neural networks ({PINNs}) are computationally efficient alternatives to traditional partial differential equation ({PDE}) solvers. However, their reliability is dependent on the accuracy of the trained neural network. In this work, we introduce a mechanism for leveraging the symmetries of a given {PDE} to improve {PINN} performance. In particular, we propose a loss function that informs the network about Lie point symmetries, similar to how traditional {PINN} models try to enforce the underlying {PDE}. Intuitively, our symmetry loss ensures that infinitesimal generators of the Lie group preserve solutions of the {PDE}. Effectively, this means that once the network learns a solution, it also learns the neighbouring solutions generated by Lie point symmetries. Our results confirm that Lie point symmetries of the respective {PDEs} are an effective inductive bias for {PINNs} and can lead to a significant increase in sample efficiency.},
	author = {Akhound-Sadegh, Tara and Perreault-Levasseur, Laurence and Brandstetter, Johannes and Welling, Max and Ravanbakhsh, Siamak},
	urldate = {2023-09-13},
	date = {2023-06-18},
	langid = {english},
}

@misc{liu_linear_2023,
	title = {Linear Regression on Manifold Structured Data: the Impact of Extrinsic Geometry on Solutions},
	url = {http://arxiv.org/abs/2307.02478},
	doi = {10.48550/arXiv.2307.02478},
	shorttitle = {Linear Regression on Manifold Structured Data},
	abstract = {In this paper, we study linear regression applied to data structured on a manifold. We assume that the data manifold is smooth and is embedded in a Euclidean space, and our objective is to reveal the impact of the data manifold's extrinsic geometry on the regression. Specifically, we analyze the impact of the manifold's curvatures (or higher order nonlinearity in the parameterization when the curvatures are locally zero) on the uniqueness of the regression solution. Our findings suggest that the corresponding linear regression does not have a unique solution when the embedded submanifold is flat in some dimensions. Otherwise, the manifold's curvature (or higher order nonlinearity in the embedding) may contribute significantly, particularly in the solution associated with the normal directions of the manifold. Our findings thus reveal the role of data manifold geometry in ensuring the stability of regression models for out-of-distribution inferences.},
	number = {{arXiv}:2307.02478},
	publisher = {{arXiv}},
	author = {Liu, Liangchen and He, Juncai and Tsai, Richard},
	urldate = {2023-09-13},
	date = {2023-07-22},
	eprinttype = {arxiv},
	eprint = {2307.02478 [cs, math]},
	keywords = {53Z50 62J05 (Primary) 65D18 68T07 (Secondary), Computer Science - Machine Learning, G.1.2, G.4, Mathematics - Differential Geometry},
}

@article{wipf_marginalization_2023,
	title = {Marginalization is not Marginal: No Bad {VAE} Local Minima when Learning Optimal Sparse Representations},
	url = {https://openreview.net/forum?id=NUtErghzv4},
	shorttitle = {Marginalization is not Marginal},
	abstract = {Although the variational autoencoder ({VAE}) represents a widely-used deep generative model, the underlying energy function when applied to continuous data remains poorly understood. In fact, most prior theoretical analysis has assumed a simplified affine decoder such that the model collapses to probabilistic {PCA}, a restricted regime whereby existing classical algorithms can also be trivially applied to guarantee globally optimal solutions. To push our understanding into more complex, practically-relevant settings, this paper instead adopts a deceptively sophisticated single-layer decoder that nonetheless allows the {VAE} to address the fundamental challenge of learning optimally sparse representations of continuous data originating from popular multiple-response regression models. In doing so, we can then examine {VAE} properties within the non-trivial context of solving difficult, {NP}-hard inverse problems. More specifically, we prove rigorous conditions which guarantee that any minimum of the {VAE} energy (local or global) will produce the optimally sparse latent representation, meaning zero reconstruction error using a minimal number of active latent dimensions. This is ultimately possible because {VAE} marginalization over the latent posterior selectively smooths away bad local minima as has been conjectured but not actually proven in prior work. We then discuss how equivalent-capacity deterministic autoencoders, even with appropriate sparsity-promoting regularization of the latent space, maintain bad local minima that do not correspond with such parsimonious representations. Overall, these results serve to elucidate key properties of the {VAE} loss surface relative to finding low-dimensional structure in data.},
	author = {Wipf, David},
	urldate = {2023-08-02},
	date = {2023-06-15},
	langid = {english},
}

@misc{lin_minimal_2023,
	title = {Minimal Random Code Learning with Mean-{KL} Parameterization},
	url = {http://arxiv.org/abs/2307.07816},
	doi = {10.48550/arXiv.2307.07816},
	abstract = {This paper studies the qualitative behavior and robustness of two variants of Minimal Random Code Learning ({MIRACLE}) used to compress variational Bayesian neural networks. {MIRACLE} implements a powerful, conditionally Gaussian variational approximation for the weight posterior \$Q\_\{{\textbackslash}mathbf\{w\}\}\$ and uses relative entropy coding to compress a weight sample from the posterior using a Gaussian coding distribution \$P\_\{{\textbackslash}mathbf\{w\}\}\$. To achieve the desired compression rate, \$D\_\{{\textbackslash}mathrm\{{KL}\}\}[Q\_\{{\textbackslash}mathbf\{w\}\} {\textbackslash}Vert P\_\{{\textbackslash}mathbf\{w\}\}]\$ must be constrained, which requires a computationally expensive annealing procedure under the conventional mean-variance (Mean-Var) parameterization for \$Q\_\{{\textbackslash}mathbf\{w\}\}\$. Instead, we parameterize \$Q\_\{{\textbackslash}mathbf\{w\}\}\$ by its mean and {KL} divergence from \$P\_\{{\textbackslash}mathbf\{w\}\}\$ to constrain the compression cost to the desired value by construction. We demonstrate that variational training with Mean-{KL} parameterization converges twice as fast and maintains predictive performance after compression. Furthermore, we show that Mean-{KL} leads to more meaningful variational distributions with heavier tails and compressed weight samples which are more robust to pruning.},
	number = {{arXiv}:2307.07816},
	publisher = {{arXiv}},
	author = {Lin, Jihao Andreas and Flamich, Gergely and Hernández-Lobato, José Miguel},
	urldate = {2023-09-13},
	date = {2023-07-15},
	eprinttype = {arxiv},
	eprint = {2307.07816 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ballu_mirror_2023,
	title = {Mirror Sinkhorn: Fast Online Optimization on Transport Polytopes},
	url = {http://arxiv.org/abs/2211.10420},
	doi = {10.48550/arXiv.2211.10420},
	shorttitle = {Mirror Sinkhorn},
	abstract = {Optimal transport is an important tool in machine learning, allowing to capture geometric properties of the data through a linear program on transport polytopes. We present a single-loop optimization algorithm for minimizing general convex objectives on these domains, utilizing the principles of Sinkhorn matrix scaling and mirror descent. The proposed algorithm is robust to noise, and can be used in an online setting. We provide theoretical guarantees for convex objectives and experimental results showcasing it effectiveness on both synthetic and real-world data.},
	number = {{arXiv}:2211.10420},
	publisher = {{arXiv}},
	author = {Ballu, Marin and Berthet, Quentin},
	urldate = {2023-08-02},
	date = {2023-06-20},
	eprinttype = {arxiv},
	eprint = {2211.10420 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{yang_mitigating_2023,
	title = {Mitigating Spurious Correlations in Multi-modal Models during Fine-tuning},
	url = {http://arxiv.org/abs/2304.03916},
	doi = {10.48550/arXiv.2304.03916},
	abstract = {Spurious correlations that degrade model generalization or lead the model to be right for the wrong reasons are one of the main robustness concerns for real-world deployments. However, mitigating these correlations during pre-training for large-scale models can be costly and impractical, particularly for those without access to high-performance computing resources. This paper proposes a novel approach to address spurious correlations during fine-tuning for a given domain of interest. With a focus on multi-modal models (e.g., {CLIP}), the proposed method leverages different modalities in these models to detect and explicitly set apart spurious attributes from the affected class, achieved through a multi-modal contrastive loss function that expresses spurious relationships through language. Our experimental results and in-depth visualizations on {CLIP} show that such an intervention can effectively i) improve the model's accuracy when spurious attributes are not present, and ii) directs the model's activation maps towards the actual class rather than the spurious attribute when present. In particular, on the Waterbirds dataset, our algorithm achieved a worst-group accuracy 23\% higher than {ERM} on {CLIP} with a {ResNet}-50 backbone, and 32\% higher on {CLIP} with a {ViT} backbone, while maintaining the same average accuracy as {ERM}.},
	number = {{arXiv}:2304.03916},
	publisher = {{arXiv}},
	author = {Yang, Yu and Nushi, Besmira and Palangi, Hamid and Mirzasoleiman, Baharan},
	urldate = {2023-09-13},
	date = {2023-05-30},
	eprinttype = {arxiv},
	eprint = {2304.03916 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{hodgkinson_monotonicity_2023,
	title = {Monotonicity and Double Descent in Uncertainty Estimation with Gaussian Processes},
	url = {http://arxiv.org/abs/2210.07612},
	doi = {10.48550/arXiv.2210.07612},
	abstract = {Despite their importance for assessing reliability of predictions, uncertainty quantification ({UQ}) measures for machine learning models have only recently begun to be rigorously characterized. One prominent issue is the curse of dimensionality: it is commonly believed that the marginal likelihood should be reminiscent of cross-validation metrics and that both should deteriorate with larger input dimensions. We prove that by tuning hyperparameters to maximize marginal likelihood (the empirical Bayes procedure), the performance, as measured by the marginal likelihood, improves monotonically with the input dimension. On the other hand, we prove that cross-validation metrics exhibit qualitatively different behavior that is characteristic of double descent. Cold posteriors, which have recently attracted interest due to their improved performance in certain settings, appear to exacerbate these phenomena. We verify empirically that our results hold for real data, beyond our considered assumptions, and we explore consequences involving synthetic covariates.},
	number = {{arXiv}:2210.07612},
	publisher = {{arXiv}},
	author = {Hodgkinson, Liam and van der Heide, Chris and Roosta, Fred and Mahoney, Michael W.},
	urldate = {2023-08-02},
	date = {2023-07-25},
	eprinttype = {arxiv},
	eprint = {2210.07612 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{saito_multi-class_2023,
	title = {Multi-class Graph Clustering via Approximated Effective \$p\$-Resistance},
	url = {http://arxiv.org/abs/2306.08617},
	doi = {10.48550/arXiv.2306.08617},
	abstract = {This paper develops an approximation to the (effective) \$p\$-resistance and applies it to multi-class clustering. Spectral methods based on the graph Laplacian and its generalization to the graph \$p\$-Laplacian have been a backbone of non-euclidean clustering techniques. The advantage of the \$p\$-Laplacian is that the parameter \$p\$ induces a controllable bias on cluster structure. The drawback of \$p\$-Laplacian eigenvector based methods is that the third and higher eigenvectors are difficult to compute. Thus, instead, we are motivated to use the \$p\$-resistance induced by the \$p\$-Laplacian for clustering. For \$p\$-resistance, small \$p\$ biases towards clusters with high internal connectivity while large \$p\$ biases towards clusters of small "extent," that is a preference for smaller shortest-path distances between vertices in the cluster. However, the \$p\$-resistance is expensive to compute. We overcome this by developing an approximation to the \$p\$-resistance. We prove upper and lower bounds on this approximation and observe that it is exact when the graph is a tree. We also provide theoretical justification for the use of \$p\$-resistance for clustering. Finally, we provide experiments comparing our approximated \$p\$-resistance clustering to other \$p\$-Laplacian based methods.},
	number = {{arXiv}:2306.08617},
	publisher = {{arXiv}},
	author = {Saito, Shota and Herbster, Mark},
	urldate = {2023-08-02},
	date = {2023-07-18},
	eprinttype = {arxiv},
	eprint = {2306.08617 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{maurais_multi-fidelity_2023,
	title = {Multi-Fidelity Covariance Estimation in the Log-Euclidean Geometry},
	url = {http://arxiv.org/abs/2301.13749},
	doi = {10.48550/arXiv.2301.13749},
	abstract = {We introduce a multi-fidelity estimator of covariance matrices that employs the log-Euclidean geometry of the symmetric positive-definite manifold. The estimator fuses samples from a hierarchy of data sources of differing fidelities and costs for variance reduction while guaranteeing definiteness, in contrast with previous approaches. The new estimator makes covariance estimation tractable in applications where simulation or data collection is expensive; to that end, we develop an optimal sample allocation scheme that minimizes the mean-squared error of the estimator given a fixed budget. Guaranteed definiteness is crucial to metric learning, data assimilation, and other downstream tasks. Evaluations of our approach using data from physical applications (heat conduction, fluid dynamics) demonstrate more accurate metric learning and speedups of more than one order of magnitude compared to benchmarks.},
	number = {{arXiv}:2301.13749},
	publisher = {{arXiv}},
	author = {Maurais, Aimee and Alsup, Terrence and Peherstorfer, Benjamin and Marzouk, Youssef},
	urldate = {2023-08-02},
	date = {2023-05-26},
	eprinttype = {arxiv},
	eprint = {2301.13749 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Computation},
}

@inproceedings{yao_multiadam_2023,
	title = {{MultiAdam}: Parameter-wise Scale-invariant Optimizer for Multiscale Training of Physics-informed Neural Networks},
	url = {https://proceedings.mlr.press/v202/yao23c.html},
	shorttitle = {{MultiAdam}},
	abstract = {Physics-informed Neural Networks ({PINNs}) have recently achieved remarkable progress in solving Partial Differential Equations ({PDEs}) in various fields by minimizing a weighted sum of {PDE} loss and boundary loss. However, there are several critical challenges in the training of {PINNs}, including the lack of theoretical frameworks and the imbalance between {PDE} loss and boundary loss. In this paper, we present an analysis of second-order non-homogeneous {PDEs}, which are classified into three categories and applicable to various common problems. We also characterize the connections between the training loss and actual error, guaranteeing convergence under mild conditions. The theoretical analysis inspires us to further propose {MultiAdam}, a scale-invariant optimizer that leverages gradient momentum to parameter-wisely balance the loss terms. Extensive experiment results on multiple problems from different physical domains demonstrate that our {MultiAdam} solver can improve the predictive accuracy by 1-2 orders of magnitude compared with strong baselines.},
	eventtitle = {International Conference on Machine Learning},
	pages = {39702--39721},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Yao, Jiachen and Su, Chang and Hao, Zhongkai and Liu, Songming and Su, Hang and Zhu, Jun},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{ayme_naive_2023,
	title = {Naive imputation implicitly regularizes high-dimensional linear models},
	url = {http://arxiv.org/abs/2301.13585},
	doi = {10.48550/arXiv.2301.13585},
	abstract = {Two different approaches exist to handle missing values for prediction: either imputation, prior to fitting any predictive algorithms, or dedicated methods able to natively incorporate missing values. While imputation is widely (and easily) use, it is unfortunately biased when low-capacity predictors (such as linear models) are applied afterward. However, in practice, naive imputation exhibits good predictive performance. In this paper, we study the impact of imputation in a high-dimensional linear model with {MCAR} missing data. We prove that zero imputation performs an implicit regularization closely related to the ridge method, often used in high-dimensional problems. Leveraging on this connection, we establish that the imputation bias is controlled by a ridge bias, which vanishes in high dimension. As a predictor, we argue in favor of the averaged {SGD} strategy, applied to zero-imputed data. We establish an upper bound on its generalization error, highlighting that imputation is benign in the d \${\textbackslash}sqrt\$ n regime. Experiments illustrate our findings.},
	number = {{arXiv}:2301.13585},
	publisher = {{arXiv}},
	author = {Ayme, Alexis and Boyer, Claire and Dieuleveut, Aymeric and Scornet, Erwan},
	urldate = {2023-09-13},
	date = {2023-01-31},
	eprinttype = {arxiv},
	eprint = {2301.13585 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@misc{fu_nerfool_2023,
	title = {{NeRFool}: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations},
	url = {http://arxiv.org/abs/2306.06359},
	doi = {10.48550/arXiv.2306.06359},
	shorttitle = {{NeRFool}},
	abstract = {Generalizable Neural Radiance Fields ({GNeRF}) are one of the most promising real-world solutions for novel view synthesis, thanks to their cross-scene generalization capability and thus the possibility of instant rendering on new scenes. While adversarial robustness is essential for real-world applications, little study has been devoted to understanding its implication on {GNeRF}. We hypothesize that because {GNeRF} is implemented by conditioning on the source views from new scenes, which are often acquired from the Internet or third-party providers, there are potential new security concerns regarding its real-world applications. Meanwhile, existing understanding and solutions for neural networks' adversarial robustness may not be applicable to {GNeRF}, due to its 3D nature and uniquely diverse operations. To this end, we present {NeRFool}, which to the best of our knowledge is the first work that sets out to understand the adversarial robustness of {GNeRF}. Specifically, {NeRFool} unveils the vulnerability patterns and important insights regarding {GNeRF}'s adversarial robustness. Built upon the above insights gained from {NeRFool}, we further develop {NeRFool}+, which integrates two techniques capable of effectively attacking {GNeRF} across a wide range of target views, and provide guidelines for defending against our proposed attacks. We believe that our {NeRFool}/{NeRFool}+ lays the initial foundation for future innovations in developing robust real-world {GNeRF} solutions. Our codes are available at: https://github.com/{GATECH}-{EIC}/{NeRFool}.},
	number = {{arXiv}:2306.06359},
	publisher = {{arXiv}},
	author = {Fu, Yonggan and Yuan, Ye and Kundu, Souvik and Wu, Shang and Zhang, Shunyao and Lin, Yingyan},
	urldate = {2023-09-13},
	date = {2023-06-10},
	eprinttype = {arxiv},
	eprint = {2306.06359 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
}

@inproceedings{li_nesterov_2023,
	title = {Nesterov Meets Optimism: Rate-Optimal Separable Minimax Optimization},
	url = {https://proceedings.mlr.press/v202/li23aq.html},
	shorttitle = {Nesterov Meets Optimism},
	abstract = {We propose a new first-order optimization algorithm — {AcceleratedGradient}-{OptimisticGradient} ({AG}-{OG}) Descent Ascent—for separable convex-concave minimax optimization. The main idea of our algorithm is to carefully leverage the structure of the minimax problem, performing Nesterov acceleration on the individual component and optimistic gradient on the coupling component. Equipped with proper restarting, we show that {AG}-{OG} achieves the optimal convergence rate (up to a constant) for a variety of settings, including bilinearly coupled strongly convex-strongly concave minimax optimization (bi-{SC}-{SC}), bilinearly coupled convex-strongly concave minimax optimization (bi-C-{SC}), and bilinear games. We also extend our algorithm to the stochastic setting and achieve the optimal convergence rate in both bi-{SC}-{SC} and bi-C-{SC} settings. {AG}-{OG} is the first single-call algorithm with optimal convergence rates in both deterministic and stochastic settings for bilinearly coupled minimax optimization problems.},
	eventtitle = {International Conference on Machine Learning},
	pages = {20351--20383},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Li, Chris Junchi and Yuan, Huizhuo and Gidel, Gauthier and Gu, Quanquan and Jordan, Michael},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@online{refinetti_neural_2022,
	title = {Neural networks trained with {SGD} learn distributions of increasing complexity},
	url = {https://arxiv.org/abs/2211.11567v2},
	abstract = {The ability of deep neural networks to generalise well even when they interpolate their training data has been explained using various "simplicity biases". These theories postulate that neural networks avoid overfitting by first learning simple functions, say a linear classifier, before learning more complex, non-linear functions. Meanwhile, data structure is also recognised as a key ingredient for good generalisation, yet its role in simplicity biases is not yet understood. Here, we show that neural networks trained using stochastic gradient descent initially classify their inputs using lower-order input statistics, like mean and covariance, and exploit higher-order statistics only later during training. We first demonstrate this distributional simplicity bias ({DSB}) in a solvable model of a neural network trained on synthetic data. We empirically demonstrate {DSB} in a range of deep convolutional networks and visual transformers trained on {CIFAR}10, and show that it even holds in networks pre-trained on {ImageNet}. We discuss the relation of {DSB} to other simplicity biases and consider its implications for the principle of Gaussian universality in learning.},
	titleaddon = {{arXiv}.org},
	author = {Refinetti, Maria and Ingrosso, Alessandro and Goldt, Sebastian},
	urldate = {2023-08-03},
	date = {2022-11-21},
	langid = {english},
}

@misc{altekruger_neural_2023,
	title = {Neural Wasserstein Gradient Flows for Maximum Mean Discrepancies with Riesz Kernels},
	url = {http://arxiv.org/abs/2301.11624},
	abstract = {Wasserstein gradient flows of maximum mean discrepancy ({MMD}) functionals with non-smooth Riesz kernels show a rich structure as singular measures can become absolutely continuous ones and conversely. In this paper we contribute to the understanding of such flows. We propose to approximate the backward scheme of Jordan, Kinderlehrer and Otto for computing such Wasserstein gradient flows as well as a forward scheme for so-called Wasserstein steepest descent flows by neural networks ({NNs}). Since we cannot restrict ourselves to absolutely continuous measures, we have to deal with transport plans and velocity plans instead of usual transport maps and velocity fields. Indeed, we approximate the disintegration of both plans by generative {NNs} which are learned with respect to appropriate loss functions. In order to evaluate the quality of both neural schemes, we benchmark them on the interaction energy. Here we provide analytic formulas for Wasserstein schemes starting at a Dirac measure and show their convergence as the time step size tends to zero. Finally, we illustrate our neural {MMD} flows by numerical examples.},
	number = {{arXiv}:2301.11624},
	publisher = {{arXiv}},
	author = {Altekrüger, Fabian and Hertrich, Johannes and Steidl, Gabriele},
	urldate = {2023-08-02},
	date = {2023-06-02},
	eprinttype = {arxiv},
	eprint = {2301.11624 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Mathematics - Probability},
}

@article{keller_neural_2023,
	title = {Neural Wave Machines: Learning Spatiotemporally Structured Representations with Locally Coupled Oscillatory Recurrent Neural Networks},
	url = {https://openreview.net/forum?id=5tJSt3kn4s},
	shorttitle = {Neural Wave Machines},
	abstract = {Traveling waves have been measured at a diversity of regions and scales in the brain, however a consensus as to their computational purpose has yet to be reached. An intriguing hypothesis is that traveling waves serve to structure neural representations both in space and time, thereby acting as an inductive bias towards natural data. In this work, we investigate this hypothesis by introducing the Neural Wave Machine ({NWM}) -- a locally coupled oscillatory recurrent neural network capable of exhibiting traveling waves in its hidden state. After training on simple dynamic sequences, we show that this model indeed learns static spatial structure such as topographic organization, and further uses complex spatiotemporal structure such as traveling waves to encode observed transformations. To measure the computational implications of this structure, we use a suite of sequence classification and physical dynamics modeling tasks to show that the {NWM} is both more parameter efficient, and is able to forecast future trajectories of simple physical dynamical systems more accurately than existing state of the art counterparts.},
	author = {Keller, T. Anderson and Welling, Max},
	urldate = {2023-08-02},
	date = {2023-06-15},
	langid = {english},
}

@inproceedings{keller_neural_2023-1,
	title = {Neural Wave Machines: Learning Spatiotemporally Structured Representations with Locally Coupled Oscillatory Recurrent Neural Networks},
	url = {https://proceedings.mlr.press/v202/keller23a.html},
	shorttitle = {Neural Wave Machines},
	abstract = {Traveling waves have been measured at a diversity of regions and scales in the brain, however a consensus as to their computational purpose has yet to be reached. An intriguing hypothesis is that traveling waves serve to structure neural representations both in space and time, thereby acting as an inductive bias towards natural data. In this work, we investigate this hypothesis by introducing the Neural Wave Machine ({NWM}) – a locally coupled oscillatory recurrent neural network capable of exhibiting traveling waves in its hidden state. After training on simple dynamic sequences, we show that this model indeed learns static spatial structure such as topographic organization, and further uses complex spatiotemporal structure such as traveling waves to encode observed transformations. To measure the computational implications of this structure, we use a suite of sequence classification and physical dynamics modeling tasks to show that the {NWM} is both more parameter efficient, and is able to forecast future trajectories of simple physical dynamical systems more accurately than existing state of the art counterparts.},
	eventtitle = {International Conference on Machine Learning},
	pages = {16168--16189},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Keller, T. Anderson and Welling, Max},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{zhou_nnsplitter_2023,
	title = {{NNSplitter}: An Active Defense Solution for {DNN} Model via Automated Weight Obfuscation},
	url = {http://arxiv.org/abs/2305.00097},
	doi = {10.48550/arXiv.2305.00097},
	shorttitle = {{NNSplitter}},
	abstract = {As a type of valuable intellectual property ({IP}), deep neural network ({DNN}) models have been protected by techniques like watermarking. However, such passive model protection cannot fully prevent model abuse. In this work, we propose an active model {IP} protection scheme, namely {NNSplitter}, which actively protects the model by splitting it into two parts: the obfuscated model that performs poorly due to weight obfuscation, and the model secrets consisting of the indexes and original values of the obfuscated weights, which can only be accessed by authorized users with the support of the trusted execution environment. Experimental results demonstrate the effectiveness of {NNSplitter}, e.g., by only modifying 275 out of over 11 million (i.e., 0.002\%) weights, the accuracy of the obfuscated {ResNet}-18 model on {CIFAR}-10 can drop to 10\%. Moreover, {NNSplitter} is stealthy and resilient against norm clipping and fine-tuning attacks, making it an appealing solution for {DNN} model protection. The code is available at: https://github.com/Tongzhou0101/{NNSplitter}.},
	number = {{arXiv}:2305.00097},
	publisher = {{arXiv}},
	author = {Zhou, Tong and Luo, Yukui and Ren, Shaolei and Xu, Xiaolin},
	urldate = {2023-09-13},
	date = {2023-08-16},
	eprinttype = {arxiv},
	eprint = {2305.00097 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@article{grande_non-isotropic_2023,
	title = {Non-isotropic Persistent Homology},
	url = {https://openreview.net/forum?id=INO5ePB5yk},
	abstract = {Persistent Homology is a widely used topological data analysis tool that creates a concise description of the topological properties of a point cloud based on a specified filtration. Most of these filtrations used for persistent homology depend (implicitly) on a chosen metric, which is typically agnostically chosen as the standard euclidean metric on \${\textbackslash}mathbb\{R\}{\textasciicircum}n\$. Recent work has tried to uncover the "true" metric on the point cloud using distance-to-measure functions, in order to obtain more meaningful persistent homology results. Here we propose an alternative look at this problem: we posit that information on the point cloud is lost when restricting persistent homology to a single (correct) distance function. Instead, we show how by varying the distance function on the underlying space and analysing the corresponding shifts in the persistence diagrams, we can extract additional topological and geometrical information. Finally, we show in synthetic experiments that non-isotropic persistent homology ({NIPH}) can extract information on orientation, orientational variance, and scaling of randomly generated point clouds with good accuracy.},
	author = {Grande, Vincent Peter and Schaub, Michael T.},
	urldate = {2023-09-13},
	date = {2023-06-18},
	langid = {english},
}

@misc{nielsen_non-linear_2023,
	title = {Non-linear Embeddings in Hilbert Simplex Geometry},
	url = {http://arxiv.org/abs/2203.11434},
	doi = {10.48550/arXiv.2203.11434},
	abstract = {A key technique of machine learning and computer vision is to embed discrete weighted graphs into continuous spaces for further downstream processing. Embedding discrete hierarchical structures in hyperbolic geometry has proven very successful since it was shown that any weighted tree can be embedded in that geometry with arbitrary low distortion. Various optimization methods for hyperbolic embeddings based on common models of hyperbolic geometry have been studied. In this paper, we consider Hilbert geometry for the standard simplex which is isometric to a vector space equipped with the variation polytope norm. We study the representation power of this Hilbert simplex geometry by embedding distance matrices of graphs. Our findings demonstrate that Hilbert simplex geometry is competitive to alternative geometries such as the Poincar{\textbackslash}'e hyperbolic ball or the Euclidean geometry for embedding tasks while being fast and numerically robust.},
	number = {{arXiv}:2203.11434},
	publisher = {{arXiv}},
	author = {Nielsen, Frank and Sun, Ke},
	urldate = {2023-09-13},
	date = {2023-08-16},
	eprinttype = {arxiv},
	eprint = {2203.11434 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{wei_ntk-approximating_2023,
	title = {{NTK}-approximating {MLP} Fusion for Efficient Language Model Fine-tuning},
	url = {http://arxiv.org/abs/2307.08941},
	doi = {10.48550/arXiv.2307.08941},
	abstract = {Fine-tuning a pre-trained language model ({PLM}) emerges as the predominant strategy in many natural language processing applications. However, even fine-tuning the {PLMs} and doing inference are expensive, especially on edge devices with low computing power. Some general approaches (e.g. quantization and distillation) have been widely studied to reduce the compute/memory of {PLM} fine-tuning, while very few one-shot compression techniques are explored. In this paper, we investigate the neural tangent kernel ({NTK})--which reveals the gradient descent dynamics of neural networks--of the multilayer perceptrons ({MLP}) modules in a {PLM} and propose to coin a lightweight {PLM} through {NTK}-approximating {MLP} fusion. To achieve this, we reconsider the {MLP} as a bundle of sub-{MLPs}, and cluster them into a given number of centroids, which can then be restored as a compressed {MLP} and surprisingly shown to well approximate the {NTK} of the original {PLM}. Extensive experiments of {PLM} fine-tuning on both natural language understanding ({NLU}) and generation ({NLG}) tasks are provided to verify the effectiveness of the proposed method {MLP} fusion. Our code is available at https://github.com/weitianxin/{MLP}\_Fusion.},
	number = {{arXiv}:2307.08941},
	publisher = {{arXiv}},
	author = {Wei, Tianxin and Guo, Zeming and Chen, Yifan and He, Jingrui},
	urldate = {2023-08-08},
	date = {2023-08-04},
	eprinttype = {arxiv},
	eprint = {2307.08941 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@inproceedings{dominguez-olmedo_data_2023,
	title = {On Data Manifolds Entailed by Structural Causal Models},
	url = {https://proceedings.mlr.press/v202/dominguez-olmedo23a.html},
	abstract = {The geometric structure of data is an important inductive bias in machine learning. In this work, we characterize the data manifolds entailed by structural causal models. The strengths of the proposed framework are twofold: firstly, the geometric structure of the data manifolds is causally informed, and secondly, it enables causal reasoning about the data manifolds in an interventional and a counterfactual sense. We showcase the versatility of the proposed framework by applying it to the generation of causally-grounded counterfactual explanations for machine learning classifiers, measuring distances along the data manifold in a differential geometric-principled manner.},
	eventtitle = {International Conference on Machine Learning},
	pages = {8188--8201},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Dominguez-Olmedo, Ricardo and Karimi, Amir-Hossein and Arvanitidis, Georgios and Schölkopf, Bernhard},
	urldate = {2023-08-03},
	date = {2023-07-03},
        year = {2023},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{vyas_provable_2023,
	title = {On Provable Copyright Protection for Generative Models},
	url = {http://arxiv.org/abs/2302.10870},
	doi = {10.48550/arXiv.2302.10870},
	abstract = {There is a growing concern that learned conditional generative models may output samples that are substantially similar to some copyrighted data \$C\$ that was in their training set. We give a formal definition of \${\textbackslash}textit\{near access-freeness ({NAF})\}\$ and prove bounds on the probability that a model satisfying this definition outputs a sample similar to \$C\$, even if \$C\$ is included in its training set. Roughly speaking, a generative model \$p\$ is \${\textbackslash}textit\{\$k\$-{NAF}\}\$ if for every potentially copyrighted data \$C\$, the output of \$p\$ diverges by at most \$k\$-bits from the output of a model \$q\$ that \${\textbackslash}textit\{did not access \$C\$ at all\}\$. We also give generative model learning algorithms, which efficiently modify the original generative model learning algorithm in a black box manner, that output generative models with strong bounds on the probability of sampling protected content. Furthermore, we provide promising experiments for both language (transformers) and image (diffusion) generative models, showing minimal degradation in output quality while ensuring strong protections against sampling protected content.},
	number = {{arXiv}:2302.10870},
	publisher = {{arXiv}},
	author = {Vyas, Nikhil and Kakade, Sham and Barak, Boaz},
	urldate = {2023-08-02},
	date = {2023-07-21},
	eprinttype = {arxiv},
	eprint = {2302.10870 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{min_convergence_2023,
	title = {On the Convergence of Gradient Flow on Multi-layer Linear Models},
	url = {https://proceedings.mlr.press/v202/min23d.html},
	abstract = {In this paper, we analyze the convergence of gradient flow on a multi-layer linear model with a loss function of the form ��(��1��2⋯����)f(W1W2⋯{WL})f(W\_1W\_2{\textbackslash}cdots W\_L). We show that when ��ff satisfies the gradient dominance property, proper weight initialization leads to exponential convergence of the gradient flow to a global minimum of the loss. Moreover, the convergence rate depends on two trajectory-specific quantities that are controlled by the weight initialization: the imbalance matrices, which measure the difference between the weights of adjacent layers, and the least singular value of the weight product ��=��1��2⋯����W=W1W2⋯{WLW}=W\_1W\_2{\textbackslash}cdots W\_L. Our analysis exploits the fact that the gradient of the overparameterized loss can be written as the composition of the non-overparametrized gradient with a time-varying (weight-dependent) linear operator whose smallest eigenvalue controls the convergence rate. The key challenge we address is to derive a uniform lower bound for this time-varying eigenvalue that lead to improved rates for several multi-layer network models studied in the literature.},
	eventtitle = {International Conference on Machine Learning},
	pages = {24850--24887},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Min, Hancheng and Vidal, Rene and Mallada, Enrique},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@article{southern_expressive_nodate,
	title = {On the Expressive Power of Ollivier-Ricci Curvature on Graphs},
	abstract = {Discrete curvature has recently been used in graph machine learning to improve performance, understand message-passing and assess structural differences between graphs. Despite these advancements, the theoretical properties of discrete curvature measures, such as their representational power and their relationship to graph features is yet to be fully explored. This paper studies Ollivier–Ricci curvature on graphs, providing both a discussion and empirical analysis of its expressivity, i.e. the ability to distinguish nonisomorphic graphs.},
	author = {Southern, Joshua and Wayland, Jeremy and Bronstein, Michael and Rieck, Bastian},
	langid = {english},
}

@misc{xiao_forward_2023,
	title = {On the Forward Invariance of Neural {ODEs}},
	url = {http://arxiv.org/abs/2210.04763},
	doi = {10.48550/arXiv.2210.04763},
	abstract = {We propose a new method to ensure neural ordinary differential equations ({ODEs}) satisfy output specifications by using invariance set propagation. Our approach uses a class of control barrier functions to transform output specifications into constraints on the parameters and inputs of the learning system. This setup allows us to achieve output specification guarantees simply by changing the constrained parameters/inputs both during training and inference. Moreover, we demonstrate that our invariance set propagation through data-controlled neural {ODEs} not only maintains generalization performance but also creates an additional degree of robustness by enabling causal manipulation of the system's parameters/inputs. We test our method on a series of representation learning tasks, including modeling physical dynamics and convexity portraits, as well as safe collision avoidance for autonomous vehicles.},
	number = {{arXiv}:2210.04763},
	publisher = {{arXiv}},
	author = {Xiao, Wei and Wang, Tsun-Hsuan and Hasani, Ramin and Lechner, Mathias and Ban, Yutong and Gan, Chuang and Rus, Daniela},
	urldate = {2023-08-02},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2210.04763 [cs, eess]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics, Electrical Engineering and Systems Science - Systems and Control},
}

@inproceedings{dbouk_robustness_2023,
	title = {On the Robustness of Randomized Ensembles to Adversarial Perturbations},
	url = {https://proceedings.mlr.press/v202/dbouk23a.html},
	abstract = {Randomized ensemble classifiers ({RECs}), where one classifier is randomly selected during inference, have emerged as an attractive alternative to traditional ensembling methods for realizing adversarially robust classifiers with limited compute requirements. However, recent works have shown that existing methods for constructing {RECs} are more vulnerable than initially claimed, casting major doubts on their efficacy and prompting fundamental questions such as: "When are {RECs} useful?", "What are their limits?", and "How do we train them?". In this work, we first demystify {RECs} as we derive fundamental results regarding their theoretical limits, necessary and sufficient conditions for them to be useful, and more. Leveraging this new understanding, we propose a new boosting algorithm ({BARRE}) for training robust {RECs}, and empirically demonstrate its effectiveness at defending against strong \${\textbackslash}ell\_{\textbackslash}infty\$ norm-bounded adversaries across various network architectures and datasets. Our code can be found at https://github.com/hsndbk4/{BARRE}.},
	eventtitle = {International Conference on Machine Learning},
	pages = {7303--7328},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Dbouk, Hassan and Shanbhag, Naresh},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{cha_orthogonality-enforced_2023,
	title = {Orthogonality-Enforced Latent Space in Autoencoders: An Approach to Learning Disentangled Representations},
	url = {https://proceedings.mlr.press/v202/cha23b.html},
	shorttitle = {Orthogonality-Enforced Latent Space in Autoencoders},
	abstract = {Noting the importance of factorizing (or disentangling) the latent space, we propose a novel, non-probabilistic disentangling framework for autoencoders, based on the principles of symmetry transformations that are independent of one another. To the best of our knowledge, this is the first deterministic model that is aiming to achieve disentanglement based on autoencoders using only a reconstruction loss without pairs of images or labels, by explicitly introducing inductive biases into a model architecture through Euler encoding. The proposed model is then compared with a number of state-of-the-art models, relevant to disentanglement, including symmetry-based models and generative models. Our evaluation using six different disentanglement metrics, including the unsupervised disentanglement metric we propose here in this paper, shows that the proposed model can offer better disentanglement, especially when variances of the features are different, where other methods may struggle. We believe that this model opens several opportunities for linear disentangled representation learning based on deterministic autoencoders.},
	eventtitle = {International Conference on Machine Learning},
	pages = {3913--3948},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Cha, Jaehoon and Thiyagalingam, Jeyan},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{gao_out--domain_2023,
	title = {Out-of-Domain Robustness via Targeted Augmentations},
	url = {http://arxiv.org/abs/2302.11861},
	doi = {10.48550/arXiv.2302.11861},
	abstract = {Models trained on one set of domains often suffer performance drops on unseen domains, e.g., when wildlife monitoring models are deployed in new camera locations. In this work, we study principles for designing data augmentations for out-of-domain ({OOD}) generalization. In particular, we focus on real-world scenarios in which some domain-dependent features are robust, i.e., some features that vary across domains are predictive {OOD}. For example, in the wildlife monitoring application above, image backgrounds vary across camera locations but indicate habitat type, which helps predict the species of photographed animals. Motivated by theoretical analysis on a linear setting, we propose targeted augmentations, which selectively randomize spurious domain-dependent features while preserving robust ones. We prove that targeted augmentations improve {OOD} performance, allowing models to generalize better with fewer domains. In contrast, existing approaches such as generic augmentations, which fail to randomize domain-dependent features, and domain-invariant augmentations, which randomize all domain-dependent features, both perform poorly {OOD}. In experiments on three real-world datasets, we show that targeted augmentations set new states-of-the-art for {OOD} performance by 3.2-15.2\%.},
	number = {{arXiv}:2302.11861},
	publisher = {{arXiv}},
	author = {Gao, Irena and Sagawa, Shiori and Koh, Pang Wei and Hashimoto, Tatsunori and Liang, Percy},
	urldate = {2023-08-02},
	date = {2023-06-26},
	eprinttype = {arxiv},
	eprint = {2302.11861 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@inproceedings{dimitriadis_pareto_2023,
	title = {Pareto Manifold Learning: Tackling multiple tasks via ensembles of single-task models},
	url = {https://proceedings.mlr.press/v202/dimitriadis23a.html},
	shorttitle = {Pareto Manifold Learning},
	abstract = {In Multi-Task Learning ({MTL}), tasks may compete and limit the performance achieved on each other, rather than guiding the optimization to a solution, superior to all its single-task trained counterparts. Since there is often not a unique solution optimal for all tasks, practitioners have to balance tradeoffs between tasks’ performance, and resort to optimality in the Pareto sense. Most {MTL} methodologies either completely neglect this aspect, and instead of aiming at learning a Pareto Front, produce one solution predefined by their optimization schemes, or produce diverse but discrete solutions. Recent approaches parameterize the Pareto Front via neural networks, leading to complex mappings from tradeoff to objective space. In this paper, we conjecture that the Pareto Front admits a linear parameterization in parameter space, which leads us to propose Pareto Manifold Learning, an ensembling method in weight space. Our approach produces a continuous Pareto Front in a single training run, that allows to modulate the performance on each task during inference. Experiments on multi-task learning benchmarks, ranging from image classification to tabular datasets and scene understanding, show that Pareto Manifold Learning outperforms state-of-the-art single-point algorithms, while learning a better Pareto parameterization than multi-point baselines.},
	eventtitle = {International Conference on Machine Learning},
	pages = {8015--8052},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Dimitriadis, Nikolaos and Frossard, Pascal and Fleuret, François},
	urldate = {2023-08-01},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{lee_pix2struct_2023,
	title = {Pix2Struct: Screenshot Parsing as Pretraining for Visual Language Understanding},
	url = {https://proceedings.mlr.press/v202/lee23g.html},
	shorttitle = {Pix2Struct},
	abstract = {Visually-situated language is ubiquitous—sources range from textbooks with diagrams to web pages with images and tables, to mobile apps with buttons and forms. Perhaps due to this diversity, previous work has typically relied on domain-specific recipes with limited sharing of the underlying data, model architectures, and objectives. We present Pix2Struct, a pretrained image-to-text model for purely visual language understanding, which can be finetuned on tasks containing visually-situated language. Pix2Struct is pretrained by learning to parse masked screenshots of web pages into simplified {HTML}. The web, with its richness of visual elements cleanly reflected in the {HTML} structure, provides a large source of pretraining data well suited to the diversity of downstream tasks. Intuitively, this objective subsumes common pretraining signals such as {OCR}, language modeling, and image captioning. In addition to the novel pretraining strategy, we introduce a variable-resolution input representation and a more flexible integration of language and vision inputs, where language prompts such as questions are rendered directly on top of the input image. For the first time, we show that a single pretrained model can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.},
	eventtitle = {International Conference on Machine Learning},
	pages = {18893--18912},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Lee, Kenton and Joshi, Mandar and Turc, Iulia Raluca and Hu, Hexiang and Liu, Fangyu and Eisenschlos, Julian Martin and Khandelwal, Urvashi and Shaw, Peter and Chang, Ming-Wei and Toutanova, Kristina},
	urldate = {2023-08-01},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{berzins_polyhedral_2023,
	title = {Polyhedral Complex Extraction from {ReLU} Networks using Edge Subdivision},
	url = {http://arxiv.org/abs/2306.07212},
	doi = {10.48550/arXiv.2306.07212},
	abstract = {A neural network consisting of piecewise affine building blocks, such as fully-connected layers and {ReLU} activations, is itself a piecewise affine function supported on a polyhedral complex. This complex has been previously studied to characterize theoretical properties of neural networks, but, in practice, extracting it remains a challenge due to its high combinatorial complexity. A natural idea described in previous works is to subdivide the regions via intersections with hyperplanes induced by each neuron. However, we argue that this view leads to computational redundancy. Instead of regions, we propose to subdivide edges, leading to a novel method for polyhedral complex extraction. A key to this are sign-vectors, which encode the combinatorial structure of the complex. Our approach allows to use standard tensor operations on a {GPU}, taking seconds for millions of cells on a consumer grade machine. Motivated by the growing interest in neural shape representation, we use the speed and differentiability of our method to optimize geometric properties of the complex. The code is available at https://github.com/arturs-berzins/relu\_edge\_subdivision .},
	number = {{arXiv}:2306.07212},
	publisher = {{arXiv}},
	author = {Berzins, Arturs},
	urldate = {2023-09-13},
	date = {2023-06-12},
	eprinttype = {arxiv},
	eprint = {2306.07212 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{lim_positional_nodate,
	title = {Positional Encodings as Group Representations: A Unified Framework},
	abstract = {Positional encodings are ubiquitous as an input featurization tool in language modeling, computer vision, and graph representation learning, enabling neural networks to capture important geometric structure of the input. Traditionally, positional encodings have been defined anew for each data domain. In this work, we reinterpret positional encodings for disparate data types —including sequences, grids, graphs, and manifolds — in the unifying framework of group representations. We show how to express existing positional encodings as group representations, and conversely, propose new positional encodings by choosing suitable groups and representations. We validate our framework with experiments on implicit neural representations of images and vector fields, highlighting the practical utility of such positional encodings for encouraging approximate equivariance and capturing geometric structure.},
	author = {Lim, Derek and Lawrence, Hannah},
	langid = {english},
}

@article{sun_pot_2023,
	title = {{PoT}: Securely Proving Legitimacy of Training Data and Logic for {AI} Regulation},
	abstract = {The widespread use of generative models has raised concerns about the legitimacy of training data and algorithms in the training phase. In response to the copyright and privacy legislation, we propose Proof of Training ({PoT}), a provably secure protocol that allows model developers to prove to the public that they have used legitimate data and algorithms in the training phase, while also preserving the model’s privacy such as its weights and training dataset. Unlike the previous works on verifiable (un)learning, {PoT} emphasizes the legitimacy of training data and provides a proof of (non-)membership to testify whether a specific data point is included/excluded from the training set. By combining cryptographic primitives like zk-{SNARK}, {PoT} enables the model owner to prove that the training dataset is free from poisoning attacks and that the model and data were called following the logic of training algorithm (e.g., no backdoor is implanted), without leaking sensitive information to the verifiers. {PoT} is applicable in the federated learning settings by new multi-party computation ({MPC}) protocols that accommodate its additional security requirements such as robustness to Byzantine attacks.},
	author = {Sun, Haochen and Zhang, Hongyang},
	date = {2023},
	langid = {english},
}

@misc{he_probabilistic_2022,
	title = {Probabilistic Categorical Adversarial Attack \& Adversarial Training},
	url = {http://arxiv.org/abs/2210.09364},
	doi = {10.48550/arXiv.2210.09364},
	abstract = {The existence of adversarial examples brings huge concern for people to apply Deep Neural Networks ({DNNs}) in safety-critical tasks. However, how to generate adversarial examples with categorical data is an important problem but lack of extensive exploration. Previously established methods leverage greedy search method, which can be very time-consuming to conduct successful attack. This also limits the development of adversarial training and potential defenses for categorical data. To tackle this problem, we propose Probabilistic Categorical Adversarial Attack ({PCAA}), which transfers the discrete optimization problem to a continuous problem that can be solved efficiently by Projected Gradient Descent. In our paper, we theoretically analyze its optimality and time complexity to demonstrate its significant advantage over current greedy based attacks. Moreover, based on our attack, we propose an efficient adversarial training framework. Through a comprehensive empirical study, we justify the effectiveness of our proposed attack and defense algorithms.},
	number = {{arXiv}:2210.09364},
	publisher = {{arXiv}},
	author = {He, Pengfei and Xu, Han and Ren, Jie and Wan, Yuxuan and Liu, Zitao and Tang, Jiliang},
	urldate = {2023-09-13},
	date = {2022-10-24},
	eprinttype = {arxiv},
	eprint = {2210.09364 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
}

@misc{kirchhof_probabilistic_2023,
	title = {Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs},
	url = {http://arxiv.org/abs/2302.02865},
	doi = {10.48550/arXiv.2302.02865},
	abstract = {Contrastively trained encoders have recently been proven to invert the data-generating process: they encode each input, e.g., an image, into the true latent vector that generated the image (Zimmermann et al., 2021). However, real-world observations often have inherent ambiguities. For instance, images may be blurred or only show a 2D view of a 3D object, so multiple latents could have generated them. This makes the true posterior for the latent vector probabilistic with heteroscedastic uncertainty. In this setup, we extend the common {InfoNCE} objective and encoders to predict latent distributions instead of points. We prove that these distributions recover the correct posteriors of the data-generating process, including its level of aleatoric uncertainty, up to a rotation of the latent space. In addition to providing calibrated uncertainty estimates, these posteriors allow the computation of credible intervals in image retrieval. They comprise images with the same latent as a given query, subject to its uncertainty. Code is available at https://github.com/mkirchhof/Probabilistic\_Contrastive\_Learning},
	number = {{arXiv}:2302.02865},
	publisher = {{arXiv}},
	author = {Kirchhof, Michael and Kasneci, Enkelejda and Oh, Seong Joon},
	urldate = {2023-08-02},
	date = {2023-05-17},
	eprinttype = {arxiv},
	eprint = {2302.02865 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{oh_provable_2023,
	title = {Provable Benefit of Mixup for Finding Optimal Decision Boundaries},
	url = {http://arxiv.org/abs/2306.00267},
	doi = {10.48550/arXiv.2306.00267},
	abstract = {We investigate how pair-wise data augmentation techniques like Mixup affect the sample complexity of finding optimal decision boundaries in a binary linear classification problem. For a family of data distributions with a separability constant \${\textbackslash}kappa\$, we analyze how well the optimal classifier in terms of training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal classifier). For vanilla training without augmentation, we uncover an interesting phenomenon named the curse of separability. As we increase \${\textbackslash}kappa\$ to make the data distribution more separable, the sample complexity of vanilla training increases exponentially in \${\textbackslash}kappa\$; perhaps surprisingly, the task of finding optimal decision boundaries becomes harder for more separable distributions. For Mixup training, we show that Mixup mitigates this problem by significantly reducing the sample complexity. To this end, we develop new concentration results applicable to \$n{\textasciicircum}2\$ pair-wise augmented data points constructed from \$n\$ independent data, by carefully dealing with dependencies between overlapping pairs. Lastly, we study other masking-based Mixup-style techniques and show that they can distort the training loss and make its minimizer converge to a suboptimal classifier in terms of test accuracy.},
	number = {{arXiv}:2306.00267},
	publisher = {{arXiv}},
	author = {Oh, Junsoo and Yun, Chulhee},
	urldate = {2023-08-02},
	date = {2023-06-05},
	eprinttype = {arxiv},
	eprint = {2306.00267 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@misc{tukan_provable_2023,
	title = {Provable Data Subset Selection For Efficient Neural Network Training},
	url = {http://arxiv.org/abs/2303.05151},
	doi = {10.48550/arXiv.2303.05151},
	abstract = {Radial basis function neural networks ({\textbackslash}emph\{{RBFNN}\}) are \{well-known\} for their capability to approximate any continuous function on a closed bounded set with arbitrary precision given enough hidden neurons. In this paper, we introduce the first algorithm to construct coresets for {\textbackslash}emph\{{RBFNNs}\}, i.e., small weighted subsets that approximate the loss of the input data on any radial basis function network and thus approximate any function defined by an {\textbackslash}emph\{{RBFNN}\} on the larger input data. In particular, we construct coresets for radial basis and Laplacian loss functions. We then use our coresets to obtain a provable data subset selection algorithm for training deep neural networks. Since our coresets approximate every function, they also approximate the gradient of each weight in a neural network, which is a particular function on the input. We then perform empirical evaluations on function approximation and dataset subset selection on popular network architectures and data sets, demonstrating the efficacy and accuracy of our coreset construction.},
	number = {{arXiv}:2303.05151},
	publisher = {{arXiv}},
	author = {Tukan, Murad and Zhou, Samson and Maalouf, Alaa and Rus, Daniela and Braverman, Vladimir and Feldman, Dan},
	urldate = {2023-09-13},
	date = {2023-03-09},
	eprinttype = {arxiv},
	eprint = {2303.05151 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{sarnthein_random_2023,
	title = {Random Teachers are Good Teachers},
	url = {http://arxiv.org/abs/2302.12091},
	doi = {10.48550/arXiv.2302.12091},
	abstract = {In this work, we investigate the implicit regularization induced by teacher-student learning dynamics in self-distillation. To isolate its effect, we describe a simple experiment where we consider teachers at random initialization instead of trained teachers. Surprisingly, when distilling a student into such a random teacher, we observe that the resulting model and its representations already possess very interesting characteristics; (1) we observe a strong improvement of the distilled student over its teacher in terms of probing accuracy. (2) The learned representations are data-dependent and transferable between different tasks but deteriorate strongly if trained on random inputs. (3) The student checkpoint contains sparse subnetworks, so-called lottery tickets, and lies on the border of linear basins in the supervised loss landscape. These observations have interesting consequences for several important areas in machine learning: (1) Self-distillation can work solely based on the implicit regularization present in the gradient dynamics without relying on any dark knowledge, (2) self-supervised learning can learn features even in the absence of data augmentation and (3) training dynamics during the early phase of supervised training do not necessarily require label information. Finally, we shed light on an intriguing local property of the loss landscape: the process of feature learning is strongly amplified if the student is initialized closely to the teacher. These results raise interesting questions about the nature of the landscape that have remained unexplored so far. Code is available at https://github.com/safelix/dinopl.},
	number = {{arXiv}:2302.12091},
	publisher = {{arXiv}},
	author = {Sarnthein, Felix and Bachmann, Gregor and Anagnostidis, Sotiris and Hofmann, Thomas},
	urldate = {2023-09-13},
	date = {2023-06-19},
	eprinttype = {arxiv},
	eprint = {2302.12091 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{takeno_randomized_2023,
	title = {Randomized Gaussian Process Upper Confidence Bound with Tighter Bayesian Regret Bounds},
	url = {http://arxiv.org/abs/2302.01511},
	doi = {10.48550/arXiv.2302.01511},
	abstract = {Gaussian process upper confidence bound ({GP}-{UCB}) is a theoretically promising approach for black-box optimization; however, the confidence parameter \${\textbackslash}beta\$ is considerably large in the theorem and chosen heuristically in practice. Then, randomized {GP}-{UCB} ({RGP}-{UCB}) uses a randomized confidence parameter, which follows the Gamma distribution, to mitigate the impact of manually specifying \${\textbackslash}beta\$. This study first generalizes the regret analysis of {RGP}-{UCB} to a wider class of distributions, including the Gamma distribution. Furthermore, we propose improved {RGP}-{UCB} ({IRGP}-{UCB}) based on a two-parameter exponential distribution, which achieves tighter Bayesian regret bounds. {IRGP}-{UCB} does not require an increase in the confidence parameter in terms of the number of iterations, which avoids over-exploration in the later iterations. Finally, we demonstrate the effectiveness of {IRGP}-{UCB} through extensive experiments.},
	number = {{arXiv}:2302.01511},
	publisher = {{arXiv}},
	author = {Takeno, Shion and Inatsu, Yu and Karasuyama, Masayuki},
	urldate = {2023-09-13},
	date = {2023-06-11},
	eprinttype = {arxiv},
	eprint = {2302.01511 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{lou_reflected_2023,
	title = {Reflected Diffusion Models},
	url = {http://arxiv.org/abs/2304.04740},
	doi = {10.48550/arXiv.2304.04740},
	abstract = {Score-based diffusion models learn to reverse a stochastic differential equation that maps data to noise. However, for complex tasks, numerical error can compound and result in highly unnatural samples. Previous work mitigates this drift with thresholding, which projects to the natural data domain (such as pixel space for images) after each diffusion step, but this leads to a mismatch between the training and generative processes. To incorporate data constraints in a principled manner, we present Reflected Diffusion Models, which instead reverse a reflected stochastic differential equation evolving on the support of the data. Our approach learns the perturbed score function through a generalized score matching loss and extends key components of standard diffusion models including diffusion guidance, likelihood-based training, and {ODE} sampling. We also bridge the theoretical gap with thresholding: such schemes are just discretizations of reflected {SDEs}. On standard image benchmarks, our method is competitive with or surpasses the state of the art without architectural modifications and, for classifier-free guidance, our approach enables fast exact sampling with {ODEs} and produces more faithful samples under high guidance weight.},
	number = {{arXiv}:2304.04740},
	publisher = {{arXiv}},
	author = {Lou, Aaron and Ermon, Stefano},
	urldate = {2023-08-02},
	date = {2023-06-08},
	eprinttype = {arxiv},
	eprint = {2304.04740 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{xiong_relevant_2023,
	title = {Relevant Walk Search for Explaining Graph Neural Networks},
	url = {https://proceedings.mlr.press/v202/xiong23b.html},
	abstract = {Graph Neural Networks ({GNNs}) have become important machine learning tools for graph analysis, and its explainability is crucial for safety, fairness, and robustness. Layer-wise relevance propagation for {GNNs} ({GNN}-{LRP}) evaluates the relevance of walks to reveal important information flows in the network, and provides higher-order explanations, which have been shown to be superior to the lower-order, i.e., node-/edge-level, explanations. However, identifying relevant walks by {GNN}-{LRP} requires exponential computational complexity with respect to the network depth, which we will remedy in this paper. Specifically, we propose polynomial-time algorithms for finding top-\$K\$ relevant walks, which drastically reduces the computation and thus increases the applicability of {GNN}-{LRP} to large-scale problems. Our proposed algorithms are based on the max-product algorithm—a common tool for finding the maximum likelihood configurations in probabilistic graphical models—and can find the most relevant walks exactly at the neuron level and approximately at the node level. Our experiments demonstrate the performance of our algorithms at scale and their utility across application domains, i.e., on epidemiology, molecular, and natural language benchmarks. We provide our codes under github.com/xiong-ping/rel\_walk\_gnnlrp.},
	eventtitle = {International Conference on Machine Learning},
	pages = {38301--38324},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Xiong, Ping and Schnake, Thomas and Gastegger, Michael and Montavon, Grégoire and Muller, Klaus Robert and Nakajima, Shinichi},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{marbut_reliable_2023,
	title = {Reliable Measures of Spread in High Dimensional Latent Spaces},
	url = {https://proceedings.mlr.press/v202/marbut23a.html},
	abstract = {Understanding geometric properties of the latent spaces of natural language processing models allows the manipulation of these properties for improved performance on downstream tasks. One such property is the amount of data spread in a model’s latent space, or how fully the available latent space is being used. We demonstrate that the commonly used measures of data spread, average cosine similarity and a partition function min/max ratio I(V), do not provide reliable metrics to compare the use of latent space across data distributions. We propose and examine six alternative measures of data spread, all of which improve over these current metrics when applied to seven synthetic data distributions. Of our proposed measures, we recommend one principal component-based measure and one entropy-based measure that provide reliable, relative measures of spread and can be used to compare models of different sizes and dimensionalities.},
	eventtitle = {International Conference on Machine Learning},
	pages = {23871--23885},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Marbut, Anna and Mckinney-Bock, Katy and Wheeler, Travis},
	urldate = {2023-08-01},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@article{drummond_replicability_nodate,
	title = {Replicability is not Reproducibility: Nor is it Good Science},
	abstract = {At various machine learning conferences, at various times, there have been discussions arising from the inability to replicate the experimental results published in a paper. There seems to be a wide spread view that we need to do something to address this problem, as it is essential to the advancement of our ﬁeld. The most compelling argument would seem to be that reproducibility of experimental results is the hallmark of science. Therefore, given that most of us regard machine learning as a scientiﬁc discipline, being able to replicate experiments is paramount. I want to challenge this view by separating the notion of reproducibility, a generally desirable property, from replicability, its poor cousin. I claim there are important diﬀerences between the two. Reproducibility requires changes; replicability avoids them. Although reproducibility is desirable, I contend that the impoverished version, replicability, is one not worth having.},
	author = {Drummond, Chris},
	langid = {english},
}

@misc{chandak_representations_2023,
	title = {Representations and Exploration for Deep Reinforcement Learning using Singular Value Decomposition},
	url = {http://arxiv.org/abs/2305.00654},
	doi = {10.48550/arXiv.2305.00654},
	abstract = {Representation learning and exploration are among the key challenges for any deep reinforcement learning agent. In this work, we provide a singular value decomposition based method that can be used to obtain representations that preserve the underlying transition structure in the domain. Perhaps interestingly, we show that these representations also capture the relative frequency of state visitations, thereby providing an estimate for pseudo-counts for free. To scale this decomposition method to large-scale domains, we provide an algorithm that never requires building the transition matrix, can make use of deep networks, and also permits mini-batch training. Further, we draw inspiration from predictive state representations and extend our decomposition method to partially observable environments. With experiments on multi-task settings with partially observable domains, we show that the proposed method can not only learn useful representation on {DM}-Lab-30 environments (that have inputs involving language instructions, pixel images, and rewards, among others) but it can also be effective at hard exploration tasks in {DM}-Hard-8 environments.},
	number = {{arXiv}:2305.00654},
	publisher = {{arXiv}},
	author = {Chandak, Yash and Thakoor, Shantanu and Guo, Zhaohan Daniel and Tang, Yunhao and Munos, Remi and Dabney, Will and Borsa, Diana L.},
	urldate = {2023-09-13},
	date = {2023-05-02},
	eprinttype = {arxiv},
	eprint = {2305.00654 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{nguyen_revisiting_2023,
	title = {Revisiting Over-smoothing and Over-squashing Using Ollivier-Ricci Curvature},
	url = {http://arxiv.org/abs/2211.15779},
	doi = {10.48550/arXiv.2211.15779},
	abstract = {Graph Neural Networks ({GNNs}) had been demonstrated to be inherently susceptible to the problems of over-smoothing and over-squashing. These issues prohibit the ability of {GNNs} to model complex graph interactions by limiting their effectiveness in taking into account distant information. Our study reveals the key connection between the local graph geometry and the occurrence of both of these issues, thereby providing a unified framework for studying them at a local scale using the Ollivier-Ricci curvature. Specifically, we demonstrate that over-smoothing is linked to positive graph curvature while over-squashing is linked to negative graph curvature. Based on our theory, we propose the Batch Ollivier-Ricci Flow, a novel rewiring algorithm capable of simultaneously addressing both over-smoothing and over-squashing.},
	number = {{arXiv}:2211.15779},
	publisher = {{arXiv}},
	author = {Nguyen, Khang and Nong, Hieu and Nguyen, Vinh and Ho, Nhat and Osher, Stanley and Nguyen, Tan},
	urldate = {2023-09-13},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2211.15779 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{cai_robust_2023,
	title = {Robust Weight Signatures: Gaining Robustness as Easy as Patching Weights?},
	url = {http://arxiv.org/abs/2302.12480},
	doi = {10.48550/arXiv.2302.12480},
	shorttitle = {Robust Weight Signatures},
	abstract = {Given a robust model trained to be resilient to one or multiple types of distribution shifts (e.g., natural image corruptions), how is that "robustness" encoded in the model weights, and how easily can it be disentangled and/or "zero-shot" transferred to some other models? This paper empirically suggests a surprisingly simple answer: linearly - by straightforward model weight arithmetic! We start by drawing several key observations: (1)assuming that we train the same model architecture on both a clean dataset and its corrupted version, resultant weights mostly differ in shallow layers; (2)the weight difference after projection, which we call "Robust Weight Signature" ({RWS}), appears to be discriminative and indicative of different corruption types; (3)for the same corruption type, the {RWSs} obtained by one model architecture are highly consistent and transferable across different datasets. We propose a minimalistic model robustness "patching" framework that carries a model trained on clean data together with its pre-extracted {RWSs}. In this way, injecting certain robustness to the model is reduced to directly adding the corresponding {RWS} to its weight. We verify our proposed framework to be remarkably (1)lightweight. since {RWSs} concentrate on the shallowest few layers and we further show they can be painlessly quantized, storing an {RWS} is up to 13 x more compact than storing the full weight copy; (2)in-situ adjustable. {RWSs} can be appended as needed and later taken off to restore the intact clean model. We further demonstrate one can linearly re-scale the {RWS} to control the patched robustness strength; (3)composable. Multiple {RWSs} can be added simultaneously to patch more comprehensive robustness at once; and (4)transferable. Even when the clean model backbone is continually adapted or updated, {RWSs} remain as effective patches due to their outstanding cross-dataset transferability.},
	number = {{arXiv}:2302.12480},
	publisher = {{arXiv}},
	author = {Cai, Ruisi and Zhang, Zhenyu and Wang, Zhangyang},
	urldate = {2023-09-13},
	date = {2023-02-24},
	eprinttype = {arxiv},
	eprint = {2302.12480 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{tahmasebi_sample_nodate,
	title = {Sample Complexity Bounds for Estimating the Wasserstein Distance under Invariances},
	abstract = {Group-invariant probability distributions appear in many data-generative models in machine learning, such as graphs, point clouds, and images. In practice, one often needs to estimate divergences between such distributions. In this work, we study how the inherent invariances with respect to any smooth action of a Lie group on a manifold improve sample complexity when estimating the Wasserstein distance. Our result indicates a twofold gain: (1) reducing the sample complexity by a multiplicative factor corresponding to the group size (for finite groups) or the normalized volume of the quotient space (for groups of positive dimension), (2) improving the exponent in the convergence rate (for groups of positive dimension). These results are completely new for groups of positive dimension and tighten recent bounds for finite group actions.},
	author = {Tahmasebi, Behrooz and Jegelka, Stefanie},
	langid = {english},
}

@misc{hayakawa_sampling-based_2023,
	title = {Sampling-based Nystr{\textbackslash}"om Approximation and Kernel Quadrature},
	url = {http://arxiv.org/abs/2301.09517},
	doi = {10.48550/arXiv.2301.09517},
	abstract = {We analyze the Nystr{\textbackslash}"om approximation of a positive definite kernel associated with a probability measure. We first prove an improved error bound for the conventional Nystr{\textbackslash}"om approximation with i.i.d. sampling and singular-value decomposition in the continuous regime; the proof techniques are borrowed from statistical learning theory. We further introduce a refined selection of subspaces in Nystr{\textbackslash}"om approximation with theoretical guarantees that is applicable to non-i.i.d. landmark points. Finally, we discuss their application to convex kernel quadrature and give novel theoretical guarantees as well as numerical observations.},
	number = {{arXiv}:2301.09517},
	publisher = {{arXiv}},
	author = {Hayakawa, Satoshi and Oberhauser, Harald and Lyons, Terry},
	urldate = {2023-09-13},
	date = {2023-05-22},
	eprinttype = {arxiv},
	eprint = {2301.09517 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Numerical Analysis, Statistics - Machine Learning},
}

@misc{doshi_self-repellent_2023,
	title = {Self-Repellent Random Walks on General Graphs -- Achieving Minimal Sampling Variance via Nonlinear Markov Chains},
	url = {http://arxiv.org/abs/2305.05097},
	doi = {10.48550/arXiv.2305.05097},
	abstract = {We consider random walks on discrete state spaces, such as general undirected graphs, where the random walkers are designed to approximate a target quantity over the network topology via sampling and neighborhood exploration in the form of Markov chain Monte Carlo ({MCMC}) procedures. Given any Markov chain corresponding to a target probability distribution, we design a self-repellent random walk ({SRRW}) which is less likely to transition to nodes that were highly visited in the past, and more likely to transition to seldom visited nodes. For a class of {SRRWs} parameterized by a positive real \{{\textbackslash}alpha\}, we prove that the empirical distribution of the process converges almost surely to the the target (stationary) distribution of the underlying Markov chain kernel. We then provide a central limit theorem and derive the exact form of the arising asymptotic co-variance matrix, which allows us to show that the {SRRW} with a stronger repellence (larger \{{\textbackslash}alpha\}) always achieves a smaller asymptotic covariance, in the sense of Loewner ordering of co-variance matrices. Especially for {SRRW}-driven {MCMC} algorithms, we show that the decrease in the asymptotic sampling variance is of the order O(1/\{{\textbackslash}alpha\}), eventually going down to zero. Finally, we provide numerical simulations complimentary to our theoretical results, also empirically testing a version of {SRRW} with \{{\textbackslash}alpha\} increasing in time to combine the benefits of smaller asymptotic variance due to large \{{\textbackslash}alpha\}, with empirically observed faster mixing properties of {SRRW} with smaller \{{\textbackslash}alpha\}.},
	number = {{arXiv}:2305.05097},
	publisher = {{arXiv}},
	author = {Doshi, Vishwaraj and Hu, Jie and Eun, Do Young},
	urldate = {2023-08-02},
	date = {2023-07-17},
	eprinttype = {arxiv},
	eprint = {2305.05097 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Probability},
}

@misc{raghu_sequential_2023,
	title = {Sequential Multi-Dimensional Self-Supervised Learning for Clinical Time Series},
	url = {http://arxiv.org/abs/2307.10923},
	doi = {10.48550/arXiv.2307.10923},
	abstract = {Self-supervised learning ({SSL}) for clinical time series data has received significant attention in recent literature, since these data are highly rich and provide important information about a patient's physiological state. However, most existing {SSL} methods for clinical time series are limited in that they are designed for unimodal time series, such as a sequence of structured features (e.g., lab values and vitals signs) or an individual high-dimensional physiological signal (e.g., an electrocardiogram). These existing methods cannot be readily extended to model time series that exhibit multimodality, with structured features and high-dimensional data being recorded at each timestep in the sequence. In this work, we address this gap and propose a new {SSL} method -- Sequential Multi-Dimensional {SSL} -- where a {SSL} loss is applied both at the level of the entire sequence and at the level of the individual high-dimensional data points in the sequence in order to better capture information at both scales. Our strategy is agnostic to the specific form of loss function used at each level -- it can be contrastive, as in {SimCLR}, or non-contrastive, as in {VICReg}. We evaluate our method on two real-world clinical datasets, where the time series contains sequences of (1) high-frequency electrocardiograms and (2) structured data from lab values and vitals signs. Our experimental results indicate that pre-training with our method and then fine-tuning on downstream tasks improves performance over baselines on both datasets, and in several settings, can lead to improvements across different self-supervised loss functions.},
	number = {{arXiv}:2307.10923},
	publisher = {{arXiv}},
	author = {Raghu, Aniruddh and Chandak, Payal and Alam, Ridwan and Guttag, John and Stultz, Collin M.},
	urldate = {2023-09-13},
	date = {2023-07-20},
	eprinttype = {arxiv},
	eprint = {2307.10923 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{andriushchenko_sgd_2023,
	title = {{SGD} with Large Step Sizes Learns Sparse Features},
	url = {http://arxiv.org/abs/2210.05337},
	doi = {10.48550/arXiv.2210.05337},
	abstract = {We showcase important features of the dynamics of the Stochastic Gradient Descent ({SGD}) in the training of neural networks. We present empirical observations that commonly used large step sizes (i) lead the iterates to jump from one side of a valley to the other causing loss stabilization, and (ii) this stabilization induces a hidden stochastic dynamics orthogonal to the bouncing directions that biases it implicitly toward sparse predictors. Furthermore, we show empirically that the longer large step sizes keep {SGD} high in the loss landscape valleys, the better the implicit regularization can operate and find sparse representations. Notably, no explicit regularization is used so that the regularization effect comes solely from the {SGD} training dynamics influenced by the step size schedule. Therefore, these observations unveil how, through the step size schedules, both gradient and noise drive together the {SGD} dynamics through the loss landscape of neural networks. We justify these findings theoretically through the study of simple neural network models as well as qualitative arguments inspired from stochastic processes. Finally, this analysis allows us to shed a new light on some common practice and observed phenomena when training neural networks. The code of our experiments is available at https://github.com/tml-epfl/sgd-sparse-features.},
	number = {{arXiv}:2210.05337},
	publisher = {{arXiv}},
	author = {Andriushchenko, Maksym and Varre, Aditya and Pillaud-Vivien, Loucas and Flammarion, Nicolas},
	urldate = {2023-08-03},
	date = {2023-06-07},
	eprinttype = {arxiv},
	eprint = {2210.05337 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{woodruff_sharper_2023,
	title = {Sharper Bounds for \${\textbackslash}ell\_p\$ Sensitivity Sampling},
	url = {http://arxiv.org/abs/2306.00732},
	doi = {10.48550/arXiv.2306.00732},
	abstract = {In large scale machine learning, random sampling is a popular way to approximate datasets by a small representative subset of examples. In particular, sensitivity sampling is an intensely studied technique which provides provable guarantees on the quality of approximation, while reducing the number of examples to the product of the {VC} dimension \$d\$ and the total sensitivity \${\textbackslash}mathfrak S\$ in remarkably general settings. However, guarantees going beyond this general bound of \${\textbackslash}mathfrak S d\$ are known in perhaps only one setting, for \${\textbackslash}ell\_2\$ subspace embeddings, despite intense study of sensitivity sampling in prior work. In this work, we show the first bounds for sensitivity sampling for \${\textbackslash}ell\_p\$ subspace embeddings for \$p{\textbackslash}neq 2\$ that improve over the general \${\textbackslash}mathfrak S d\$ bound, achieving a bound of roughly \${\textbackslash}mathfrak S{\textasciicircum}\{2/p\}\$ for \$1{\textbackslash}leq p{\textless}2\$ and \${\textbackslash}mathfrak S{\textasciicircum}\{2-2/p\}\$ for \$2{\textless}p{\textless}{\textbackslash}infty\$. For \$1{\textbackslash}leq p{\textless}2\$, we show that this bound is tight, in the sense that there exist matrices for which \${\textbackslash}mathfrak S{\textasciicircum}\{2/p\}\$ samples is necessary. Furthermore, our techniques yield further new results in the study of sampling algorithms, showing that the root leverage score sampling algorithm achieves a bound of roughly \$d\$ for \$1{\textbackslash}leq p{\textless}2\$, and that a combination of leverage score and sensitivity sampling achieves an improved bound of roughly \$d{\textasciicircum}\{2/p\}{\textbackslash}mathfrak S{\textasciicircum}\{2-4/p\}\$ for \$2{\textless}p{\textless}{\textbackslash}infty\$. Our sensitivity sampling results yield the best known sample complexity for a wide class of structured matrices that have small \${\textbackslash}ell\_p\$ sensitivity.},
	number = {{arXiv}:2306.00732},
	publisher = {{arXiv}},
	author = {Woodruff, David P. and Yasuda, Taisuke},
	urldate = {2023-08-08},
	date = {2023-06-01},
	eprinttype = {arxiv},
	eprint = {2306.00732 [cs, stat]},
	keywords = {Computer Science - Data Structures and Algorithms, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{ohana_shedding_2023,
	title = {Shedding a {PAC}-Bayesian Light on Adaptive Sliced-Wasserstein Distances},
	url = {http://arxiv.org/abs/2206.03230},
	doi = {10.48550/arXiv.2206.03230},
	abstract = {The Sliced-Wasserstein distance ({SW}) is a computationally efficient and theoretically grounded alternative to the Wasserstein distance. Yet, the literature on its statistical properties -- or, more accurately, its generalization properties -- with respect to the distribution of slices, beyond the uniform measure, is scarce. To bring new contributions to this line of research, we leverage the {PAC}-Bayesian theory and a central observation that {SW} may be interpreted as an average risk, the quantity {PAC}-Bayesian bounds have been designed to characterize. We provide three types of results: i) {PAC}-Bayesian generalization bounds that hold on what we refer as adaptive Sliced-Wasserstein distances, i.e. {SW} defined with respect to arbitrary distributions of slices (among which data-dependent distributions), ii) a principled procedure to learn the distribution of slices that yields maximally discriminative {SW}, by optimizing our theoretical bounds, and iii) empirical illustrations of our theoretical findings.},
	number = {{arXiv}:2206.03230},
	publisher = {{arXiv}},
	author = {Ohana, Ruben and Nadjahi, Kimia and Rakotomamonjy, Alain and Ralaivola, Liva},
	urldate = {2023-08-02},
	date = {2023-05-31},
	eprinttype = {arxiv},
	eprint = {2206.03230 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{reid_simplex_2023,
	title = {Simplex Random Features},
	url = {https://proceedings.mlr.press/v202/reid23a.html},
	abstract = {We present Simplex Random Features ({SimRFs}), a new random feature ({RF}) mechanism for unbiased approximation of the softmax and Gaussian kernels by geometrical correlation of random projection vectors. We prove that {SimRFs} provide the smallest possible mean square error ({MSE}) on unbiased estimates of these kernels among the class of weight-independent geometrically-coupled positive random feature ({PRF}) mechanisms, substantially outperforming the previously most accurate Orthogonal Random Features ({ORFs}) at no observable extra cost. We present a more computationally expensive {SimRFs}+ variant, which we prove is asymptotically optimal in the broader family of weight-dependent geometrical coupling schemes (which permit correlations between random vector directions and norms). In extensive empirical studies, we show consistent gains provided by {SimRFs} in settings including pointwise kernel estimation, nonparametric classification and scalable Transformers.},
	eventtitle = {International Conference on Machine Learning},
	pages = {28864--28888},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Reid, Isaac and Choromanski, Krzysztof Marcin and Likhosherstov, Valerii and Weller, Adrian},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{bonet_sliced-wasserstein_2023,
	title = {Sliced-Wasserstein on Symmetric Positive Definite Matrices for M/{EEG} Signals},
	url = {http://arxiv.org/abs/2303.05798},
	doi = {10.48550/arXiv.2303.05798},
	abstract = {When dealing with electro or magnetoencephalography records, many supervised prediction tasks are solved by working with covariance matrices to summarize the signals. Learning with these matrices requires using Riemanian geometry to account for their structure. In this paper, we propose a new method to deal with distributions of covariance matrices and demonstrate its computational efficiency on M/{EEG} multivariate time series. More specifically, we define a Sliced-Wasserstein distance between measures of symmetric positive definite matrices that comes with strong theoretical guarantees. Then, we take advantage of its properties and kernel methods to apply this distance to brain-age prediction from {MEG} data and compare it to state-of-the-art algorithms based on Riemannian geometry. Finally, we show that it is an efficient surrogate to the Wasserstein distance in domain adaptation for Brain Computer Interface applications.},
	number = {{arXiv}:2303.05798},
	publisher = {{arXiv}},
	author = {Bonet, Clément and Malézieux, Benoît and Rakotomamonjy, Alain and Drumetz, Lucas and Moreau, Thomas and Kowalski, Matthieu and Courty, Nicolas},
	urldate = {2023-08-02},
	date = {2023-05-24},
	eprinttype = {arxiv},
	eprint = {2303.05798 [cs, eess, stat]},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
}

@online{noauthor_smart_nodate,
	title = {Smart Initial Basis Selection for Linear Programs {\textbar} {OpenReview}},
	url = {https://openreview.net/forum?id=Mha86sOok1},
	urldate = {2023-08-01},
}

@misc{mohtashami_special_2023,
	title = {Special Properties of Gradient Descent with Large Learning Rates},
	url = {http://arxiv.org/abs/2205.15142},
	doi = {10.48550/arXiv.2205.15142},
	abstract = {When training neural networks, it has been widely observed that a large step size is essential in stochastic gradient descent ({SGD}) for obtaining superior models. However, the effect of large step sizes on the success of {SGD} is not well understood theoretically. Several previous works have attributed this success to the stochastic noise present in {SGD}. However, we show through a novel set of experiments that the stochastic noise is not sufficient to explain good non-convex training, and that instead the effect of a large learning rate itself is essential for obtaining best performance.We demonstrate the same effects also in the noise-less case, i.e. for full-batch {GD}. We formally prove that {GD} with large step size -- on certain non-convex function classes -- follows a different trajectory than {GD} with a small step size, which can lead to convergence to a global minimum instead of a local one. Our settings provide a framework for future analysis which allows comparing algorithms based on behaviors that can not be observed in the traditional settings.},
	number = {{arXiv}:2205.15142},
	publisher = {{arXiv}},
	author = {Mohtashami, Amirkeivan and Jaggi, Martin and Stich, Sebastian},
	urldate = {2023-09-13},
	date = {2023-02-16},
	eprinttype = {arxiv},
	eprint = {2205.15142 [cs, math]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control},
}

@misc{nagler_statistical_2023,
	title = {Statistical Foundations of Prior-Data Fitted Networks},
	url = {http://arxiv.org/abs/2305.11097},
	doi = {10.48550/arXiv.2305.11097},
	abstract = {Prior-data fitted networks ({PFNs}) were recently proposed as a new paradigm for machine learning. Instead of training the network to an observed training set, a fixed model is pre-trained offline on small, simulated training sets from a variety of tasks. The pre-trained model is then used to infer class probabilities in-context on fresh training sets with arbitrary size and distribution. Empirically, {PFNs} achieve state-of-the-art performance on tasks with similar size to the ones used in pre-training. Surprisingly, their accuracy further improves when passed larger data sets during inference. This article establishes a theoretical foundation for {PFNs} and illuminates the statistical mechanisms governing their behavior. While {PFNs} are motivated by Bayesian ideas, a purely frequentistic interpretation of {PFNs} as pre-tuned, but untrained predictors explains their behavior. A predictor's variance vanishes if its sensitivity to individual training samples does and the bias vanishes only if it is appropriately localized around the test feature. The transformer architecture used in current {PFN} implementations ensures only the former. These findings shall prove useful for designing architectures with favorable empirical behavior.},
	number = {{arXiv}:2305.11097},
	publisher = {{arXiv}},
	author = {Nagler, Thomas},
	urldate = {2023-08-03},
	date = {2023-05-18},
	eprinttype = {arxiv},
	eprint = {2305.11097 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{simchowitz_statistical_2023,
	title = {Statistical Learning under Heterogenous Distribution Shift},
	url = {http://arxiv.org/abs/2302.13934},
	doi = {10.48550/arXiv.2302.13934},
	abstract = {This paper studies the prediction of a target \${\textbackslash}mathbf\{z\}\$ from a pair of random variables \$({\textbackslash}mathbf\{x\},{\textbackslash}mathbf\{y\})\$, where the ground-truth predictor is additive \${\textbackslash}mathbb\{E\}[{\textbackslash}mathbf\{z\} {\textbackslash}mid {\textbackslash}mathbf\{x\},{\textbackslash}mathbf\{y\}] = f\_{\textbackslash}star({\textbackslash}mathbf\{x\}) +g\_\{{\textbackslash}star\}({\textbackslash}mathbf\{y\})\$. We study the performance of empirical risk minimization ({ERM}) over functions \$f+g\$, \$f {\textbackslash}in {\textbackslash}mathcal\{F\}\$ and \$g {\textbackslash}in {\textbackslash}mathcal\{G\}\$, fit on a given training distribution, but evaluated on a test distribution which exhibits covariate shift. We show that, when the class \${\textbackslash}mathcal\{F\}\$ is "simpler" than \${\textbackslash}mathcal\{G\}\$ (measured, e.g., in terms of its metric entropy), our predictor is more resilient to {\textbackslash}emph\{heterogenous covariate shifts\} in which the shift in \${\textbackslash}mathbf\{x\}\$ is much greater than that in \${\textbackslash}mathbf\{y\}\$. These results rely on a novel H{\textbackslash}"older style inequality for the Dudley integral which may be of independent interest. Moreover, we corroborate our theoretical findings with experiments demonstrating improved resilience to shifts in "simpler" features across numerous domains.},
	number = {{arXiv}:2302.13934},
	publisher = {{arXiv}},
	author = {Simchowitz, Max and Ajay, Anurag and Agrawal, Pulkit and Krishnamurthy, Akshay},
	urldate = {2023-09-13},
	date = {2023-02-27},
	eprinttype = {arxiv},
	eprint = {2302.13934 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{immer_stochastic_2023,
	title = {Stochastic Marginal Likelihood Gradients using Neural Tangent Kernels},
	url = {http://arxiv.org/abs/2306.03968},
	doi = {10.48550/arXiv.2306.03968},
	abstract = {Selecting hyperparameters in deep learning greatly impacts its effectiveness but requires manual effort and expertise. Recent works show that Bayesian model selection with Laplace approximations can allow to optimize such hyperparameters just like standard neural network parameters using gradients and on the training data. However, estimating a single hyperparameter gradient requires a pass through the entire dataset, limiting the scalability of such algorithms. In this work, we overcome this issue by introducing lower bounds to the linearized Laplace approximation of the marginal likelihood. In contrast to previous estimators, these bounds are amenable to stochastic-gradient-based optimization and allow to trade off estimation accuracy against computational complexity. We derive them using the function-space form of the linearized Laplace, which can be estimated using the neural tangent kernel. Experimentally, we show that the estimators can significantly accelerate gradient-based hyperparameter optimization.},
	number = {{arXiv}:2306.03968},
	publisher = {{arXiv}},
	author = {Immer, Alexander and van der Ouderaa, Tycho F. A. and van der Wilk, Mark and Rätsch, Gunnar and Schölkopf, Bernhard},
	urldate = {2023-09-13},
	date = {2023-06-06},
	eprinttype = {arxiv},
	eprint = {2306.03968 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chen_subequivariant_2023,
	title = {Subequivariant Graph Reinforcement Learning in 3D Environments},
	url = {http://arxiv.org/abs/2305.18951},
	doi = {10.48550/arXiv.2305.18951},
	abstract = {Learning a shared policy that guides the locomotion of different agents is of core interest in Reinforcement Learning ({RL}), which leads to the study of morphology-agnostic {RL}. However, existing benchmarks are highly restrictive in the choice of starting point and target point, constraining the movement of the agents within 2D space. In this work, we propose a novel setup for morphology-agnostic {RL}, dubbed Subequivariant Graph {RL} in 3D environments (3D-{SGRL}). Specifically, we first introduce a new set of more practical yet challenging benchmarks in 3D space that allows the agent to have full Degree-of-Freedoms to explore in arbitrary directions starting from arbitrary configurations. Moreover, to optimize the policy over the enlarged state-action space, we propose to inject geometric symmetry, i.e., subequivariance, into the modeling of the policy and Q-function such that the policy can generalize to all directions, improving exploration efficiency. This goal is achieved by a novel {SubEquivariant} Transformer ({SET}) that permits expressive message exchange. Finally, we evaluate the proposed method on the proposed benchmarks, where our method consistently and significantly outperforms existing approaches on single-task, multi-task, and zero-shot generalization scenarios. Extensive ablations are also conducted to verify our design. Code and videos are available on our project page: https://alpc91.github.io/{SGRL}/.},
	number = {{arXiv}:2305.18951},
	publisher = {{arXiv}},
	author = {Chen, Runfa and Han, Jiaqi and Sun, Fuchun and Huang, Wenbing},
	urldate = {2023-08-02},
	date = {2023-05-30},
	eprinttype = {arxiv},
	eprint = {2305.18951 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Robotics},
}

@inproceedings{buschoff_acquisition_2023,
	title = {The Acquisition of Physical Knowledge in Generative Neural Networks},
	url = {https://proceedings.mlr.press/v202/schulze-buschoff23a.html},
	abstract = {As children grow older, they develop an intuitive understanding of the physical processes around them. Their physical understanding develops in stages, moving along developmental trajectories which have been mapped out extensively in previous empirical research. Here, we investigate how the learning trajectories of deep generative neural networks compare to children’s developmental trajectories using physical understanding as a testbed. We outline an approach that allows us to examine two distinct hypotheses of human development – stochastic optimization and complexity increase. We find that while our models are able to accurately predict a number of physical processes, their learning trajectories under both hypotheses do not follow the developmental trajectories of children.},
	eventtitle = {International Conference on Machine Learning},
	pages = {30321--30341},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Buschoff, Luca M. Schulze and Schulz, Eric and Binz, Marcel},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{tahmasebi_exact_2023,
	title = {The Exact Sample Complexity Gain from Invariances for Kernel Regression on Manifolds},
	url = {http://arxiv.org/abs/2303.14269},
	doi = {10.48550/arXiv.2303.14269},
	abstract = {In practice, encoding invariances into models helps sample complexity. In this work, we tighten and generalize theoretical results on how invariances improve sample complexity. In particular, we provide minimax optimal rates for kernel ridge regression on any manifold, with a target function that is invariant to an arbitrary group action on the manifold. Our results hold for (almost) any group action, even groups of positive dimension. For a finite group, the gain increases the "effective" number of samples by the group size. For groups of positive dimension, the gain is observed by a reduction in the manifold's dimension, in addition to a factor proportional to the volume of the quotient space. Our proof takes the viewpoint of differential geometry, in contrast to the more common strategy of using invariant polynomials. Hence, this new geometric viewpoint on learning with invariances may be of independent interest.},
	number = {{arXiv}:2303.14269},
	publisher = {{arXiv}},
	author = {Tahmasebi, Behrooz and Jegelka, Stefanie},
	urldate = {2023-09-13},
	date = {2023-03-24},
	eprinttype = {arxiv},
	eprint = {2303.14269 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{singh_hessian_2023,
	title = {The Hessian perspective into the Nature of Convolutional Neural Networks},
	url = {http://arxiv.org/abs/2305.09088},
	doi = {10.48550/arXiv.2305.09088},
	abstract = {While Convolutional Neural Networks ({CNNs}) have long been investigated and applied, as well as theorized, we aim to provide a slightly different perspective into their nature -- through the perspective of their Hessian maps. The reason is that the loss Hessian captures the pairwise interaction of parameters and therefore forms a natural ground to probe how the architectural aspects of {CNN} get manifested in its structure and properties. We develop a framework relying on Toeplitz representation of {CNNs}, and then utilize it to reveal the Hessian structure and, in particular, its rank. We prove tight upper bounds (with linear activations), which closely follow the empirical trend of the Hessian rank and hold in practice in more general settings. Overall, our work generalizes and establishes the key insight that, even in {CNNs}, the Hessian rank grows as the square root of the number of parameters.},
	number = {{arXiv}:2305.09088},
	publisher = {{arXiv}},
	author = {Singh, Sidak Pal and Hofmann, Thomas and Schölkopf, Bernhard},
	urldate = {2023-09-13},
	date = {2023-05-15},
	eprinttype = {arxiv},
	eprint = {2305.09088 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{hodgkinson_interpolating_2023,
	title = {The Interpolating Information Criterion for Overparameterized Models},
	url = {http://arxiv.org/abs/2307.07785},
	doi = {10.48550/arXiv.2307.07785},
	abstract = {The problem of model selection is considered for the setting of interpolating estimators, where the number of model parameters exceeds the size of the dataset. Classical information criteria typically consider the large-data limit, penalizing model size. However, these criteria are not appropriate in modern settings where overparameterized models tend to perform well. For any overparameterized model, we show that there exists a dual underparameterized model that possesses the same marginal likelihood, thus establishing a form of Bayesian duality. This enables more classical methods to be used in the overparameterized setting, revealing the Interpolating Information Criterion, a measure of model quality that naturally incorporates the choice of prior into the model selection. Our new information criterion accounts for prior misspecification, geometric and spectral properties of the model, and is numerically consistent with known empirical and theoretical behavior in this regime.},
	number = {{arXiv}:2307.07785},
	publisher = {{arXiv}},
	author = {Hodgkinson, Liam and van der Heide, Chris and Salomone, Robert and Roosta, Fred and Mahoney, Michael W.},
	urldate = {2023-08-02},
	date = {2023-07-15},
	eprinttype = {arxiv},
	eprint = {2307.07785 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{de_silva_value_2023,
	title = {The Value of Out-of-Distribution Data},
	url = {http://arxiv.org/abs/2208.10967},
	doi = {10.48550/arXiv.2208.10967},
	abstract = {We expect the generalization error to improve with more samples from a similar task, and to deteriorate with more samples from an out-of-distribution ({OOD}) task. In this work, we show a counter-intuitive phenomenon: the generalization error of a task can be a non-monotonic function of the number of {OOD} samples. As the number of {OOD} samples increases, the generalization error on the target task improves before deteriorating beyond a threshold. In other words, there is value in training on small amounts of {OOD} data. We use Fisher's Linear Discriminant on synthetic datasets and deep networks on computer vision benchmarks such as {MNIST}, {CIFAR}-10, {CINIC}-10, {PACS} and {DomainNet} to demonstrate and analyze this phenomenon. In the idealistic setting where we know which samples are {OOD}, we show that these non-monotonic trends can be exploited using an appropriately weighted objective of the target and {OOD} empirical risk. While its practical utility is limited, this does suggest that if we can detect {OOD} samples, then there may be ways to benefit from them. When we do not know which samples are {OOD}, we show how a number of go-to strategies such as data-augmentation, hyper-parameter optimization, and pre-training are not enough to ensure that the target generalization error does not deteriorate with the number of {OOD} samples in the dataset.},
	number = {{arXiv}:2208.10967},
	publisher = {{arXiv}},
	author = {De Silva, Ashwin and Ramesh, Rahul and Priebe, Carey E. and Chaudhari, Pratik and Vogelstein, Joshua T.},
	urldate = {2023-08-03},
	date = {2023-07-13},
	eprinttype = {arxiv},
	eprint = {2208.10967 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{chiu_tight_2023,
	title = {Tight Certification of Adversarially Trained Neural Networks via Nonconvex Low-Rank Semidefinite Relaxations},
	url = {http://arxiv.org/abs/2211.17244},
	doi = {10.48550/arXiv.2211.17244},
	abstract = {Adversarial training is well-known to produce high-quality neural network models that are empirically robust against adversarial perturbations. Nevertheless, once a model has been adversarially trained, one often desires a certification that the model is truly robust against all future attacks. Unfortunately, when faced with adversarially trained models, all existing approaches have significant trouble making certifications that are strong enough to be practically useful. Linear programming ({LP}) techniques in particular face a "convex relaxation barrier" that prevent them from making high-quality certifications, even after refinement with mixed-integer linear programming ({MILP}) and branch-and-bound ({BnB}) techniques. In this paper, we propose a nonconvex certification technique, based on a low-rank restriction of a semidefinite programming ({SDP}) relaxation. The nonconvex relaxation makes strong certifications comparable to much more expensive {SDP} methods, while optimizing over dramatically fewer variables comparable to much weaker {LP} methods. Despite nonconvexity, we show how off-the-shelf local optimization algorithms can be used to achieve and to certify global optimality in polynomial time. Our experiments find that the nonconvex relaxation almost completely closes the gap towards exact certification of adversarially trained models.},
	number = {{arXiv}:2211.17244},
	publisher = {{arXiv}},
	author = {Chiu, Hong-Ming and Zhang, Richard Y.},
	urldate = {2023-08-02},
	date = {2023-06-14},
	eprinttype = {arxiv},
	eprint = {2211.17244 [cs, math, stat]},
	keywords = {Computer Science - Machine Learning, Mathematics - Optimization and Control, Statistics - Machine Learning},
}

@inproceedings{hu_tighter_2023,
	title = {Tighter Analysis for {ProxSkip}},
	url = {https://proceedings.mlr.press/v202/hu23a.html},
	abstract = {In this paper, we provide a tighter analysis for {ProxSkip}, an algorithm that allows fewer proximal operator computations to solve composite optimization problems. We improve the existing decreasing speed of Lyapunov function from (��2)O(p2){\textbackslash}mathcal\{O\}(p{\textasciicircum}2) to (��)O(p){\textbackslash}mathcal\{O\}(p), when ��pp, the frequency of the proximal operators is small enough. Our theoretical analysis also reveals the drawbacks of using large step sizes for gradient descent in {ProxSkip} when the proximal operator part is the bottleneck. Our main motivation comes from the continuous limit in which the original analysis of {ProxSkip} fails to guarantee convergence when both the step size ��γ{\textbackslash}gamma and frequency ��pp tend to zero. We construct a counterexample to demonstrate why such counterintuitive behavior occurs for the original analysis and then propose a novel Lyapunov function variant to construct a tighter analysis, avoiding the problem of the old one. Such a new Lyapunov function can be directly extended to many other variants of {ProxSkip}. When applied to stochastic gradient setup, our analysis leads to an improved proximal operator complexity for {SProxSkip} from (1����2‾‾‾‾√log(1��))O(1εμ2log⁡(1ε)){\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{{\textbackslash}frac\{1\}\{{\textbackslash}varepsilon{\textbackslash}mu{\textasciicircum}2\}\}{\textbackslash}log({\textbackslash}frac\{1\}\{{\textbackslash}varepsilon\})) to (��‾‾√log(1��))O(κlog⁡(1ε)){\textbackslash}mathcal\{O\}({\textbackslash}sqrt\{{\textbackslash}kappa\}{\textbackslash}log({\textbackslash}frac\{1\}\{{\textbackslash}varepsilon\})).},
	eventtitle = {International Conference on Machine Learning},
	pages = {13469--13496},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Hu, Zhengmian and Huang, Heng},
	urldate = {2023-08-08},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{li_tips_2023,
	title = {{TIPS}: Topologically Important Path Sampling for Anytime Neural Networks},
	url = {http://arxiv.org/abs/2305.08021},
	doi = {10.48550/arXiv.2305.08021},
	shorttitle = {{TIPS}},
	abstract = {Anytime neural networks ({AnytimeNNs}) are a promising solution to adaptively adjust the model complexity at runtime under various hardware resource constraints. However, the manually-designed {AnytimeNNs} are biased by designers' prior experience and thus provide sub-optimal solutions. To address the limitations of existing hand-crafted approaches, we first model the training process of {AnytimeNNs} as a discrete-time Markov chain ({DTMC}) and use it to identify the paths that contribute the most to the training of {AnytimeNNs}. Based on this new {DTMC}-based analysis, we further propose {TIPS}, a framework to automatically design {AnytimeNNs} under various hardware constraints. Our experimental results show that {TIPS} can improve the convergence rate and test accuracy of {AnytimeNNs}. Compared to the existing {AnytimeNNs} approaches, {TIPS} improves the accuracy by 2\%-6.6\% on multiple datasets and achieves {SOTA} accuracy-{FLOPs} tradeoffs.},
	number = {{arXiv}:2305.08021},
	publisher = {{arXiv}},
	author = {Li, Guihong and Bhardwaj, Kartikeya and Yang, Yuedong and Marculescu, Radu},
	urldate = {2023-08-03},
	date = {2023-06-19},
	eprinttype = {arxiv},
	eprint = {2305.08021 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{von_rohrscheidt_topological_2023,
	title = {Topological Singularity Detection at Multiple Scales},
	url = {http://arxiv.org/abs/2210.00069},
	doi = {10.48550/arXiv.2210.00069},
	abstract = {The manifold hypothesis, which assumes that data lies on or close to an unknown manifold of low intrinsic dimension, is a staple of modern machine learning research. However, recent work has shown that real-world data exhibits distinct non-manifold structures, i.e. singularities, that can lead to erroneous findings. Detecting such singularities is therefore crucial as a precursor to interpolation and inference tasks. We address this issue by developing a topological framework that (i) quantifies the local intrinsic dimension, and (ii) yields a Euclidicity score for assessing the 'manifoldness' of a point along multiple scales. Our approach identifies singularities of complex spaces, while also capturing singular structures and local geometric complexity in image data.},
	number = {{arXiv}:2210.00069},
	publisher = {{arXiv}},
	author = {von Rohrscheidt, Julius and Rieck, Bastian},
	urldate = {2023-08-03},
	date = {2023-06-14},
	eprinttype = {arxiv},
	eprint = {2210.00069 [cs, math, stat]},
	keywords = {55N31 (Primary), 32S50 (Secondary), Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Algebraic Topology, Statistics - Machine Learning},
}

@misc{abedsoltan_toward_2023,
	title = {Toward Large Kernel Models},
	url = {http://arxiv.org/abs/2302.02605},
	doi = {10.48550/arXiv.2302.02605},
	abstract = {Recent studies indicate that kernel machines can often perform similarly or better than deep neural networks ({DNNs}) on small datasets. The interest in kernel machines has been additionally bolstered by the discovery of their equivalence to wide neural networks in certain regimes. However, a key feature of {DNNs} is their ability to scale the model size and training data size independently, whereas in traditional kernel machines model size is tied to data size. Because of this coupling, scaling kernel machines to large data has been computationally challenging. In this paper, we provide a way forward for constructing large-scale general kernel models, which are a generalization of kernel machines that decouples the model and data, allowing training on large datasets. Specifically, we introduce {EigenPro} 3.0, an algorithm based on projected dual preconditioned {SGD} and show scaling to model and data sizes which have not been possible with existing kernel methods.},
	number = {{arXiv}:2302.02605},
	publisher = {{arXiv}},
	author = {Abedsoltan, Amirhesam and Belkin, Mikhail and Pandit, Parthe},
	urldate = {2023-08-02},
	date = {2023-06-19},
	eprinttype = {arxiv},
	eprint = {2302.02605 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{zhang_towards_2023,
	title = {Towards a Persistence Diagram that is Robust to Noise and Varied Densities},
	url = {https://proceedings.mlr.press/v202/zhang23bb.html},
	abstract = {Recent works have identified that existing methods, which construct persistence diagrams in Topological Data Analysis ({TDA}), are not robust to noise and varied densities in a point cloud. We analyze the necessary properties of an approach that can address these two issues, and propose a new filter function for {TDA} based on a new data-dependent kernel which possesses these properties. Our empirical evaluation reveals that the proposed filter function provides a better means for t-{SNE} visualization and {SVM} classification than three existing methods of {TDA}.},
	eventtitle = {International Conference on Machine Learning},
	pages = {41952--41972},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Zhang, Hang and Zhang, Kaifeng and Ting, Kai Ming and Zhu, Ye},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{zhang_towards_2023-1,
	title = {Towards Coherent Image Inpainting Using Denoising Diffusion Implicit Models},
	url = {http://arxiv.org/abs/2304.03322},
	doi = {10.48550/arXiv.2304.03322},
	abstract = {Image inpainting refers to the task of generating a complete, natural image based on a partially revealed reference image. Recently, many research interests have been focused on addressing this problem using fixed diffusion models. These approaches typically directly replace the revealed region of the intermediate or final generated images with that of the reference image or its variants. However, since the unrevealed regions are not directly modified to match the context, it results in incoherence between revealed and unrevealed regions. To address the incoherence problem, a small number of methods introduce a rigorous Bayesian framework, but they tend to introduce mismatches between the generated and the reference images due to the approximation errors in computing the posterior distributions. In this paper, we propose {COPAINT}, which can coherently inpaint the whole image without introducing mismatches. {COPAINT} also uses the Bayesian framework to jointly modify both revealed and unrevealed regions, but approximates the posterior distribution in a way that allows the errors to gradually drop to zero throughout the denoising steps, thus strongly penalizing any mismatches with the reference image. Our experiments verify that {COPAINT} can outperform the existing diffusion-based methods under both objective and subjective metrics. The codes are available at https://github.com/{UCSB}-{NLP}-Chang/{CoPaint}/.},
	number = {{arXiv}:2304.03322},
	publisher = {{arXiv}},
	author = {Zhang, Guanhua and Ji, Jiabao and Zhang, Yang and Yu, Mo and Jaakkola, Tommi and Chang, Shiyu},
	urldate = {2023-09-13},
	date = {2023-04-06},
	eprinttype = {arxiv},
	eprint = {2304.03322 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning},
}

@misc{takeno_towards_2023,
	title = {Towards Practical Preferential Bayesian Optimization with Skew Gaussian Processes},
	url = {http://arxiv.org/abs/2302.01513},
	doi = {10.48550/arXiv.2302.01513},
	abstract = {We study preferential Bayesian optimization ({BO}) where reliable feedback is limited to pairwise comparison called duels. An important challenge in preferential {BO}, which uses the preferential Gaussian process ({GP}) model to represent flexible preference structure, is that the posterior distribution is a computationally intractable skew {GP}. The most widely used approach for preferential {BO} is Gaussian approximation, which ignores the skewness of the true posterior. Alternatively, Markov chain Monte Carlo ({MCMC}) based preferential {BO} is also proposed. In this work, we first verify the accuracy of Gaussian approximation, from which we reveal the critical problem that the predictive probability of duels can be inaccurate. This observation motivates us to improve the {MCMC}-based estimation for skew {GP}, for which we show the practical efficiency of Gibbs sampling and derive the low variance {MC} estimator. However, the computational time of {MCMC} can still be a bottleneck in practice. Towards building a more practical preferential {BO}, we develop a new method that achieves both high computational efficiency and low sample complexity, and then demonstrate its effectiveness through extensive numerical experiments.},
	number = {{arXiv}:2302.01513},
	publisher = {{arXiv}},
	author = {Takeno, Shion and Nomura, Masahiro and Karasuyama, Masayuki},
	urldate = {2023-08-02},
	date = {2023-06-11},
	eprinttype = {arxiv},
	eprint = {2302.01513 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@inproceedings{hu_understanding_2023,
	title = {Understanding the Impact of Adversarial Robustness on Accuracy Disparity},
	url = {https://proceedings.mlr.press/v202/hu23j.html},
	abstract = {While it has long been empirically observed that adversarial robustness may be at odds with standard accuracy and may have further disparate impacts on different classes, it remains an open question to what extent such observations hold and how the class imbalance plays a role within. In this paper, we attempt to understand this question of accuracy disparity by taking a closer look at linear classifiers under a Gaussian mixture model. We decompose the impact of adversarial robustness into two parts: an inherent effect that will degrade the standard accuracy on all classes due to the robustness constraint, and the other caused by the class imbalance ratio, which will increase the accuracy disparity compared to standard training. Furthermore, we also show that such effects extend beyond the Gaussian mixture model, by generalizing our data model to the general family of stable distributions. More specifically, we demonstrate that while the constraint of adversarial robustness consistently degrades the standard accuracy in the balanced class setting, the class imbalance ratio plays a fundamentally different role in accuracy disparity compared to the Gaussian case, due to the heavy tail of the stable distribution. We additionally perform experiments on both synthetic and real-world datasets to corroborate our theoretical findings. Our empirical results also suggest that the implications may extend to nonlinear models over real-world datasets. Our code is publicly available on {GitHub} at https://github.com/Accuracy-Disparity/{AT}-on-{AD}.},
	eventtitle = {International Conference on Machine Learning},
	pages = {13679--13709},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Hu, Yuzheng and Wu, Fan and Zhang, Hongyang and Zhao, Han},
	urldate = {2023-09-13},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@inproceedings{podina_universal_2023,
	title = {Universal Physics-Informed Neural Networks: Symbolic Differential Operator Discovery with Sparse Data},
	url = {https://proceedings.mlr.press/v202/podina23a.html},
	shorttitle = {Universal Physics-Informed Neural Networks},
	abstract = {In this work we perform symbolic discovery of differential operators in a situation where there is sparse experimental data. This small data regime in machine learning can be made tractable by providing our algorithms with prior information about the underlying dynamics. Physics Informed Neural Networks ({PINNs}) have been very successful in this regime (reconstructing entire {ODE} solutions using only a single point or entire {PDE} solutions with very few measurements of the initial condition). The Universal {PINN} approach ({UPINN}) adds a neural network that learns a representation of unknown hidden terms in the differential equation. The algorithm yields both a surrogate solution to the differential equation and a black-box representation of the hidden terms. These hidden term neural networks can then be converted into symbolic equations using symbolic regression techniques like {AI} Feynman. In order to achieve convergence of the neural networks, we provide our algorithms with (noisy) measurements of both the initial condition as well as (synthetic) experimental data obtained at later times. We demonstrate strong performance of {UPINNs} even when provided with very few measurements of noisy data in both the {ODE} and {PDE} regime.},
	eventtitle = {International Conference on Machine Learning},
	pages = {27948--27956},
	booktitle = {Proceedings of the 40th International Conference on Machine Learning},
	publisher = {{PMLR}},
	author = {Podina, Lena and Eastman, Brydon and Kohandel, Mohammad},
	urldate = {2023-08-02},
	date = {2023-07-03},
	langid = {english},
	note = {{ISSN}: 2640-3498},
}

@misc{cao_variational_2023,
	title = {Variational sparse inverse Cholesky approximation for latent Gaussian processes via double Kullback-Leibler minimization},
	url = {http://arxiv.org/abs/2301.13303},
	doi = {10.48550/arXiv.2301.13303},
	abstract = {To achieve scalable and accurate inference for latent Gaussian processes, we propose a variational approximation based on a family of Gaussian distributions whose covariance matrices have sparse inverse Cholesky ({SIC}) factors. We combine this variational approximation of the posterior with a similar and efficient {SIC}-restricted Kullback-Leibler-optimal approximation of the prior. We then focus on a particular {SIC} ordering and nearest-neighbor-based sparsity pattern resulting in highly accurate prior and posterior approximations. For this setting, our variational approximation can be computed via stochastic gradient descent in polylogarithmic time per iteration. We provide numerical comparisons showing that the proposed double-Kullback-Leibler-optimal Gaussian-process approximation ({DKLGP}) can sometimes be vastly more accurate for stationary kernels than alternative approaches such as inducing-point and mean-field approximations at similar computational complexity.},
	number = {{arXiv}:2301.13303},
	publisher = {{arXiv}},
	author = {Cao, Jian and Kang, Myeongjong and Jimenez, Felix and Sang, Huiyan and Schafer, Florian and Katzfuss, Matthias},
	urldate = {2023-08-02},
	date = {2023-05-26},
	eprinttype = {arxiv},
	eprint = {2301.13303 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Computation, Statistics - Machine Learning},
}

@article{behpour2023,
  author       = {Sima Behpour and
                  Thang Doan and
                  Xin Li and
                  Wenbin He and
                  Liang Gou and
                  Liu Ren},
  title        = {GradOrth: {A} Simple yet Efficient Out-of-Distribution Detection with
                  Orthogonal Projection of Gradients},
  journal      = {CoRR},
  volume       = {abs/2308.00310},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.00310},
  doi          = {10.48550/arXiv.2308.00310},
  eprinttype    = {arXiv},
  eprint       = {2308.00310},
  timestamp    = {Mon, 21 Aug 2023 17:38:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-00310.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang2021,
  author       = {Rui Huang and
                  Andrew Geng and
                  Yixuan Li},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {On the Importance of Gradients for Detecting Distributional Shifts
                  in the Wild},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {677--689},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/063e26c670d07bb7c4d30e6fc69fe056-Abstract.html},
  timestamp    = {Tue, 03 May 2022 16:20:46 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/HuangGL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{bell2023,
  author       = {Brian Bell and
                  Michael Geyer and
                  David Glickenstein and
                  Amanda S. Fernandez and
                  Juston Moore},
  title        = {An Exact Kernel Equivalence for Finite Classification Models},
  journal      = {CoRR},
  volume       = {abs/2308.00824},
  year         = {2023},
  url          = {https://doi.org/10.48550/arXiv.2308.00824},
  doi          = {10.48550/arXiv.2308.00824},
  eprinttype    = {arXiv},
  eprint       = {2308.00824},
  timestamp    = {Mon, 21 Aug 2023 17:38:10 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2308-00824.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{chen2021atom,
  author       = {Jiefeng Chen and
                  Yixuan Li and
                  Xi Wu and
                  Yingyu Liang and
                  Somesh Jha},
  editor       = {Nuria Oliver and
                  Fernando P{\'{e}}rez{-}Cruz and
                  Stefan Kramer and
                  Jesse Read and
                  Jos{\'{e}} Antonio Lozano},
  title        = {{ATOM:} Robustifying Out-of-Distribution Detection Using Outlier Mining},
  booktitle    = {Machine Learning and Knowledge Discovery in Databases. Research Track
                  - European Conference, {ECML} {PKDD} 2021, Bilbao, Spain, September
                  13-17, 2021, Proceedings, Part {III}},
  series       = {Lecture Notes in Computer Science},
  volume       = {12977},
  pages        = {430--445},
  publisher    = {Springer},
  year         = {2021},
  url          = {https://doi.org/10.1007/978-3-030-86523-8\_26},
  doi          = {10.1007/978-3-030-86523-8\_26},
  timestamp    = {Tue, 22 Aug 2023 12:42:57 +0200},
  biburl       = {https://dblp.org/rec/conf/pkdd/ChenLWLJ21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{desilva2023,
  author       = {Ashwin De Silva and
                  Rahul Ramesh and
                  Carey E. Priebe and
                  Pratik Chaudhari and
                  Joshua T. Vogelstein},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {The Value of Out-of-Distribution Data},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {7366--7389},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/de-silva23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/SilvaRPCV23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{sarnthein2023,
  author       = {Felix Sarnthein and
                  Gregor Bachmann and
                  Sotiris Anagnostidis and
                  Thomas Hofmann},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Random Teachers are Good Teachers},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {30022--30041},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/sarnthein23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/SarntheinBAH23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{djurisic2023extremely,
  author       = {Andrija Djurisic and
                  Nebojsa Bozanic and
                  Arjun Ashok and
                  Rosanne Liu},
  title        = {Extremely Simple Activation Shaping for Out-of-Distribution Detection},
  booktitle    = {The Eleventh International Conference on Learning Representations,
                  {ICLR} 2023, Kigali, Rwanda, May 1-5, 2023},
  publisher    = {OpenReview.net},
  year         = {2023},
  url          = {https://openreview.net/pdf?id=ndYXTEL6cZz},
  timestamp    = {Fri, 30 Jun 2023 14:55:53 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/DjurisicBAL23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{filos2020autonomous,
  author       = {Angelos Filos and
                  Panagiotis Tigas and
                  Rowan McAllister and
                  Nicholas Rhinehart and
                  Sergey Levine and
                  Yarin Gal},
  title        = {Can Autonomous Vehicles Identify, Recover From, and Adapt to Distribution
                  Shifts?},
  booktitle    = {Proceedings of the 37th International Conference on Machine Learning,
                  {ICML} 2020, 13-18 July 2020, Virtual Event},
  series       = {Proceedings of Machine Learning Research},
  volume       = {119},
  pages        = {3145--3153},
  publisher    = {{PMLR}},
  year         = {2020},
  url          = {http://proceedings.mlr.press/v119/filos20a.html},
  timestamp    = {Fri, 05 Feb 2021 11:08:07 +0100},
  biburl       = {https://dblp.org/rec/conf/icml/FilosTMRLG20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{pillai2013classification,
  author       = {Ignazio Pillai and
                  Giorgio Fumera and
                  Fabio Roli},
  title        = {Multi-label classification with a reject option},
  journal      = {Pattern Recognit.},
  volume       = {46},
  number       = {8},
  pages        = {2256--2266},
  year         = {2013},
  url          = {https://doi.org/10.1016/j.patcog.2013.01.035},
  doi          = {10.1016/j.patcog.2013.01.035},
  timestamp    = {Sat, 09 Apr 2022 12:23:18 +0200},
  biburl       = {https://dblp.org/rec/journals/pr/PillaiFR13a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{fumera2002,
  author       = {Giorgio Fumera and
                  Fabio Roli},
  editor       = {Seong{-}Whan Lee and
                  Alessandro Verri},
  title        = {Support Vector Machines with Embedded Reject Option},
  booktitle    = {Pattern Recognition with Support Vector Machines, First International
                  Workshop, {SVM} 2002, Niagara Falls, Canada, August 10, 2002, Proceedings},
  series       = {Lecture Notes in Computer Science},
  volume       = {2388},
  pages        = {68--82},
  publisher    = {Springer},
  year         = {2002},
  url          = {https://doi.org/10.1007/3-540-45665-1\_6},
  doi          = {10.1007/3-540-45665-1\_6},
  timestamp    = {Sat, 09 Apr 2022 12:47:57 +0200},
  biburl       = {https://dblp.org/rec/conf/svm/FumeraR02.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{he2015,
  author       = {Kaiming He and
                  Xiangyu Zhang and
                  Shaoqing Ren and
                  Jian Sun},
  title        = {Deep Residual Learning for Image Recognition},
  journal      = {CoRR},
  volume       = {abs/1512.03385},
  year         = {2015},
  url          = {http://arxiv.org/abs/1512.03385},
  eprinttype    = {arXiv},
  eprint       = {1512.03385},
  timestamp    = {Wed, 25 Jan 2023 11:01:16 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{biggio2014,
  author       = {Battista Biggio and
                  Igino Corona and
                  Blaine Nelson and
                  Benjamin I. P. Rubinstein and
                  Davide Maiorca and
                  Giorgio Fumera and
                  Giorgio Giacinto and
                  Fabio Roli},
  title        = {Security Evaluation of Support Vector Machines in Adversarial Environments},
  journal      = {CoRR},
  volume       = {abs/1401.7727},
  year         = {2014},
  url          = {http://arxiv.org/abs/1401.7727},
  eprinttype    = {arXiv},
  eprint       = {1401.7727},
  timestamp    = {Mon, 13 Aug 2018 16:48:03 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/BiggioCNRMFGR14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{hendrycks2019,
  author       = {Dan Hendrycks and
                  Thomas G. Dietterich},
  title        = {Benchmarking Neural Network Robustness to Common Corruptions and Perturbations},
  booktitle    = {7th International Conference on Learning Representations, {ICLR} 2019,
                  New Orleans, LA, USA, May 6-9, 2019},
  publisher    = {OpenReview.net},
  year         = {2019},
  url          = {https://openreview.net/forum?id=HJz6tiCqYm},
  timestamp    = {Thu, 25 Jul 2019 14:25:46 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HendrycksD19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{hendrycks2017,
  author       = {Dan Hendrycks and
                  Kevin Gimpel},
  title        = {A Baseline for Detecting Misclassified and Out-of-Distribution Examples
                  in Neural Networks},
  booktitle    = {5th International Conference on Learning Representations, {ICLR} 2017,
                  Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2017},
  url          = {https://openreview.net/forum?id=Hkg4TI9xl},
  timestamp    = {Thu, 25 Jul 2019 14:25:55 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/HendrycksG17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang2021scaling,
  author       = {Rui Huang and
                  Yixuan Li},
  title        = {{MOS:} Towards Scaling Out-of-Distribution Detection for Large Semantic
                  Space},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
                  2021, virtual, June 19-25, 2021},
  pages        = {8710--8719},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2021},
  url          = {https://openaccess.thecvf.com/content/CVPR2021/html/Huang\_MOS\_Towards\_Scaling\_Out-of-Distribution\_Detection\_for\_Large\_Semantic\_Space\_CVPR\_2021\_paper.html},
  doi          = {10.1109/CVPR46437.2021.00860},
  timestamp    = {Mon, 18 Jul 2022 16:47:41 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/HuangL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{huang2021gradients,
  author       = {Rui Huang and
                  Andrew Geng and
                  Yixuan Li},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {On the Importance of Gradients for Detecting Distributional Shifts
                  in the Wild},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {677--689},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/063e26c670d07bb7c4d30e6fc69fe056-Abstract.html},
  timestamp    = {Tue, 03 May 2022 16:20:46 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/HuangGL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{igoe2022,
  author       = {Conor Igoe and
                  Youngseog Chung and
                  Ian Char and
                  Jeff Schneider},
  title        = {How Useful are Gradients for {OOD} Detection Really?},
  journal      = {CoRR},
  volume       = {abs/2205.10439},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2205.10439},
  doi          = {10.48550/arXiv.2205.10439},
  eprinttype    = {arXiv},
  eprint       = {2205.10439},
  timestamp    = {Thu, 29 Dec 2022 13:48:26 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2205-10439.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lakshminarayanan2017,
  author       = {Balaji Lakshminarayanan and
                  Alexander Pritzel and
                  Charles Blundell},
  editor       = {Isabelle Guyon and
                  Ulrike von Luxburg and
                  Samy Bengio and
                  Hanna M. Wallach and
                  Rob Fergus and
                  S. V. N. Vishwanathan and
                  Roman Garnett},
  title        = {Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  booktitle    = {Advances in Neural Information Processing Systems 30: Annual Conference
                  on Neural Information Processing Systems 2017, December 4-9, 2017,
                  Long Beach, CA, {USA}},
  pages        = {6402--6413},
  year         = {2017},
  url          = {https://proceedings.neurips.cc/paper/2017/hash/9ef2ed4b7fd2c810847ffa5fa85bce38-Abstract.html},
  timestamp    = {Thu, 21 Jan 2021 15:15:21 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/Lakshminarayanan17.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lee2020,
  author       = {Jinsol Lee and
                  Ghassan AlRegib},
  title        = {Gradients as a Measure of Uncertainty in Neural Networks},
  booktitle    = {{IEEE} International Conference on Image Processing, {ICIP} 2020,
                  Abu Dhabi, United Arab Emirates, October 25-28, 2020},
  pages        = {2416--2420},
  publisher    = {{IEEE}},
  year         = {2020},
  url          = {https://doi.org/10.1109/ICIP40778.2020.9190679},
  doi          = {10.1109/ICIP40778.2020.9190679},
  timestamp    = {Fri, 09 Apr 2021 18:43:23 +0200},
  biburl       = {https://dblp.org/rec/conf/icip/LeeA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{lee2018,
  author       = {Kimin Lee and
                  Kibok Lee and
                  Honglak Lee and
                  Jinwoo Shin},
  title        = {A Simple Unified Framework for Detecting Out-of-Distribution Samples
                  and Adversarial Attacks},
  journal      = {CoRR},
  volume       = {abs/1807.03888},
  year         = {2018},
  url          = {http://arxiv.org/abs/1807.03888},
  eprinttype    = {arXiv},
  eprint       = {1807.03888},
  timestamp    = {Mon, 13 Aug 2018 16:46:52 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1807-03888.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{liang2018,
  author       = {Shiyu Liang and
                  Yixuan Li and
                  R. Srikant},
  title        = {Enhancing The Reliability of Out-of-distribution Image Detection in
                  Neural Networks},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=H1VGkIxRZ},
  timestamp    = {Fri, 18 Nov 2022 15:40:45 +0100},
  biburl       = {https://dblp.org/rec/conf/iclr/LiangLS18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lin2021,
  author       = {Ziqian Lin and
                  Sreya Dutta Roy and
                  Yixuan Li},
  title        = {{MOOD:} Multi-Level Out-of-Distribution Detection},
  booktitle    = {{IEEE} Conference on Computer Vision and Pattern Recognition, {CVPR}
                  2021, virtual, June 19-25, 2021},
  pages        = {15313--15323},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2021},
  url          = {https://openaccess.thecvf.com/content/CVPR2021/html/Lin\_MOOD\_Multi-Level\_Out-of-Distribution\_Detection\_CVPR\_2021\_paper.html},
  doi          = {10.1109/CVPR46437.2021.01506},
  timestamp    = {Mon, 18 Jul 2022 16:47:41 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/LinRL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{liu2020,
  author       = {Weitang Liu and
                  Xiaoyun Wang and
                  John D. Owens and
                  Yixuan Li},
  title        = {Energy-based Out-of-distribution Detection},
  journal      = {CoRR},
  volume       = {abs/2010.03759},
  year         = {2020},
  url          = {https://arxiv.org/abs/2010.03759},
  eprinttype    = {arXiv},
  eprint       = {2010.03759},
  timestamp    = {Tue, 13 Oct 2020 15:25:23 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2010-03759.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{mohseni2020,
  author       = {Sina Mohseni and
                  Mandar Pitale and
                  J. B. S. Yadawa and
                  Zhangyang Wang},
  title        = {Self-Supervised Learning for Generalizable Out-of-Distribution Detection},
  booktitle    = {The Thirty-Fourth {AAAI} Conference on Artificial Intelligence, {AAAI}
                  2020, The Thirty-Second Innovative Applications of Artificial Intelligence
                  Conference, {IAAI} 2020, The Tenth {AAAI} Symposium on Educational
                  Advances in Artificial Intelligence, {EAAI} 2020, New York, NY, USA,
                  February 7-12, 2020},
  pages        = {5216--5223},
  publisher    = {{AAAI} Press},
  year         = {2020},
  url          = {https://doi.org/10.1609/aaai.v34i04.5966},
  doi          = {10.1609/aaai.v34i04.5966},
  timestamp    = {Mon, 04 Sep 2023 16:50:26 +0200},
  biburl       = {https://dblp.org/rec/conf/aaai/MohseniPYW20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{snoek2019,
  author       = {Jasper Snoek and
                  Yaniv Ovadia and
                  Emily Fertig and
                  Balaji Lakshminarayanan and
                  Sebastian Nowozin and
                  D. Sculley and
                  Joshua V. Dillon and
                  Jie Ren and
                  Zachary Nado},
  editor       = {Hanna M. Wallach and
                  Hugo Larochelle and
                  Alina Beygelzimer and
                  Florence d'Alch{\'{e}}{-}Buc and
                  Emily B. Fox and
                  Roman Garnett},
  title        = {Can you trust your model's uncertainty? Evaluating predictive uncertainty
                  under dataset shift},
  booktitle    = {Advances in Neural Information Processing Systems 32: Annual Conference
                  on Neural Information Processing Systems 2019, NeurIPS 2019, December
                  8-14, 2019, Vancouver, BC, Canada},
  pages        = {13969--13980},
  year         = {2019},
  url          = {https://proceedings.neurips.cc/paper/2019/hash/8558cb408c1d76621371888657d2eb1d-Abstract.html},
  timestamp    = {Mon, 16 May 2022 15:41:51 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/SnoekOFLNSDRN19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sun2022,
  author       = {Yiyou Sun and
                  Yixuan Li},
  editor       = {Shai Avidan and
                  Gabriel J. Brostow and
                  Moustapha Ciss{\'{e}} and
                  Giovanni Maria Farinella and
                  Tal Hassner},
  title        = {{DICE:} Leveraging Sparsification for Out-of-Distribution Detection},
  booktitle    = {Computer Vision - {ECCV} 2022: 17th European Conference, Tel Aviv,
                  Israel, October 23-27, 2022, Proceedings, Part {XXIV}},
  series       = {Lecture Notes in Computer Science},
  volume       = {13684},
  pages        = {691--708},
  publisher    = {Springer},
  year         = {2022},
  url          = {https://doi.org/10.1007/978-3-031-20053-3\_40},
  doi          = {10.1007/978-3-031-20053-3\_40},
  timestamp    = {Sun, 25 Dec 2022 14:02:20 +0100},
  biburl       = {https://dblp.org/rec/conf/eccv/SunL22a.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{sun2021,
  author       = {Yiyou Sun and
                  Chuan Guo and
                  Yixuan Li},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {ReAct: Out-of-distribution Detection With Rectified Activations},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {144--157},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/01894d6f048493d2cacde3c579c315a3-Abstract.html},
  timestamp    = {Fri, 18 Nov 2022 15:40:45 +0100},
  biburl       = {https://dblp.org/rec/conf/nips/SunGL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{sun2022deep,
  author       = {Yiyou Sun and
                  Yifei Ming and
                  Xiaojin Zhu and
                  Yixuan Li},
  title        = {Out-of-distribution Detection with Deep Nearest Neighbors},
  journal      = {CoRR},
  volume       = {abs/2204.06507},
  year         = {2022},
  url          = {https://doi.org/10.48550/arXiv.2204.06507},
  doi          = {10.48550/arXiv.2204.06507},
  eprinttype    = {arXiv},
  eprint       = {2204.06507},
  timestamp    = {Fri, 18 Nov 2022 15:40:46 +0100},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2204-06507.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{swaminathan2020,
  author       = {Sridhar Swaminathan and
                  Deepak Garg and
                  Rajkumar Kannan and
                  Fr{\'{e}}d{\'{e}}ric Andr{\`{e}}s},
  title        = {Sparse low rank factorization for deep neural network compression},
  journal      = {Neurocomputing},
  volume       = {398},
  pages        = {185--196},
  year         = {2020},
  url          = {https://doi.org/10.1016/j.neucom.2020.02.035},
  doi          = {10.1016/j.neucom.2020.02.035},
  timestamp    = {Tue, 16 Aug 2022 23:06:48 +0200},
  biburl       = {https://dblp.org/rec/journals/ijon/SwaminathanGKA20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{wang2021,
  author       = {Haoran Wang and
                  Weitang Liu and
                  Alex Bocchieri and
                  Yixuan Li},
  editor       = {Marc'Aurelio Ranzato and
                  Alina Beygelzimer and
                  Yann N. Dauphin and
                  Percy Liang and
                  Jennifer Wortman Vaughan},
  title        = {Can multi-label classification networks know what they don't know?},
  booktitle    = {Advances in Neural Information Processing Systems 34: Annual Conference
                  on Neural Information Processing Systems 2021, NeurIPS 2021, December
                  6-14, 2021, virtual},
  pages        = {29074--29087},
  year         = {2021},
  url          = {https://proceedings.neurips.cc/paper/2021/hash/f3b7e5d3eb074cde5b76e26bc0fb5776-Abstract.html},
  timestamp    = {Tue, 03 May 2022 16:20:49 +0200},
  biburl       = {https://dblp.org/rec/conf/nips/WangLBL21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@misc{xu2023vra,
      title={VRA: Variational Rectified Activation for Out-of-distribution Detection}, 
      author={Mingyu Xu and Zheng Lian and Bin Liu and Jianhua Tao},
      year={2023},
      eprint={2302.11716},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{yang2020,
  author       = {Huanrui Yang and
                  Minxue Tang and
                  Wei Wen and
                  Feng Yan and
                  Daniel Hu and
                  Ang Li and
                  Hai Li and
                  Yiran Chen},
  title        = {Learning Low-rank Deep Neural Networks via Singular Vector Orthogonality
                  Regularization and Singular Value Sparsification},
  booktitle    = {2020 {IEEE/CVF} Conference on Computer Vision and Pattern Recognition,
                  {CVPR} Workshops 2020, Seattle, WA, USA, June 14-19, 2020},
  pages        = {2899--2908},
  publisher    = {Computer Vision Foundation / {IEEE}},
  year         = {2020},
  url          = {https://openaccess.thecvf.com/content\_CVPRW\_2020/html/w40/Yang\_Learning\_Low-Rank\_Deep\_Neural\_Networks\_via\_Singular\_Vector\_Orthogonality\_Regularization\_CVPRW\_2020\_paper.html},
  doi          = {10.1109/CVPRW50498.2020.00347},
  timestamp    = {Mon, 04 Jul 2022 14:19:33 +0200},
  biburl       = {https://dblp.org/rec/conf/cvpr/YangTW0HLLC20.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{yang2021,
  author       = {Jingkang Yang and
                  Kaiyang Zhou and
                  Yixuan Li and
                  Ziwei Liu},
  title        = {Generalized Out-of-Distribution Detection: {A} Survey},
  journal      = {CoRR},
  volume       = {abs/2110.11334},
  year         = {2021},
  url          = {https://arxiv.org/abs/2110.11334},
  eprinttype    = {arXiv},
  eprint       = {2110.11334},
  timestamp    = {Fri, 05 May 2023 15:54:56 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-2110-11334.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{zhang2021,
  author       = {Chiyuan Zhang and
                  Samy Bengio and
                  Moritz Hardt and
                  Benjamin Recht and
                  Oriol Vinyals},
  title        = {Understanding deep learning (still) requires rethinking generalization},
  journal      = {Commun. {ACM}},
  volume       = {64},
  number       = {3},
  pages        = {107--115},
  year         = {2021},
  url          = {https://doi.org/10.1145/3446776},
  doi          = {10.1145/3446776},
  timestamp    = {Tue, 01 Jun 2021 09:59:05 +0200},
  biburl       = {https://dblp.org/rec/journals/cacm/ZhangBHRV21.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{lee2023implicit,
  author       = {Sungyoon Lee and
                  Jinseong Park and
                  Jaewook Lee},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Implicit Jacobian regularization weighted with impurity of probability
                  output},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {19141--19184},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/lee23q.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/Lee0023.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{bombari2023,
  author       = {Simone Bombari and
                  Shayan Kiyani and
                  Marco Mondelli},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Beyond the Universal Law of Robustness: Sharper Laws for Random Features
                  and Neural Tangent Kernels},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {2738--2776},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/bombari23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/BombariKM23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{nagler2023,
  author       = {Thomas Nagler},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Statistical Foundations of Prior-Data Fitted Networks},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {25660--25676},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/nagler23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/Nagler23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{Simchowitz2023,
  author       = {Max Simchowitz and
                  Anurag Ajay and
                  Pulkit Agrawal and
                  Akshay Krishnamurthy},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Statistical Learning under Heterogenous Distribution Shift},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {31800--31851},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/simchowitz23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:09 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/SimchowitzAAK23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{kang2023,
  author       = {Juwon Kang and
                  Nayeong Kim and
                  Donghyeon Kwon and
                  Jungseul Ok and
                  Suha Kwak},
  editor       = {Andreas Krause and
                  Emma Brunskill and
                  Kyunghyun Cho and
                  Barbara Engelhardt and
                  Sivan Sabato and
                  Jonathan Scarlett},
  title        = {Leveraging Proxy of Training Data for Test-Time Adaptation},
  booktitle    = {International Conference on Machine Learning, {ICML} 2023, 23-29 July
                  2023, Honolulu, Hawaii, {USA}},
  series       = {Proceedings of Machine Learning Research},
  volume       = {202},
  pages        = {15737--15752},
  publisher    = {{PMLR}},
  year         = {2023},
  url          = {https://proceedings.mlr.press/v202/kang23a.html},
  timestamp    = {Mon, 28 Aug 2023 17:23:08 +0200},
  biburl       = {https://dblp.org/rec/conf/icml/KangKKOK23.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{giryes2014,
  author       = {Raja Giryes and
                  Yaniv Plan and
                  Roman Vershynin},
  title        = {On the Effective Measure of Dimension in the Analysis Cosparse Model},
  journal      = {CoRR},
  volume       = {abs/1410.0989},
  year         = {2014},
  url          = {http://arxiv.org/abs/1410.0989},
  eprinttype    = {arXiv},
  eprint       = {1410.0989},
  timestamp    = {Mon, 13 Aug 2018 16:47:39 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/GiryesPV14.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{gilmer2018adversarial,
  title={Adversarial spheres},
  author={Gilmer, Justin and Metz, Luke and Faghri, Fartash and Schoenholz, Samuel S and Raghu, Maithra and Wattenberg, Martin and Goodfellow, Ian},
  journal={arXiv preprint arXiv:1801.02774},
  year={2018}
}

@inproceedings{gilmer2018,
  author       = {Justin Gilmer and
                  Luke Metz and
                  Fartash Faghri and
                  Samuel S. Schoenholz and
                  Maithra Raghu and
                  Martin Wattenberg and
                  Ian J. Goodfellow},
  title        = {Adversarial Spheres},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Workshop Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=SkthlLkPf},
  timestamp    = {Thu, 04 Apr 2019 13:20:09 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/GilmerMFSRWG18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{glielmo2022,
  author       = {Aldo Glielmo and
                  Iuri Macocco and
                  Diego Doimo and
                  Matteo Carli and
                  Claudio Zeni and
                  Romina Wild and
                  Maria d'Errico and
                  Alex Rodriguez and
                  Alessandro Laio},
  title        = {DADApy: Distance-based analysis of data-manifolds in Python},
  journal      = {Patterns},
  volume       = {3},
  number       = {10},
  pages        = {100589},
  year         = {2022},
  url          = {https://doi.org/10.1016/j.patter.2022.100589},
  doi          = {10.1016/j.patter.2022.100589},
  timestamp    = {Sun, 13 Nov 2022 17:53:24 +0100},
  biburl       = {https://dblp.org/rec/journals/patterns/GlielmoMDCZWdRL22.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{facco2018,
  author       = {Elena Facco and
                  Maria d'Errico and
                  Alex Rodriguez and
                  Alessandro Laio},
  title        = {Estimating the intrinsic dimension of datasets by a minimal neighborhood
                  information},
  journal      = {CoRR},
  volume       = {abs/1803.06992},
  year         = {2018},
  url          = {http://arxiv.org/abs/1803.06992},
  eprinttype    = {arXiv},
  eprint       = {1803.06992},
  timestamp    = {Mon, 13 Aug 2018 16:46:38 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1803-06992.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@article{Hinton06,
  author       = {Geoffrey E. Hinton and
                  Simon Osindero and
                  Yee Whye Teh},
  title        = {A Fast Learning Algorithm for Deep Belief Nets},
  journal      = {Neural Comput.},
  volume       = {18},
  number       = {7},
  pages        = {1527--1554},
  year         = {2006},
  url          = {https://doi.org/10.1162/neco.2006.18.7.1527},
  doi          = {10.1162/neco.2006.18.7.1527},
  timestamp    = {Tue, 01 Sep 2020 13:11:28 +0200},
  biburl       = {https://dblp.org/rec/journals/neco/HintonOT06.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}

@book{goodfellow2016deep,
  author       = {Ian J. Goodfellow and
                  Yoshua Bengio and
                  Aaron C. Courville},
  title        = {Deep Learning},
  series       = {Adaptive computation and machine learning},
  publisher    = {{MIT} Press},
  year         = {2016},
  url          = {http://www.deeplearningbook.org/},
  isbn         = {978-0-262-03561-3},
  timestamp    = {Sat, 25 Mar 2017 20:16:59 +0100},
  biburl       = {https://dblp.org/rec/books/daglib/0040158.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@article{hutchins2011intelligence,
  title={Intelligence-driven computer network defense informed by analysis of adversary campaigns and intrusion kill chains},
  author={Hutchins, Eric M and Cloppert, Michael J and Amin, Rohan M and others},
  journal={Leading Issues in Information Warfare \& Security Research},
  volume={1},
  number={1},
  pages={80},
  year={2011}
}
@article{attali1997approximations,
  title={Approximations of functions by a multilayer perceptron: a new approach},
  author={Attali, Jean-Gabriel and Pag{\`e}s, Gilles},
  journal={Neural networks},
  volume={10},
  number={6},
  pages={1069--1081},
  year={1997},
  publisher={Elsevier}
}
