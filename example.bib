@article{vonneuman-pocs,
 ISSN = {0003486X},
 URL = {http://www.jstor.org/stable/1968653},
 author = {P. Jordan and J. V. Neumann},
 journal = {Annals of Mathematics},
 number = {3},
 pages = {719--723},
 publisher = {Annals of Mathematics},
 title = {On Inner Products in Linear, Metric Spaces},
 urldate = {2023-07-21},
 volume = {36},
 year = {1935}
}

@article{chen2021equivalence,
	author = {Chen, Yilan and Huang, Wei and Nguyen, Lam and Weng, Tsui-Wei},
	year = 2021,
	title = {On the equivalence between neural network and support vector machine},
	journal = {Advances in Neural Information Processing Systems},
	volume = 34,
	pages = {23478--23490}
}
@article{burgess1996estimating,
	author = {Burgess, A},
	year = 1996,
	title = {Estimating equivalent kernels For neural networks: A data perturbation approach},
	journal = {Advances in Neural Information Processing Systems},
	volume = 9
}
@inproceedings{lin2020gradient,
	author = {Lin, Tianyi and Jin, Chi and Jordan, Michael},
	year = 2020,
	title = {On gradient descent ascent for nonconvex-concave minimax problems},
	booktitle = {International Conference on Machine Learning},
	pages = {6083--6093},
	organization = {PMLR}
}
@misc{neuralode2018,
	author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
	year = 2018,
	title = {Neural Ordinary Differential Equations},
	publisher = {arXiv},
	doi = {10.48550/ARXIV.1806.07366},
	url = {https://arxiv.org/abs/1806.07366},
	copyright = {arXiv.org perpetual, non-exclusive license},
	keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{bilovs2021neural,
	author = {Bilo{\v{s}}, Marin and Sommer, Johanna and Rangapuram, Syama Sundar and Januschowski, Tim and G{\"u}nnemann, Stephan},
	year = 2021,
	title = {Neural flows: Efficient alternative to neural ODEs},
	journal = {Advances in Neural Information Processing Systems},
	volume = 34,
	pages = {21325--21337}
}
@inproceedings{NIPS2005_663772ea,
	author = {Bengio, Yoshua and Delalleau, Olivier and Roux, Nicolas},
	year = 2005,
	title = {The Curse of Highly Variable Functions for Local Kernel Machines},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {MIT Press},
	volume = 18,
	url = {https://proceedings.neurips.cc/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf},
	editor = {Y. Weiss and B. Sch\"{o}lkopf and J. Platt},
	bdsk-url-1 = {https://proceedings.neurips.cc/paper/2005/file/663772ea088360f95bac3dc7ffb841be-Paper.pdf}
}
@article{nose1990constant,
	author = {Nose, Shuichi},
	year = 1990,
	title = {Constant-temperature molecular dynamics},
	journal = {Journal of Physics: Condensed Matter},
	publisher = {IOP Publishing},
	volume = 2,
	number = {S},
	pages = {SA115}
}
@article{scherer2020kernel,
	author = {Scherer, Christoph and Scheid, Ren{\'e} and Andrienko, Denis and Bereau, Tristan},
	year = 2020,
	title = {Kernel-based machine learning for efficient simulations of molecular liquids},
	journal = {Journal of chemical theory and computation},
	publisher = {ACS Publications},
	volume = 16,
	number = 5,
	pages = {3194--3204}
}
@article{szegedy2013intriguing,
	author = {Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
	year = 2013,
	title = {Intriguing properties of neural networks},
	journal = {arXiv preprint arXiv:1312.6199}
}
@article{ilyas2019adversarial,
	author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
	year = 2019,
	title = {Adversarial examples are not bugs, they are features},
	journal = {Advances in neural information processing systems},
	booktitle = {Advances in Neural Information Processing Systems 32 (NeurIPS 2019) Vancouver, BC, Canada},
	volume = 32,
	pages = {125--136},
	editor = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett}
}
@article{yousefzadeh2021deep,
	author = {Yousefzadeh, Roozbeh},
	year = 2021,
	title = {Deep learning generalization and the convex hull of training sets},
	journal = {arXiv preprint arXiv:2101.09849}
}
@article{gillette2022data,
	author = {Gillette, Andrew and Kur, Eugene},
	year = 2022,
	title = {Data-driven geometric scale detection via Delaunay interpolation},
	journal = {arXiv preprint arXiv:2203.05685}
}
@book{hardle2004nonparametric,
	author = {H{\"a}rdle, Wolfgang and M{\"u}ller, Marlene and Sperlich, Stefan and Werwatz, Axel and others},
	year = 2004,
	title = {Nonparametric and semiparametric models},
	publisher = {Springer},
	volume = 1
}
@article{Selvaraju_2019,
	author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},
	year = 2019,
	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization},
	month = {oct},
	journal = {International Journal of Computer Vision},
	publisher = {Springer Science and Business Media {LLC}},
	volume = 128,
	number = 2,
	pages = {336--359},
	doi = {10.1007/s11263-019-01228-7},
	url = {https://doi.org/10.1007%2Fs11263-019-01228-7}
}
@misc{lundberg2017unified,
	author = {Lundberg, Scott and Lee, Su-In},
	year = 2017,
	title = {A Unified Approach to Interpreting Model Predictions},
	publisher = {arXiv},
	doi = {10.48550/ARXIV.1705.07874},
	url = {https://arxiv.org/abs/1705.07874},
	copyright = {arXiv.org perpetual, non-exclusive license},
	keywords = {Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{simonyan2013deep,
	author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
	year = 2013,
	title = {Deep inside convolutional networks: Visualising image classification models and saliency maps},
	journal = {arXiv preprint arXiv:1312.6034}
}
@article{domingos2020,
	author = {Pedro Domingos},
	year = 2020,
	title = {Every Model Learned by Gradient Descent Is Approximately a Kernel Machine},
	journal = {CoRR},
	volume = {abs/2012.00152},
	url = {https://arxiv.org/abs/2012.00152},
	eprinttype = {arXiv},
	eprint = {2012.00152},
	timestamp = {Fri, 04 Dec 2020 12:07:23 +0100},
	biburl = {https://dblp.org/rec/journals/corr/abs-2012-00152.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{domingos2020every,
	author = {Domingos, Pedro},
	year = 2020,
	title = {Every model learned by gradient descent is approximately a kernel machine},
	journal = {arXiv preprint arXiv:2012.00152}
}
@article{jacot2018neural,
	author = {Jacot, Arthur and Gabriel, Franck and Hongler, Cl{\'e}ment},
	year = 2018,
	title = {Neural tangent kernel: Convergence and generalization in neural networks},
	journal = {Advances in neural information processing systems},
	volume = 31
}
@article{smola2002support,
	author = {Smola, A},
	year = 2002,
	title = {Support vector machines, regularization, optimization, and beyond},
	journal = {Learning with Kernels}
}
@article{cortes2009learning,
	author = {Cortes, Corinna and Mohri, Mehryar and Rostamizadeh, Afshin},
	year = 2009,
	title = {Learning non-linear combinations of kernels},
	journal = {Advances in neural information processing systems},
	volume = 22
}
@misc{ghojogh2021,
	author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
	year = 2021,
	title = {Reproducing Kernel Hilbert Space, Mercer's Theorem, Eigenfunctions, Nyström Method, and Use of Kernels in Machine Learning: Tutorial and Survey},
	publisher = {arXiv},
	doi = {10.48550/ARXIV.2106.08443},
	url = {https://arxiv.org/abs/2106.08443},
	copyright = {arXiv.org perpetual, non-exclusive license},
	keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Functional Analysis (math.FA), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics}
}
@book{shawe2004kernel,
	author = {Shawe-Taylor, John and Cristianini, Nello and others},
	year = 2004,
	title = {Kernel methods for pattern analysis},
	publisher = {Cambridge university press}
}
@inproceedings{zhao2005extracting,
	author = {Zhao, Shubin and Grishman, Ralph},
	year = 2005,
	title = {Extracting relations with integrated information using kernel methods},
	booktitle = {Proceedings of the 43rd annual meeting of the association for computational linguistics (acl’05)},
	pages = {419--426}
}
@article{he2020bayesian,
	author = {He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
	year = 2020,
	title = {Bayesian deep ensembles via the neural tangent kernel},
	journal = {Advances in neural information processing systems},
	volume = 33,
	pages = {1010--1022}
}
@article{Poupon2004,
	author = {Anne Poupon},
	year = 2004,
	title = {Voronoi and Voronoi-related tessellations in studies of protein structure and interaction},
	journal = {Current Opinion in Structural Biology},
	volume = 14,
	number = 2,
	pages = {233--241},
	doi = {https://doi.org/10.1016/j.sbi.2004.03.010},
	issn = {0959-440X},
	url = {http://www.sciencedirect.com/science/article/pii/S0959440X04000442}
}
@article{Guo1997,
	author = {Guo, Baining and Menon, Jai and Willette, Brian},
	year = 1997,
	title = {Surface Reconstruction Using Alpha Shapes},
	journal = {Computer Graphics Forum},
	publisher = {Blackwell Publishers},
	volume = 16,
	number = 4,
	pages = {177--190},
	doi = {10.1111/1467-8659.00178},
	issn = {1467-8659},
	url = {http://dx.doi.org/10.1111/1467-8659.00178},
	keywords = {Surface topology, alpha shapes, manifolds, surface fitting}
}
@article{Maus1984,
	author = {Maus, Arne},
	year = 1984,
	title = {Delaunay triangulation and the convex hull ofn points in expected linear time},
	month = {Jun},
	day = {01},
	journal = {BIT Numerical Mathematics},
	volume = 24,
	number = 2,
	pages = {151--163},
	doi = {10.1007/BF01937482},
	issn = {1572-9125},
	url = {https://doi.org/10.1007/BF01937482},
	abstract = {An algorithm is presented which produces a Delaunay triangulation ofn points in the Euclidean plane in expected linear time. The expected execution time is achieved when the data are (not too far from) uniformly distributed. A modification of the algorithm discussed in the appendix treats most of the non-uniform distributions. The basis of this algorithm is a geographical partitioning of the plane into boxes by the well-known Radix-sort algorithm. This partitioning is also used as a basis for a linear time algorithm for finding the convex hull ofn points in the Euclidean plane.}
}
@article{Lee1980,
	author = {Lee, D. T. and Schachter, B. J.},
	year = 1980,
	title = {Two algorithms for constructing a Delaunay triangulation},
	month = {Jun},
	day = {01},
	journal = {International Journal of Computer {\&} Information Sciences},
	volume = 9,
	number = 3,
	pages = {219--242},
	doi = {10.1007/BF00977785},
	issn = {1573-7640},
	url = {https://doi.org/10.1007/BF00977785},
	abstract = {This paper provides a unified discussion of the Delaunay triangulation. Its geometric properties are reviewed and several applications are discussed. Two algorithms are presented for constructing the triangulation over a planar set ofN points. The first algorithm uses a divide-and-conquer approach. It runs inO(N logN) time, which is asymptotically optimal. The second algorithm is iterative and requiresO(N2) time in the worst case. However, its average case performance is comparable to that of the first algorithm.}
}
@article{Edelsbrunner1983,
	author = {H. Edelsbrunner and D. Kirkpatrick and R. Seidel},
	year = 1983,
	title = {On the shape of a set of points in the plane},
	month = {July},
	journal = {IEEE Transactions on Information Theory},
	volume = 29,
	number = 4,
	pages = {551--559},
	doi = {10.1109/TIT.1983.1056714},
	issn = {0018-9448},
	keywords = {Geometry;Image analysis, shape;Image shape analysis}
}
% dynamical clustering paper
@article{Drieme2017,
	author = {Anne Driemel and Francesco Silvestri},
	year = 2017,
	title = {Locality-sensitive hashing of curves},
	journal = {CoRR},
	volume = {abs/1703.04040},
	url = {http://arxiv.org/abs/1703.04040},
	archiveprefix = {arXiv},
	eprint = {1703.04040},
	timestamp = {Tue, 18 Jul 2017 10:49:06 +0200},
	biburl = {http://dblp.org/rec/bib/journals/corr/DriemelS17},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}
@inproceedings{Sankararaman2013,
	author = {Sankararaman, Swaminathan and Agarwal, Pankaj K. and M{\o}lhave, Thomas and Pan, Jiangwei and Boedihardjo, Arnold P.},
	year = 2013,
	title = {Model-driven Matching and Segmentation of Trajectories},
	booktitle = {Proceedings of the 21st ACM SIGSPATIAL International Conference on Advances in Geographic Information Systems},
	location = {Orlando, Florida},
	publisher = {ACM},
	address = {New York, NY, USA},
	series = {SIGSPATIAL'13},
	pages = {234--243},
	doi = {10.1145/2525314.2525360},
	isbn = {978-1-4503-2521-9},
	url = {http://doi.acm.org/10.1145/2525314.2525360},
	numpages = 10,
	acmid = 2525360,
	keywords = {GPS trajectories, trajectory matching, trajectory segmentation}
}
@article{Mirzargar2014,
	author = {M. Mirzargar and R. T. Whitaker and R. M. Kirby},
	year = 2014,
	title = {Curve Boxplot: Generalization of Boxplot for Ensembles of Curves},
	month = {Dec},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	volume = 20,
	number = 12,
	pages = {2654--2663},
	doi = {10.1109/TVCG.2014.2346455},
	issn = {1077-2626},
	keywords = {computational geometry;data visualisation;boundary values;boxplot generalization;computational scientists;curve boxplot;curve ensembles;data depth;descriptive statistics;nonparametric method;rendering strategies;simulation science;visualization community;visualization strategies;Computational modeling;Curve fitting;Data visualization;Robustness;Shape analysis;Statistical analysis;Uncertainty visualization;boxplots;data depth;ensemble visualization;functional data;nonparametric statistic;order statistics;parametric curves;0}
}
@article{Raj2017,
	author = {Mukund Raj and Mahsa Mirzargar and Robert Ricci and Kirby, {Robert M.} and Whitaker, {Ross T.}},
	year = 2017,
	title = {Path Boxplots: A Method for Characterizing Uncertainty in Path Ensembles on a Graph},
	month = 4,
	journal = {Journal of Computational and Graphical Statistics},
	publisher = {American Statistical Association},
	volume = 26,
	number = 2,
	pages = {243--252},
	doi = {10.1080/10618600.2016.1209115},
	issn = {1061-8600},
	abstract = {Graphs are powerful and versatile data structures that can be used to represent a wide range of different types of information. In this article, we introduce a method to analyze and then visualize an important class of data described over a graph—namely, ensembles of paths. Analysis of such path ensembles is useful in a variety of applications, in diverse fields such as transportation, computer networks, and molecular dynamics. The proposed method generalizes the concept of band depth to an ensemble of paths on a graph, which provides a center-outward ordering on the paths. This ordering is, in turn, used to construct a generalization of the conventional boxplot or whisker plot, called a path boxplot, which applies to paths on a graph. The utility of path boxplot is demonstrated for several examples of path ensembles including paths defined over computer networks and roads. Supplementary materials for this article are available online.},
	keywords = {Data depth, Descriptive statistics, Ensemble, Graph, Paths}
}
@inbook{Kharrat2008,
	author = {Kharrat, Ahmed and Popa, Iulian Sandu and Zeitouni, Karine and Faiz, Sami},
	year = 2008,
	title = {Clustering Algorithm for Network Constraint Trajectories},
	booktitle = {Headway in Spatial Data Handling: 13th International Symposium on Spatial Data Handling},
	publisher = {Springer Berlin Heidelberg},
	address = {Berlin, Heidelberg},
	pages = {631--647},
	doi = {10.1007/978-3-540-68566-1_36},
	isbn = {978-3-540-68566-1},
	url = {https://doi.org/10.1007/978-3-540-68566-1_36},
	nothing = {Ruas, Anne and Gold, Christopher},
	abstract = {Spatial data mining is an active topic in spatial databases. This paper proposes a new clustering method for moving object trajectories databases. It applies specifically to trajectories that only lie on a predefined network. The proposed algorithm (NETSCAN) is inspired from the well-known density based algorithms. However, it takes advantage of the network constraint to estimate the object density. Indeed, NETSCAN first computes dense paths in the network based on the moving object count, then, it clusters the sub-trajectories which are similar to the dense paths. The user can adjust the clustering result by setting a density threshold for the dense paths, and a similarity threshold within the clusters. This paper describes the proposed method. An implementation is reported, along with experimental results that show the effectiveness of our approach and the flexibility allowed by the user parameters.}
}
@article{Zheng2015,
	author = {Zheng, Yu},
	year = 2015,
	title = {Trajectory Data Mining: An Overview},
	month = may,
	journal = {ACM Trans. Intell. Syst. Technol.},
	publisher = {ACM},
	address = {New York, NY, USA},
	volume = 6,
	number = 3,
	pages = {29:1--29:41},
	doi = {10.1145/2743025},
	issn = {2157-6904},
	url = {http://doi.acm.org/10.1145/2743025},
	issue_date = {May 2015},
	articleno = 29,
	numpages = 41,
	acmid = 2743025,
	keywords = {Spatiotemporal data mining, trajectory classification, trajectory compression, trajectory data mining, trajectory indexing and retrieval, trajectory outlier detection, trajectory pattern mining, trajectory uncertainty, urban computing}
}
@article{Shah12017,
	author = {Shah, Sohil Atul and Koltun, Vladlen},
	year = 2017,
	title = {Robust continuous clustering},
	journal = {Proceedings of the National Academy of Sciences},
	volume = 114,
	number = 37,
	pages = {9814--9819},
	doi = {10.1073/pnas.1700770114},
	url = {http://www.pnas.org/content/114/37/9814.abstract},
	abstract = {Clustering is a fundamental procedure in the analysis of scientific data. It is used ubiquitously across the sciences. Despite decades of research, existing clustering algorithms have limited effectiveness in high dimensions and often require tuning parameters for different domains and datasets. We present a clustering algorithm that achieves high accuracy across multiple domains and scales efficiently to high dimensions and large datasets. The presented algorithm optimizes a smooth continuous objective, which is based on robust statistics and allows heavily mixed clusters to be untangled. The continuous nature of the objective also allows clustering to be integrated as a module in end-to-end feature learning pipelines. We demonstrate this by extending the algorithm to perform joint clustering and dimensionality reduction by efficiently optimizing a continuous global objective. The presented approach is evaluated on large datasets of faces, hand-written digits, objects, newswire articles, sensor readings from the Space Shuttle, and protein expression levels. Our method achieves high accuracy across all datasets, outperforming the best prior algorithm by a factor of 3 in average rank.},
	eprint = {http://www.pnas.org/content/114/37/9814.full.pdf}
}
@article{Jaromczyk1982,
	author = {J. W. Jaromczyk and G. T. Toussaint},
	year = 1992,
	title = {Relative neighborhood graphs and their relatives},
	month = {Sep},
	journal = {Proceedings of the IEEE},
	volume = 80,
	number = 9,
	pages = {1502--1517},
	doi = {10.1109/5.163414},
	issn = {0018-9219},
	keywords = {computational geometry;computer vision;pattern recognition;spatial data structures;visual databases;computational morphology;computer vision;databases;neighborhood graphs;pattern classification;spatial analysis;Application software;Bibliographies;Biology computing;Computational geometry;Computer applications;Computer science;Computer vision;Morphology;Pattern analysis;Shape}
}
@inproceedings{Chakrabarti2006,
	author = {Chakrabarti, Deepayan and Kumar, Ravi and Tomkins, Andrew},
	year = 2006,
	title = {Evolutionary clustering},
	booktitle = {Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining},
	pages = {554--560},
	organization = {ACM}
}
@inproceedings{Andrade2001,
	author = {Andrade, Diogo Vieira and de Figueiredo, Luiz Henrique},
	year = 2001,
	title = {Good approximations for the relative neighbourhood graph.},
	booktitle = {CCCG},
	pages = {25--28}
}
@book{greene2003,
	author = {Greene, W.H.},
	year = 2003,
	title = {Econometric Analysis},
	publisher = {Pearson Education},
	isbn = 9788177586848,
	url = {https://books.google.com/books?id=njAcXDlR5U8C}
}
@article{mcfadden1973,
	author = {McFadden, Daniel and others},
	year = 1973,
	title = {Conditional logit analysis of qualitative choice behavior},
	journal = {Frontiers in Econometrics},
	publisher = {Institute of Urban and Regional Development, University of California}
}
@phdthesis{novelli2015,
	author = {Novelli, Francesco},
	year = 2015,
	title = {Detection and Measurement of Sales Cannibalization in Information Technology Markets},
	school = {Technische Universit{\"a}t}
}
@article{draganska2004,
	author = {Draganska, Michaela and Jain, Dipak},
	year = 2004,
	title = {A likelihood approach to estimating market equilibrium models},
	journal = {Management Science},
	publisher = {INFORMS},
	volume = 50,
	number = 5,
	pages = {605--616}
}
@misc{shriver2015,
	author = {Shriver, Scott and Bollinger, Bryan},
	year = 2015,
	title = {A Structural Model of Channel Choice with Implications for Retail Entry}
}
@article{fisher2009,
	author = {Fisher, Marshall L and Vaidyanathan, Ramnath},
	year = 2009,
	title = {An algorithm and demand estimation procedure for retail assortment optimization},
	journal = {Philadelphia: The Wharton School}
}
@article{liu1990,
	author = {Liu, Regina Y and others},
	year = 1990,
	title = {On a notion of data depth based on random simplices},
	journal = {The Annals of Statistics},
	publisher = {Institute of Mathematical Statistics},
	volume = 18,
	number = 1,
	pages = {405--414}
}
@article{lopez2009,
	author = {L{\'o}pez-Pintado, Sara and Romo, Juan},
	year = 2009,
	title = {On the concept of depth for functional data},
	journal = {Journal of the American Statistical Association},
	publisher = {Taylor \& Francis},
	volume = 104,
	number = 486,
	pages = {718--734}
}
@article{rousseeuw1996,
	author = {Rousseeuw, Peter J and Ruts, Ida},
	year = 1996,
	title = {Algorithm AS 307: Bivariate location depth},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	publisher = {JSTOR},
	volume = 45,
	number = 4,
	pages = {516--526}
}
@inproceedings{cheng2001,
	author = {Cheng, Andrew Y and Ouyang, Ming},
	year = 2001,
	title = {On algorithms for simplicial depth.},
	booktitle = {CCCG},
	pages = {53--56}
}
@article{krishnan2006,
	author = {Krishnan, Suresh and Mustafa, Nabil H and Venkatasubramanian, Suresh},
	year = 2006,
	title = {Statistical data depth and the graphics hardware},
	journal = {DIMACS Series in Discrete Mathematics and Theoretical Computer Science},
	publisher = {AMERICAN MATHEMATICAL SOCIETY},
	volume = 72,
	pages = 223
}
@article{Zasenko2016,
	author = {Olga Zasenko and Tamon Stephen},
	year = 2016,
	title = {Algorithms for Colourful Simplicial Depth and Medians in the Plane},
	journal = {CoRR},
	volume = {abs/1608.07348},
	url = {http://arxiv.org/abs/1608.07348},
	archiveprefix = {arXiv},
	eprint = {1608.07348},
	timestamp = {Wed, 07 Jun 2017 14:42:05 +0200},
	biburl = {http://dblp.org/rec/bib/journals/corr/ZasenkoS16},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{Linderman2017,
	author = {George C. Linderman and Stefan Steinerberger},
	year = 2017,
	title = {Clustering with t-SNE, provably},
	journal = {CoRR},
	volume = {abs/1706.02582},
	url = {http://arxiv.org/abs/1706.02582},
	archiveprefix = {arXiv},
	eprint = {1706.02582},
	timestamp = {Mon, 03 Jul 2017 13:29:02 +0200},
	biburl = {http://dblp.org/rec/bib/journals/corr/LindermanS17},
	bibsource = {dblp computer science bibliography, http://dblp.org}
}
@article{wattenberg2016how,
	author = {Wattenberg, Martin and Viégas, Fernanda and Johnson, Ian},
	year = 2016,
	title = {How to Use t-SNE Effectively},
	journal = {Distill},
	doi = {10.23915/distill.00002},
	url = {http://distill.pub/2016/misread-tsne}
}
@inproceedings{Lee_Verleysen2014,
	author = {J. A. Lee and M. Verleysen},
	year = 2014,
	title = {Two key properties of dimensionality reduction methods},
	month = {Dec},
	booktitle = {2014 IEEE Symposium on Computational Intelligence and Data Mining (CIDM)},
	volume = {},
	number = {},
	pages = {163--170},
	doi = {10.1109/CIDM.2014.7008663},
	issn = {},
	keywords = {data reduction;data structures;neural nets;principal component analysis;DR;data representation;deep neural networks;dimensionality reduction;principal component analysis;Cost function;Covariance matrices;Force;Manifolds;Plastics;Principal component analysis;Vectors}
}
@inproceedings{simon1996,
	author = {Simon, Daniel R},
	year = 1996,
	title = {Anonymous communication and anonymous cash},
	booktitle = {Annual International Cryptology Conference},
	pages = {61--73},
	organization = {Springer}
}
@inproceedings{al2016,
	author = {Al-Mehairi, Yaared and Coecke, Bob and Lewis, Martha},
	year = 2016,
	title = {Categorical Compositional Cognition},
	booktitle = {International Symposium on Quantum Interaction},
	pages = {122--134},
	organization = {Springer}
}
@article{kartsaklis2013,
	author = {Kartsaklis, Dimitri and Sadrzadeh, Mehrnoosh and Pulman, Stephen and Coecke, Bob},
	year = 2013,
	title = {Reasoning about meaning in natural language with compact closed categories and frobenius algebras},
	journal = {Logic and Algebraic Structures in Quantum Computing},
	pages = 199
}
@article{berger2017cite2vec,
	author = {Berger, Matthew and McDonough, Katherine and Seversky, Lee M},
	year = 2017,
	title = {cite2vec: citation-driven document exploration via word embeddings},
	journal = {IEEE transactions on visualization and computer graphics},
	publisher = {IEEE},
	volume = 23,
	number = 1,
	pages = {691--700}
}
@article{lagarias2002beyond,
	author = {Lagarias, Jeffrey C and Mallows, Colin L and Wilks, Allan R},
	year = 2002,
	title = {Beyond the Descartes circle theorem},
	journal = {The American mathematical monthly},
	publisher = {JSTOR},
	volume = 109,
	number = 4,
	pages = {338--361}
}
@article{Gatys2015,
	author = {Leon A. Gatys and Alexander S. Ecker and Matthias Bethge},
	year = 2015,
	title = {A Neural Algorithm of Artistic Style},
	journal = {CoRR},
	volume = {abs/1508.06576},
	url = {http://arxiv.org/abs/1508.06576},
	archiveprefix = {arXiv},
	eprint = {1508.06576},
	timestamp = {Wed, 07 Jun 2017 14:41:58 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/GatysEB15a},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Haber2017,
	author = {Eldad Haber and Lars Ruthotto},
	year = 2017,
	title = {Stable Architectures for Deep Neural Networks},
	journal = {CoRR},
	volume = {abs/1705.03341},
	url = {http://arxiv.org/abs/1705.03341},
	archiveprefix = {arXiv},
	eprint = {1705.03341},
	timestamp = {Wed, 07 Jun 2017 14:40:21 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/HaberR17},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Chang2017,
	author = {Bo Chang and Lili Meng and Eldad Haber and Lars Ruthotto and David Begert and Elliot Holtham},
	year = 2017,
	title = {Reversible Architectures for Arbitrarily Deep Residual Neural Networks},
	journal = {CoRR},
	volume = {abs/1709.03698},
	url = {http://arxiv.org/abs/1709.03698},
	archiveprefix = {arXiv},
	eprint = {1709.03698},
	timestamp = {Thu, 05 Oct 2017 09:42:58 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1709-03698},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
                  
@inproceedings{Szegedy2013,
  author       = {Christian Szegedy and
                  Wojciech Zaremba and
                  Ilya Sutskever and
                  Joan Bruna and
                  Dumitru Erhan and
                  Ian J. Goodfellow and
                  Rob Fergus},
  editor       = {Yoshua Bengio and
                  Yann LeCun},
  title        = {Intriguing properties of neural networks},
  booktitle    = {2nd International Conference on Learning Representations, {ICLR} 2014,
                  Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings},
  year         = {2014},
  url          = {http://arxiv.org/abs/1312.6199},
  timestamp    = {Thu, 25 Jul 2019 14:35:25 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/SzegedyZSBEGF13.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}                  
@article{,
	author = {Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian J. Goodfellow and Rob Fergus},
	year = 2013,
	title = {Intriguing properties of neural networks},
	journal = {CoRR},
	booktitle = {International Conference on Learning Representations ({ICLR})},
	volume = {abs/1312.6199},
	url = {http://arxiv.org/abs/1312.6199},
	archiveprefix = {arXiv},
	eprint = {1312.6199},
	timestamp = {Wed, 07 Jun 2017 14:41:52 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/SzegedyZSBEGF13},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Johnson2016,
	author = {Justin Johnson and Alexandre Alahi and Fei{-}Fei Li},
	year = 2016,
	title = {Perceptual Losses for Real-Time Style Transfer and Super-Resolution},
	journal = {CoRR},
	volume = {abs/1603.08155},
	url = {http://arxiv.org/abs/1603.08155},
	archiveprefix = {arXiv},
	eprint = {1603.08155},
	timestamp = {Wed, 07 Jun 2017 14:40:26 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/JohnsonAL16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{prakash2018,
	author = {Aaditya Prakash and Nick Moran and Solomon Garber and Antonella DiLillo and James A. Storer},
	year = 2018,
	title = {Deflecting Adversarial Attacks with Pixel Deflection},
	journal = {CoRR},
	booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition ({CVPR}) Salt Lake City, UT, USA},
	volume = {abs/1801.08926},
	pages = {8571--8580},
	url = {http://arxiv.org/abs/1801.08926},
	archiveprefix = {arXiv},
	eprint = {1801.08926},
	timestamp = {Fri, 02 Feb 2018 14:20:25 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1801-08926},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{prakash_ecting_nodate,
	author = {Prakash, Aaditya and Moran, Nick and Garber, Solomon and DiLillo, Antonella and Storer, James and University, Brandeis},
	title = {Deflecting Adversarial Attacks with Pixel Deflection},
	pages = 17,
	abstract = {CNNs are poised to become integral parts of many critical systems. Despite their robustness to natural variations, image pixel values can be manipulated, via small, carefully crafted, imperceptible perturbations, to cause a model to misclassify images. We present an algorithm to process an image so that classiﬁcation accuracy is signiﬁcantly preserved in the presence of such adversarial manipulations. Image classiﬁers tend to be robust to natural noise, and adversarial attacks tend to be agnostic to object location. These observations motivate our strategy, which leverages model robustness to defend against adversarial perturbations by forcing the image to match natural image statistics. Our algorithm locally corrupts the image by redistributing pixel values via a process we term pixel deflection. A subsequent wavelet-based denoising operation softens this corruption, as well as some of the adversarial changes. We demonstrate experimentally that the combination of these techniques enables the effective recovery of the true class, against a variety of robust attacks. Our results compare favorably with current state-of-the-art defenses, without requiring retraining or modifying the CNN.},
	language = {en},
	file = {Prakash et al. - Deflecting Adversarial Attacks with Pixel Deflection.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\QUNFRPBY\\Prakash et al. - Deflecting Adversarial Attacks with Pixel Deflection.pdf:application/pdf}
}
@article{haber_stable_2018,
	author = {Haber, Eldad and Ruthotto, Lars},
	year = 2018,
	title = {Stable architectures for deep neural networks},
	month = jan,
	journal = {Inverse Problems},
	volume = 34,
	number = 1,
	pages = {014004},
	doi = {10.1088/1361-6420/aa9a90},
	issn = {0266-5611, 1361-6420},
	url = {http://stacks.iop.org/0266-5611/34/i=1/a=014004?key=crossref.1cc46f347b817746f33b5329460be31b},
	urldate = {2018-04-25},
	abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g., classiﬁcation of text or images. While often oﬀering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Important issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper we propose new forward propagation techniques inspired by systems of Ordinary Diﬀerential Equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks.},
	language = {en},
	file = {Haber and Ruthotto - 2018 - Stable architectures for deep neural networks.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\VGE5DB2I\\Haber and Ruthotto - 2018 - Stable architectures for deep neural networks.pdf:application/pdf}
}
@article{gatys_neural_2016,
	author = {Gatys, Leon and Ecker, Alexander and Bethge, Matthias},
	year = 2016,
	title = {A {Neural} {Algorithm} of {Artistic} {Style}},
	month = sep,
	journal = {Journal of Vision},
	volume = 16,
	number = 12,
	pages = 326,
	doi = {10.1167/16.12.326},
	issn = {1534-7362},
	url = {http://jov.arvojournals.org/article.aspx?doi=10.1167/16.12.326},
	urldate = {2018-04-25},
	language = {en},
	file = {Gatys et al. - 2016 - A Neural Algorithm of Artistic Style.pdf:C\:\\Use'rs\\Nexus\\Zotero\\storage\\8TSSA4PG\\Gatys et al. - 2016 - A Neural Algorithm of Artistic Style.pdf:application/pdf}
}
@article{chang_reversible_2017,
	author = {Chang, Bo and Meng, Lili and Haber, Eldad and Ruthotto, Lars and Begert, David and Holtham, Elliot},
	year = 2017,
	title = {Reversible Architectures for Arbitrarily Deep Residual Neural Networks},
	month = {Nov},
	pages = 8,
	abstract = {Recently, deep residual networks have been successfully applied in many computer vision and natural language processing tasks, pushing the state-of-the-art performance with deeper and wider architectures. In this work, we interpret deep residual networks as ordinary differential equations (ODEs), which have long been studied in mathematics and physics with rich theoretical and empirical success. From this interpretation, we develop a theoretical framework on stability and reversibility of deep neural networks, and derive three reversible neural network architectures that can go arbitrarily deep in theory. The reversibility property allows a memoryefﬁcient implementation, which does not need to store the activations for most hidden layers. Together with the stability of our architectures, this enables training deeper networks using only modest computational resources. We provide both theoretical analyses and empirical results. Experimental results demonstrate the efﬁcacy of our architectures against several strong baselines on CIFAR-10, CIFAR-100 and STL-10 with superior or on-par state-of-the-art performance. Furthermore, we show our architectures yield superior results when trained using fewer training data.},
	language = {en},
	file = {Chang et al. - Reversible Architectures for Arbitrarily Deep Resi.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\CS6F3WMH\\Chang et al. - Reversible Architectures for Arbitrarily Deep Resi.pdf:application/pdf}
}
@article{johnson_perceptual_2016,
	author = {Johnson, Justin and Alahi, Alexandre and Fei-Fei, Li},
	year = 2016,
	title = {Perceptual {Losses} for {Real}-{Time} {Style} {Transfer} and {Super}-{Resolution}},
	month = mar,
	journal = {arXiv:1603.08155 [cs]},
	url = {http://arxiv.org/abs/1603.08155},
	urldate = {2018-04-25},
	note = {arXiv: 1603.08155},
	abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a {\textbackslash}emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing {\textbackslash}emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	file = {arXiv\:1603.08155 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\8P957BLX\\Johnson et al. - 2016 - Perceptual Losses for Real-Time Style Transfer and.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\5XBNB97T\\1603.html:text/html}
}
@article{goodfellow_explaining_2014,
	author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
	year = 2014,
	title = {Explaining and {Harnessing} {Adversarial} {Examples}},
	month = dec,
	journal = {arXiv:1412.6572 [cs, stat]},
	booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015, San Diego, CA, USA},
	url = {http://arxiv.org/abs/1412.6572},
	urldate = {2018-04-25},
	note = {arXiv: 1412.6572},
	abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples---inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1412.6572 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\4AJ5ZRYV\\Goodfellow et al. - 2014 - Explaining and Harnessing Adversarial Examples.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\KB34UKC8\\1412.html:text/html},
	editor = {Yoshua Bengio and Yann LeCun}
}
@article{kurakin_adversarial_2016,
	author = {Kurakin, Alexey and Goodfellow, Ian and Bengio, Samy},
	year = 2016,
	title = {Adversarial examples in the physical world},
	month = jul,
	journal = {arXiv:1607.02533 [cs, stat]},
	booktitle = {5th International Conference on Learning Representations, {ICLR} 2017, Toulon, France},
	url = {http://arxiv.org/abs/1607.02533},
	urldate = {2018-04-25},
	note = {arXiv: 1607.02533},
	abstract = {Most existing machine learning classifiers are highly vulnerable to adversarial examples. An adversarial example is a sample of input data which has been modified very slightly in a way that is intended to cause a machine learning classifier to misclassify it. In many cases, these modifications can be so subtle that a human observer does not even notice the modification at all, yet the classifier still makes a mistake. Adversarial examples pose security concerns because they could be used to perform an attack on machine learning systems, even if the adversary has no access to the underlying model. Up to now, all previous work have assumed a threat model in which the adversary can feed data directly into the machine learning classifier. This is not always the case for systems operating in the physical world, for example those which are using signals from cameras and other sensors as an input. This paper shows that even in such physical world scenarios, machine learning systems are vulnerable to adversarial examples. We demonstrate this by feeding adversarial images obtained from cell-phone camera to an ImageNet Inception classifier and measuring the classification accuracy of the system. We find that a large fraction of adversarial examples are classified incorrectly even when perceived through the camera.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: 14 pages, 6 figures. Demo available at https://youtu.be/zQ\_uMenoBCk},
	file = {arXiv\:1607.02533 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\35I7YL6G\\Kurakin et al. - 2016 - Adversarial examples in the physical world.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\6BVECYWQ\\1607.html:text/html}
}
@article{papernot_cleverhans_2016,
	author = {Papernot, Nicolas and Carlini, Nicholas and Goodfellow, Ian and Feinman, Reuben and Faghri, Fartash and Matyasko, Alexander and Hambardzumyan, Karen and Juang, Yi-Lin and Kurakin, Alexey and Sheatsley, Ryan and Garg, Abhibhav and Lin, Yen-Chen},
	year = 2016,
	title = {cleverhans v2.0.0: an adversarial machine learning library},
	shorttitle = {cleverhans v2.0.0},
	month = oct,
	journal = {arXiv:1610.00768 [cs, stat]},
	url = {http://arxiv.org/abs/1610.00768},
	urldate = {2018-04-25},
	note = {arXiv: 1610.00768},
	abstract = {{\textbackslash}texttt\{cleverhans\} is a software library that provides standardized reference implementations of {\textbackslash}emph\{adversarial example\} construction techniques and {\textbackslash}emph\{adversarial training\}. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models' performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section{\textasciitilde}{\textbackslash}ref\{sec:introduction\} provides an overview of adversarial examples in machine learning and of the {\textbackslash}texttt\{cleverhans\} software. Section{\textasciitilde}{\textbackslash}ref\{sec:core\} presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section{\textasciitilde}{\textbackslash}ref\{sec:benchmark\} describes how to report benchmark results using the library. Section{\textasciitilde}{\textbackslash}ref\{sec:version\} describes the versioning system.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Statistics - Machine Learning},
	annote = {Comment: Technical report for https://github.com/openai/cleverhans},
	file = {arXiv\:1610.00768 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\RTY5GJJN\\Papernot et al. - 2016 - cleverhans v2.0.0 an adversarial machine learning.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\GZ9CJGY3\\1610.html:text/html}
}
@article{papernot_practical_2016,
	author = {Papernot, Nicolas and McDaniel, Patrick and Goodfellow, Ian and Jha, Somesh and Celik, Z. Berkay and Swami, Ananthram},
	year = 2016,
	title = {Practical {Black}-{Box} {Attacks} against {Machine} {Learning}},
	month = feb,
	journal = {arXiv:1602.02697 [cs]},
	url = {http://arxiv.org/abs/1602.02697},
	urldate = {2018-04-25},
	note = {arXiv: 1602.02697},
	abstract = {Machine learning (ML) models, e.g., deep neural networks (DNNs), are vulnerable to adversarial examples: malicious inputs modified to yield erroneous model outputs, while appearing unmodified to human observers. Potential attacks include having malicious content like malware identified as legitimate or controlling vehicle behavior. Yet, all existing adversarial example attacks require knowledge of either the model internals or its training data. We introduce the first practical demonstration of an attacker controlling a remotely hosted DNN with no such knowledge. Indeed, the only capability of our black-box adversary is to observe labels given by the DNN to chosen inputs. Our attack strategy consists in training a local model to substitute for the target DNN, using inputs synthetically generated by an adversary and labeled by the target DNN. We use the local substitute to craft adversarial examples, and find that they are misclassified by the targeted DNN. To perform a real-world and properly-blinded evaluation, we attack a DNN hosted by MetaMind, an online deep learning API. We find that their DNN misclassifies 84.24\% of the adversarial examples crafted with our substitute. We demonstrate the general applicability of our strategy to many ML techniques by conducting the same attack against models hosted by Amazon and Google, using logistic regression substitutes. They yield adversarial examples misclassified by Amazon and Google at rates of 96.19\% and 88.94\%. We also find that this black-box attack strategy is capable of evading defense strategies previously found to make adversarial example crafting harder.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning},
	annote = {Comment: Proceedings of the 2017 ACM Asia Conference on Computer and Communications Security, Abu Dhabi, UAE},
	file = {arXiv\:1602.02697 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\7E32ER7G\\Papernot et al. - 2016 - Practical Black-Box Attacks against Machine Learni.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\VGUUWBMH\\1602.html:text/html}
}
@article{papernot_limitations_2015,
	author = {Papernot, Nicolas and McDaniel, Patrick and Jha, Somesh and Fredrikson, Matt and Celik, Z. Berkay and Swami, Ananthram},
	year = 2015,
	title = {The {Limitations} of {Deep} {Learning} in {Adversarial} {Settings}},
	month = nov,
	journal = {arXiv:1511.07528 [cs, stat]},
	url = {http://arxiv.org/abs/1511.07528},
	urldate = {2018-04-25},
	note = {arXiv: 1511.07528},
	abstract = {Deep learning takes advantage of large datasets and computationally efficient training algorithms to outperform other approaches at various machine learning tasks. However, imperfections in the training phase of deep neural networks make them vulnerable to adversarial samples: inputs crafted by adversaries with the intent of causing deep neural networks to misclassify. In this work, we formalize the space of adversaries against deep neural networks (DNNs) and introduce a novel class of algorithms to craft adversarial samples based on a precise understanding of the mapping between inputs and outputs of DNNs. In an application to computer vision, we show that our algorithms can reliably produce samples correctly classified by human subjects but misclassified in specific targets by a DNN with a 97\% adversarial success rate while only modifying on average 4.02\% of the input features per sample. We then evaluate the vulnerability of different sample classes to adversarial perturbations by defining a hardness measure. Finally, we describe preliminary work outlining defenses against adversarial samples by defining a predictive measure of distance between a benign input and a target classification.},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	annote = {Comment: Accepted to the 1st IEEE European Symposium on Security \& Privacy, IEEE 2016. Saarbrucken, Germany},
	file = {arXiv\:1511.07528 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\SI9WW5F6\\Papernot et al. - 2015 - The Limitations of Deep Learning in Adversarial Se.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\VQ5FT6KW\\1511.html:text/html}
}
@article{moosavi-dezfooli_deepfool:_2015,
	author = {Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
	year = 2015,
	title = {{DeepFool}: a simple and accurate method to fool deep neural networks},
	shorttitle = {{DeepFool}},
	month = nov,
	journal = {arXiv:1511.04599 [cs]},
	url = {http://arxiv.org/abs/1511.04599},
	urldate = {2018-04-25},
	note = {arXiv: 1511.04599},
	abstract = {State-of-the-art deep neural networks have achieved impressive results on many image classification tasks. However, these same architectures have been shown to be unstable to small, well sought, perturbations of the images. Despite the importance of this phenomenon, no effective methods have been proposed to accurately compute the robustness of state-of-the-art deep classifiers to such perturbations on large-scale datasets. In this paper, we fill this gap and propose the DeepFool algorithm to efficiently compute perturbations that fool deep networks, and thus reliably quantify the robustness of these classifiers. Extensive experimental results show that our approach outperforms recent methods in the task of computing adversarial perturbations and making classifiers more robust.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning},
	annote = {Comment: In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016},
	file = {arXiv\:1511.04599 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\R4FMVV3B\\Moosavi-Dezfooli et al. - 2015 - DeepFool a simple and accurate method to fool dee.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\KZ58SXIR\\1511.html:text/html}
}
@article{carlini_towards_2016,
	author = {Carlini, Nicholas and Wagner, David},
	year = 2016,
	title = {Towards Evaluating the Robustness of Neural Networks},
	month = aug,
	journal = {arXiv:1608.04644 [cs]},
	booktitle = {2017 {IEEE} symposium on security and privacy ({SP})},
	pages = {39--57},
	url = {http://arxiv.org/abs/1608.04644},
	urldate = {2018-04-25},
	note = {arXiv: 1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input \$x\$ and any target classification \$t\$, it is possible to find a new input \$x'\$ that is similar to \$x\$ but classified as \$t\$. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from \$95{\textbackslash}\%\$ to \$0.5{\textbackslash}\%\$. In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with \$100{\textbackslash}\%\$ probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Cryptography and Security},
	file = {arXiv\:1608.04644 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\Q4T8UTKH\\Carlini and Wagner - 2016 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\PQ7PVTRJ\\1608.html:text/html},
	organization = {IEEE}
}
@article{madry_towards_2017,
	author = {Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
	year = 2017,
	title = {Towards {Deep} {Learning} {Models} {Resistant} to {Adversarial} {Attacks}},
	month = jun,
	journal = {arXiv:1706.06083 [cs, stat]},
	booktitle = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada},
	url = {http://arxiv.org/abs/1706.06083},
	urldate = {2018-04-25},
	note = {arXiv: 1706.06083},
	abstract = {Recent work has demonstrated that neural networks are vulnerable to adversarial examples, i.e., inputs that are almost indistinguishable from natural data and yet classified incorrectly by the network. In fact, some of the latest findings suggest that the existence of adversarial attacks may be an inherent weakness of deep learning models. To address this problem, we study the adversarial robustness of neural networks through the lens of robust optimization. This approach provides us with a broad and unifying view on much of the prior work on this topic. Its principled nature also enables us to identify methods for both training and attacking neural networks that are reliable and, in a certain sense, universal. In particular, they specify a concrete security guarantee that would protect against any adversary. These methods let us train networks with significantly improved resistance to a wide range of adversarial attacks. They also suggest the notion of security against a first-order adversary as a natural and broad security guarantee. We believe that robustness against such well-defined classes of adversaries is an important stepping stone towards fully resistant deep learning models.},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1706.06083 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\QL6LA2DS\\Madry et al. - 2017 - Towards Deep Learning Models Resistant to Adversar.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\Nexus\\Zotero\\storage\\6YSIR8L9\\1706.html:text/html}
}
@article{su_one_2017,
	author = {Su, Jiawei and Vargas, Danilo Vasconcellos and Kouichi, Sakurai},
	year = 2017,
	title = {One pixel attack for fooling deep neural networks},
	month = oct,
	journal = {arXiv:1710.08864 [cs, stat]},
	url = {http://arxiv.org/abs/1710.08864},
	urldate = {2018-04-25},
	note = {arXiv: 1710.08864},
	abstract = {Recent research has revealed that the output of Deep Neural Networks (DNN) can be easily altered by adding relatively small perturbations to the input vector. In this paper, we analyze an attack in an extremely limited scenario where only one pixel can be modified. For that we propose a novel method for generating one-pixel adversarial perturbations based on differential evolution. It requires less adversarial information and can fool more types of networks. The results show that 70.97\% of the natural images can be perturbed to at least one target class by modifying just one pixel with 97.47\% confidence on average. Thus, the proposed attack explores a different take on adversarial machine learning in an extreme limited scenario, showing that current DNNs are also vulnerable to such low dimension attacks.},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Learning, Statistics - Machine Learning},
	file = {arXiv\:1710.08864 PDF:C\:\\Users\\Nexus\\Zotero\\storage\\47WQZ5NK\\Su et al. - 2017 - One pixel attack for fooling deep neural networks.pdf:application/pdf}
}
@misc{aksh00,
	author = {Akshay Chawla},
	year = 2017,
	title = {{GTS}: {GNU} {Triangulated} {Surface} library},
	month = {November},
	howpublished = {https://github.com/akshaychawla/Adversarial-Examples-in-PyTorch}
}
@book{Bishop:2006:PRM:1162264,
	author = {Bishop, Christopher M.},
	year = 2006,
	title = {Pattern Recognition and Machine Learning (Information Science and Statistics)},
	publisher = {Springer-Verlag New York, Inc.},
	address = {Secaucus, NJ, USA},
	isbn = {0387310738}
}
@misc{arden2018,
	author = {Arden Dertat},
	year = 2018,
	title = {Applied-{Deep}-{Learning}-with-{Keras}/{Part} 4 ({GPU}) - {Convolutional} {Neural} {Networks}.ipynb at master · ardendertat/{Applied}-{Deep}-{Learning}-with-{Keras} · {GitHub}},
	url = {https://github.com/ardendertat/Applied-Deep-Learning-with-Keras/blob/master/notebooks/Part%204%20%28GPU%29%20-%20Convolutional%20Neural%20Networks.ipynb},
	urldate = {2018-06-30}
}
@article{nair_rectified_nodate,
	author = {Nair, Vinod and Hinton, Geoffrey E},
	year = 2010,
	title = {Rectified {Linear} {Units} {Improve} {Restricted} {Boltzmann} {Machines}},
	booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
	pages = {807--814},
	abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an inﬁnite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units” are unchanged. They can be approximated eﬃciently by noisy, rectiﬁed linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face veriﬁcation on the Labeled Faces in the Wild dataset. Unlike binary units, rectiﬁed linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
	language = {en},
	file = {Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:C\:\\Users\\Nexus\\Zotero\\storage\\MEQHIX28\\Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf:application/pdf}
}
@misc{goodfellow2013multidigit,
	author = {Ian J. Goodfellow and Yaroslav Bulatov and Julian Ibarz and Sacha Arnoud and Vinay Shet},
	year = 2013,
	title = {Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks},
	journal = {arXiv preprint, arXiv:1312.6082},
	eprint = {1312.6082},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@article{Khoury2018,
	author = {Marc Khoury and Dylan Hadfield{-}Menell},
	year = 2018,
	title = {On the Geometry of Adversarial Examples},
	journal = {CoRR},
	volume = {abs/1811.00525},
	url = {http://arxiv.org/abs/1811.00525},
	archiveprefix = {arXiv},
	eprint = {1811.00525},
	timestamp = {Thu, 22 Nov 2018 17:58:30 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1811-00525},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{mohammad16,
	author = {Mohammad Rastegari and Vicente Ordonez and Joseph Redmon and Ali Farhadi},
	year = 2016,
	title = {XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks},
	journal = {CoRR},
	volume = {abs/1603.05279},
	url = {http://arxiv.org/abs/1603.05279},
	archiveprefix = {arXiv},
	eprint = {1603.05279},
	timestamp = {Mon, 13 Aug 2018 16:47:22 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/RastegariORF16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{inevitable2018,
	author = {Ali Shafahi and W. Ronny Huang and Christoph Studer and Soheil Feizi and Tom Goldstein},
	year = 2018,
	title = {Are adversarial examples inevitable?},
	journal = {CoRR},
	booktitle = {International Conference on Learning Representations ({ICLR})},
	volume = {abs/1809.02104},
	url = {http://arxiv.org/abs/1809.02104},
	archiveprefix = {arXiv},
	eprint = {1809.02104},
	timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1809-02104},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{madry2018towards,
  author       = {Aleksander Madry and
                  Aleksandar Makelov and
                  Ludwig Schmidt and
                  Dimitris Tsipras and
                  Adrian Vladu},
  title        = {Towards Deep Learning Models Resistant to Adversarial Attacks},
  booktitle    = {6th International Conference on Learning Representations, {ICLR} 2018,
                  Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  publisher    = {OpenReview.net},
  year         = {2018},
  url          = {https://openreview.net/forum?id=rJzIBfZAb},
  timestamp    = {Thu, 25 Jul 2019 14:25:44 +0200},
  biburl       = {https://dblp.org/rec/conf/iclr/MadryMSTV18.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}                  
@article{tsipras2018robustness,
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	year = 2018,
	title = {Robustness may be at odds with accuracy},
	journal = {stat},
	booktitle = {7th International Conference on Learning Representations, {ICLR} 2019, New Orleans, LA, USA},
	volume = 1050,
	pages = 11
}
@inproceedings{nguyen2015deep,
	author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
	year = 2015,
	title = {Deep neural networks are easily fooled: High confidence predictions for unrecognizable images},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages = {427--436}
}
@article{simant2018,
	author = {Simant Dube},
	year = 2018,
	title = {High Dimensional Spaces, Deep Learning and Adversarial Examples},
	journal = {CoRR},
	volume = {abs/1801.00634},
	url = {http://arxiv.org/abs/1801.00634},
	archiveprefix = {arXiv},
	eprint = {1801.00634},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1801-00634},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{erichson2019,
	author = {N. Benjamin Erichson and Zhewei Yao and Michael W. Mahoney},
	year = 2019,
	title = {JumpReLU: {A} Retrofit Defense Strategy for Adversarial Attacks},
	journal = {CoRR},
	volume = {abs/1904.03750},
	url = {http://arxiv.org/abs/1904.03750},
	archiveprefix = {arXiv},
	eprint = {1904.03750},
	timestamp = {Thu, 25 Apr 2019 13:55:01 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1904-03750},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{frosst2018,
	author = {Nicholas Frosst and Sara Sabour and Geoffrey E. Hinton},
	year = 2018,
	title = {{DARCCC:} Detecting Adversaries by Reconstruction from Class Conditional Capsules},
	journal = {CoRR},
	volume = {abs/1811.06969},
	url = {http://arxiv.org/abs/1811.06969},
	archiveprefix = {arXiv},
	eprint = {1811.06969},
	timestamp = {Sun, 25 Nov 2018 18:57:12 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1811-06969},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
%%%%#######%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@article{volodmymyr2016,
	author = {Volodymyr Mnih and Adri{\`{a}} Puigdom{\`{e}}nech Badia and Mehdi Mirza and Alex Graves and Timothy P. Lillicrap and Tim Harley and David Silver and Koray Kavukcuoglu},
	year = 2016,
	title = {Asynchronous Methods for Deep Reinforcement Learning},
	journal = {CoRR},
	volume = {abs/1602.01783},
	url = {http://arxiv.org/abs/1602.01783},
	archiveprefix = {arXiv},
	eprint = {1602.01783},
	timestamp = {Wed, 07 Jun 2017 14:43:09 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/MnihBMGLHSK16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{Penghang2018,
	author = {Penghang Yin and Shuai Zhang and Jiancheng Lyu and Stanley Osher and Yingyong Qi and Jack Xin},
	year = 2018,
	title = {Blended Coarse Gradient Descent for Full Quantization of Deep Neural Networks},
	journal = {CoRR},
	volume = {abs/1808.05240},
	url = {http://arxiv.org/abs/1808.05240},
	archiveprefix = {arXiv},
	eprint = {1808.05240},
	timestamp = {Fri, 21 Dec 2018 14:34:10 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1808-05240},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{selvaraju2017,
	author = {Selvaraju, Ramprasaath R. and Cogswell, Michael and Das, Abhishek and Vedantam, Ramakrishna and Parikh, Devi and Batra, Dhruv},
	year = 2017,
	title = {Grad-{CAM}: {Visual} {Explanations} from {Deep} {Networks} via {Gradient}-{Based} {Localization}},
	shorttitle = {Grad-{CAM}},
	month = oct,
	booktitle = {2017 {IEEE} {International} {Conference} on {Computer} {Vision} ({ICCV})},
	publisher = {IEEE},
	address = {Venice},
	pages = {618--626},
	doi = {10.1109/ICCV.2017.74},
	isbn = {978-1-5386-1032-9},
	url = {http://ieeexplore.ieee.org/document/8237336/},
	urldate = {2019-05-16},
	abstract = {We propose a technique for producing â€˜visual explanationsâ€™ for decisions from a large class of Convolutional Neural Network (CNN)-based models, making them more transparent. Our approach â€“ Gradient-weighted Class Activation Mapping (Grad-CAM), uses the gradients of any target concept (say logits for â€˜dogâ€™ or even a caption), ï¬‚owing into the ï¬nal convolutional layer to produce a coarse localization map highlighting the important regions in the image for predicting the concept. Unlike previous approaches, Grad-CAM is applicable to a wide variety of CNN model-families: (1) CNNs with fully-connected layers (e.g. VGG), (2) CNNs used for structured outputs (e.g. captioning), (3) CNNs used in tasks with multi-modal inputs (e.g. VQA) or reinforcement learning, without architectural changes or re-training. We combine Grad-CAM with existing ï¬ne-grained visualizations to create a high-resolution class-discriminative visualization and apply it to image classiï¬cation, image captioning, and visual question answering (VQA) models, including ResNet-based architectures. In the context of image classiï¬cation models, our visualizations (a) lend insights into failure modes of these models (showing that seemingly unreasonable predictions have reasonable explanations), (b) are robust to adversarial images, (c) outperform previous methods on the ILSVRC-15 weakly-supervised localization task, (d) are more faithful to the underlying model, and (e) help achieve model generalization by identifying dataset bias. For image captioning and VQA, our visualizations show even non-attention based models can localize inputs. Finally, we design and conduct human studies to measure if Grad-CAM explanations help users establish appropriate trust in predictions from deep networks and show that GradCAM helps untrained users successfully discern a â€˜strongerâ€™ deep network from a â€˜weakerâ€™ one. Our code is available at https://github.com/ramprs/grad-cam/ and a demo is available on CloudCV [2]1. Video of the demo can be found at youtu.be/COjUB9Izk6E.},
	language = {en},
	file = {Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf:/home/bwbell/Zotero/storage/G7359QEK/Selvaraju et al. - 2017 - Grad-CAM Visual Explanations from Deep Networks v.pdf:application/pdf}
}
@inproceedings{Lee2018ASU,
	author = {Kimin Lee and Kibok Lee and Honglak Lee and Jinwoo Shin},
	year = 2018,
	title = {A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks},
	booktitle = {NeurIPS}
}
@inproceedings{gao2018robust,
	author = {Gao, Rui and Xie, Liyan and Xie, Yao and Xu, Huan},
	year = 2018,
	title = {Robust hypothesis testing using wasserstein uncertainty sets},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {7902--7912}
}
@inproceedings{Zhou:2010:IYR:1879141.1879193,
	author = {Zhou, Renjie and Khemmarat, Samamon and Gao, Lixin},
	year = 2010,
	title = {The Impact of YouTube Recommendation System on Video Views},
	booktitle = {Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement},
	location = {Melbourne, Australia},
	publisher = {ACM},
	address = {New York, NY, USA},
	series = {IMC '10},
	pages = {404--410},
	doi = {10.1145/1879141.1879193},
	isbn = {978-1-4503-0483-2},
	url = {http://doi.acm.org/10.1145/1879141.1879193},
	numpages = 7,
	acmid = 1879193,
	keywords = {YouTube, recommendation system, video sharing site, view diversity, view sources}
}
@article{DBLP:journals/corr/abs-1811-10104,
	author = {Ben Hutchinson and Margaret Mitchell},
	year = 2018,
	title = {50 Years of Test (Un)fairness: Lessons for Machine Learning},
	journal = {CoRR},
	volume = {abs/1811.10104},
	url = {http://arxiv.org/abs/1811.10104},
	archiveprefix = {arXiv},
	eprint = {1811.10104},
	timestamp = {Fri, 30 Nov 2018 12:44:28 +0100},
	biburl = {https://dblp.org/rec/bib/journals/corr/abs-1811-10104},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/EvtimovEFKLPRS17,
	author = {Ivan Evtimov and Kevin Eykholt and Earlence Fernandes and Tadayoshi Kohno and Bo Li and Atul Prakash and Amir Rahmati and Dawn Song},
	year = 2017,
	title = {Robust Physical-World Attacks on Machine Learning Models},
	journal = {CoRR},
	volume = {abs/1707.08945},
	url = {http://arxiv.org/abs/1707.08945},
	archiveprefix = {arXiv},
	eprint = {1707.08945},
	timestamp = {Thu, 09 May 2019 13:10:56 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/EvtimovEFKLPRS17},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{DBLP:journals/corr/BojarskiTDFFGJM16,
	author = {Mariusz Bojarski and Davide Del Testa and Daniel Dworakowski and Bernhard Firner and Beat Flepp and Prasoon Goyal and Lawrence D. Jackel and Mathew Monfort and Urs Muller and Jiakai Zhang and Xin Zhang and Jake Zhao and Karol Zieba},
	year = 2016,
	title = {End to End Learning for Self-Driving Cars},
	journal = {CoRR},
	volume = {abs/1604.07316},
	url = {http://arxiv.org/abs/1604.07316},
	archiveprefix = {arXiv},
	eprint = {1604.07316},
	timestamp = {Mon, 13 Aug 2018 16:47:06 +0200},
	biburl = {https://dblp.org/rec/bib/journals/corr/BojarskiTDFFGJM16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{wyly2008subprime,
	author = {Wyly, Elvin K and Moos, Markus and Foxcroft, Holly and Kabahizi, Emmanuel},
	year = 2008,
	title = {Subprime mortgage segmentation in the American urban system},
	journal = {Tijdschrift voor economische en sociale geografie},
	publisher = {Wiley Online Library},
	volume = 99,
	number = 1,
	pages = {3--23}
}
@article{rosenblatt1958perceptron,
	author = {Rosenblatt, Frank},
	year = 1958,
	title = {The perceptron: a probabilistic model for information storage and organization in the brain.},
	journal = {Psychological review},
	publisher = {American Psychological Association},
	volume = 65,
	number = 6,
	pages = 386
}
@book{ivakhnenko1965cybernetic,
	author = {Ivakhnenko, Alekse Grigorevich and Lapa, Valentin Grigor\'evich},
	year = 1965,
	title = {Cybernetic predicting devices},
	publisher = {CCM Information Corporation}
}
@article{SCHMIDHUBER201585,
	author = {JÃ¼rgen Schmidhuber},
	year = 2015,
	title = {Deep learning in neural networks: An overview},
	journal = {Neural Networks},
	volume = 61,
	pages = {85--117},
	doi = {https://doi.org/10.1016/j.neunet.2014.09.003},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
	keywords = {Deep learning, Supervised learning, Unsupervised learning, Reinforcement learning, Evolutionary computation},
	abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning & evolutionary computation, and indirect search for short programs encoding deep and large networks.}
}
@article{minsky1969perceptrons,
	author = {Minsky, Marvin and Papert, Seymour},
	year = 1969,
	title = {Perceptrons: Anlntroduction to computational geometry},
	journal = {MITPress, Cambridge, Massachusetts}
}
@article{werbos1974beyond,
	author = {Werbos, Paul J},
	year = 1974,
	title = {Beyond regression: New tools for prediction and analysis in the be havioral sciences/'PhD diss., Harvard Uni versity. Werbos, Paul J. 1988},
	journal = {Generalization of back propagation with application to a recurrent gas market method," Neural Networks},
	volume = 1,
	number = 4,
	pages = {339--356}
}
@article{mcclelland1986parallel,
	author = {McClelland, James L and Rumelhart, David E and PDP Research Group and others},
	year = 1986,
	title = {Parallel distributed processing},
	journal = {Explorations in the Microstructure of Cognition},
	publisher = {MIT Press Cambridge, Ma},
	volume = 2,
	pages = {216--271}
}
@article{lecun1998gradient,
	author = {LeCun, Yann and Bottou, L{\'e}on and Bengio, Yoshua and Haffner, Patrick and others},
	year = 1998,
	title = {Gradient-based learning applied to document recognition},
	journal = {Proceedings of the IEEE},
	publisher = {Taipei, Taiwan},
	volume = 86,
	number = 11,
	pages = {2278--2324}
}
@article{lecun1989backpropagation,
	author = {LeCun, Yann and Boser, Bernhard and Denker, John S and Henderson, Donnie and Howard, Richard E and Hubbard, Wayne and Jackel, Lawrence D},
	year = 1989,
	title = {Backpropagation applied to handwritten zip code recognition},
	journal = {Neural computation},
	publisher = {MIT Press},
	volume = 1,
	number = 4,
	pages = {541--551}
}
@inproceedings{goodfellow2014generative,
	author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
	year = 2014,
	title = {Generative adversarial nets},
	booktitle = {Advances in neural information processing systems},
	pages = {2672--2680}
}
@article{malik1990preattentive,
	author = {Malik, Jitendra and Perona, Pietro},
	year = 1990,
	title = {Preattentive texture discrimination with early vision mechanisms},
	journal = {JOSA A},
	publisher = {Optical Society of America},
	volume = 7,
	number = 5,
	pages = {923--932}
}
@inproceedings{li2017convergence,
	author = {Yuanzhi {Li} and Yang {Yuan}},
	year = 2017,
	title = {Convergence Analysis of Two-layer Neural Networks with ReLU Activation},
	booktitle = {Advances in Neural Information Processing Systems},
	pages = {597--607},
	notes = {Sourced from Microsoft Academic - https://academic.microsoft.com/paper/2963519230}
}
@article{petersen2018optimal,
	author = {Petersen, Philipp and Voigtlaender, Felix},
	year = 2018,
	title = {Optimal approximation of piecewise smooth functions using deep ReLU neural networks},
	journal = {Neural Networks},
	publisher = {Elsevier},
	volume = 108,
	pages = {296--330}
}
@article{HardtRS15,
	author = {Moritz Hardt and Benjamin Recht and Yoram Singer},
	year = 2015,
	title = {Train faster, generalize better: Stability of stochastic gradient descent},
	journal = {CoRR},
	volume = {abs/1509.01240},
	url = {http://arxiv.org/abs/1509.01240},
	archiveprefix = {arXiv},
	eprint = {1509.01240},
	timestamp = {Mon, 13 Aug 2018 16:46:09 +0200},
	biburl = {https://dblp.org/rec/journals/corr/HardtRS15.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{liu2015very,
	author = {Liu, Shuying and Deng, Weihong},
	year = 2015,
	title = {Very deep convolutional neural network based image classification using small training sample size},
	booktitle = {2015 3rd IAPR Asian conference on pattern recognition (ACPR)},
	pages = {730--734},
	organization = {IEEE}
}
@article{rumelhart1986learning,
	author = {Rumelhart, David E and Hinton, Geoffrey E and Williams, Ronald J},
	year = 1986,
	title = {Learning representations by back-propagating errors},
	journal = {nature},
	publisher = {Nature Publishing Group},
	volume = 323,
	number = 6088,
	pages = {533--536}
}
@inproceedings{lecun1988theoretical,
	author = {LeCun, Yann and Touresky, D and Hinton, G and Sejnowski, T},
	year = 1988,
	title = {A theoretical framework for back-propagation},
	booktitle = {Proceedings of the 1988 connectionist models summer school},
	volume = 1,
	pages = {21--28},
	organization = {CMU, Pittsburgh, Pa: Morgan Kaufmann}
}
@article{liu1989limited,
	author = {Liu, Dong C and Nocedal, Jorge},
	year = 1989,
	title = {On the limited memory BFGS method for large scale optimization},
	journal = {Mathematical programming},
	publisher = {Springer},
	volume = 45,
	number = {1-3},
	pages = {503--528}
}
@article{lecun1995convolutional,
	author = {LeCun, Yann and Bengio, Yoshua and others},
	year = 1995,
	title = {Convolutional networks for images, speech, and time series},
	journal = {The handbook of brain theory and neural networks},
	volume = 3361,
	number = 10,
	pages = 1995
}
@article{mcculloch1943logical,
	author = {McCulloch, Warren S and Pitts, Walter},
	year = 1943,
	title = {A logical calculus of the ideas immanent in nervous activity},
	journal = {The bulletin of mathematical biophysics},
	publisher = {Springer},
	volume = 5,
	number = 4,
	pages = {115--133}
}
@article{linnainmaa1970representation,
	author = {Linnainmaa, Seppo},
	year = 1970,
	title = {The representation of the cumulative rounding error of an algorithm as a Taylor expansion of the local rounding errors},
	journal = {Master's Thesis (in Finnish), Univ. Helsinki},
	pages = {6--7}
}
@article{kak1993training,
	author = {Kak, Subhash},
	year = 1993,
	title = {On training feedforward neural networks},
	journal = {Pramana},
	publisher = {Springer},
	volume = 40,
	number = 1,
	pages = {35--42}
}
@article{simonyan2014very,
	author = {Simonyan, Karen and Zisserman, Andrew},
	year = 2014,
	title = {Very deep convolutional networks for large-scale image recognition},
	journal = {arXiv preprint arXiv:1409.1556},
	booktitle = {International Conference on Learning Representations}
}
@article{wiyatno2018saliency,
	author = {Rey Wiyatno and Anqi Xu},
	year = 2018,
	title = {Maximal Jacobian-based Saliency Map Attack},
	journal = {CoRR},
	volume = {abs/1808.07945},
	url = {http://arxiv.org/abs/1808.07945},
	archiveprefix = {arXiv},
	eprint = {1808.07945},
	timestamp = {Sun, 02 Sep 2018 15:01:56 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1808-07945.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{Krause20,
	author = {Andreas Krause},
	year = 2020,
	title = {Introduction to Machine Learning},
	month = {August},
	publisher = {Learning and Adaptive Systems ETH}
}
@inproceedings{coates2011analysis,
	author = {Coates, Adam and Ng, Andrew and Lee, Honglak},
	year = 2011,
	title = {An analysis of single-layer networks in unsupervised feature learning},
	booktitle = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
	pages = {215--223},
	organization = {JMLR Workshop and Conference Proceedings}
}
@inproceedings{glorot2011deep,
	author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
	year = 2011,
	title = {Deep sparse rectifier neural networks},
	booktitle = {Proceedings of the fourteenth international conference on artificial intelligence and statistics},
	pages = {315--323},
	organization = {JMLR Workshop and Conference Proceedings}
}
@article{collobert2011natural,
	author = {Collobert, Ronan and Weston, Jason and Bottou, L{\'e}on and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
	year = 2011,
	title = {Natural language processing (almost) from scratch},
	journal = {Journal of machine learning research},
	volume = 12,
	number = {ARTICLE},
	pages = {2493--2537}
}
@inproceedings{mikolov2010recurrent,
	author = {Mikolov, Tomas and Karafi{\'a}t, Martin and Burget, Lukas and Cernock{\`y}, Jan and Khudanpur, Sanjeev},
	year = 2010,
	title = {Recurrent neural network based language model.},
	booktitle = {Interspeech},
	volume = 2,
	number = 3,
	pages = {1045--1048},
	organization = {Makuhari}
}
@article{vincent2010stacked,
	author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine and Bottou, L{\'e}on},
	year = 2010,
	title = {Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.},
	journal = {Journal of machine learning research},
	volume = 11,
	number = 12
}
@inproceedings{boureau2010learning,
	author = {Boureau, Y-Lan and Bach, Francis and LeCun, Yann and Ponce, Jean},
	year = 2010,
	title = {Learning mid-level features for recognition},
	booktitle = {2010 IEEE computer society conference on computer vision and pattern recognition},
	pages = {2559--2566},
	organization = {IEEE}
}
@article{hinton2010practical,
	author = {Hinton, Geoffrey},
	year = 2010,
	title = {A practical guide to training restricted Boltzmann machines},
	journal = {Momentum},
	volume = 9,
	number = 1,
	pages = 926
}
@inproceedings{glorot2010understanding,
	author = {Glorot, Xavier and Bengio, Yoshua},
	year = 2010,
	title = {Understanding the difficulty of training deep feedforward neural networks},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	pages = {249--256},
	organization = {JMLR Workshop and Conference Proceedings}
}
@inproceedings{erhan2010does,
	author = {Erhan, Dumitru and Courville, Aaron and Bengio, Yoshua and Vincent, Pascal},
	year = 2010,
	title = {Why does unsupervised pre-training help deep learning?},
	booktitle = {Proceedings of the thirteenth international conference on artificial intelligence and statistics},
	pages = {201--208},
	organization = {JMLR Workshop and Conference Proceedings}
}
@article{bengio2009learning,
	author = {Bengio, Yoshua and others},
	year = 2009,
	title = {Learning deep architectures for AI},
	journal = {Foundations and trends{\textregistered} in Machine Learning},
	publisher = {Now Publishers, Inc.},
	volume = 2,
	number = 1,
	pages = {1--127}
}
@inproceedings{lee2009convolutional,
	author = {Lee, Honglak and Grosse, Roger and Ranganath, Rajesh and Ng, Andrew Y},
	year = 2009,
	title = {Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations},
	booktitle = {Proceedings of the 26th annual international conference on machine learning},
	pages = {609--616}
}
@misc{bengio2007greedy,
	author = {Bengio, Y and Lamblin, P and Popovici, D and Larochelle, H},
	year = 2007,
	title = {Greedy layer-wise training of deep networks. NIPS 19 (pp. 153--160)},
	publisher = {MIT Press}
}
@article{hinton2006reducing,
	author = {Hinton, Geoffrey E and Salakhutdinov, Ruslan R},
	year = 2006,
	title = {Reducing the dimensionality of data with neural networks},
	journal = {science},
	publisher = {American Association for the Advancement of Science},
	volume = 313,
	number = 5786,
	pages = {504--507}
}
@article{hinton2006fast,
	author = {Hinton, Geoffrey E and Osindero, Simon and Teh, Yee-Whye},
	year = 2006,
	title = {A fast learning algorithm for deep belief nets},
	journal = {Neural computation},
	publisher = {MIT Press One Rogers Street, Cambridge, MA 02142-1209},
	volume = 18,
	number = 7,
	pages = {1527--1554}
}
@article{hochreiter1997long,
	author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
	year = 1997,
	title = {Long short-term memory},
	journal = {Neural computation},
	publisher = {MIT press},
	volume = 9,
	number = 8,
	pages = {1735--1780}
}
and an inexorable expansion of  pre-built and well-maintained
libraries for working with neural networks including the two most
famous: torch 2002 and tensorflow (released 2015)
@inproceedings{Collobert2002TorchAM,
	author = {Ronan Collobert and Samy Bengio and Johnny Mari{\'e}thoz},
	year = 2002,
	title = {Torch: a modular machine learning software library}
}
@incollection{pytorch2019,
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	year = 2019,
	title = {PyTorch: An Imperative Style, High-Performance Deep Learning Library},
	booktitle = {Advances in Neural Information Processing Systems 32},
	publisher = {Curran Associates, Inc.},
	pages = {8024--8035},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf}
}
@misc{tensorflow2015-whitepaper,
	author = {Mart\'{i}n~Abadi and Ashish~Agarwal and Paul~Barham and Eugene~Brevdo and Zhifeng~Chen and Craig~Citro and Greg~S.~Corrado and Andy~Davis and Jeffrey~Dean and Matthieu~Devin and Sanjay~Ghemawat and Ian~Goodfellow and Andrew~Harp and Geoffrey~Irving and Michael~Isard and Yangqing Jia and Rafal~Jozefowicz and Lukasz~Kaiser and Manjunath~Kudlur and Josh~Levenberg and Dandelion~Man\'{e} and Rajat~Monga and Sherry~Moore and Derek~Murray and Chris~Olah and Mike~Schuster and Jonathon~Shlens and Benoit~Steiner and Ilya~Sutskever and Kunal~Talwar and Paul~Tucker and Vincent~Vanhoucke and Vijay~Vasudevan and Fernanda~Vi\'{e}gas and Oriol~Vinyals and Pete~Warden and Martin~Wattenberg and Martin~Wicke and Yuan~Yu and Xiaoqiang~Zheng},
	year = 2015,
	title = {{TensorFlow}: Large-Scale Machine Learning on Heterogeneous Systems},
	url = {https://www.tensorflow.org/},
	note = {Software available from tensorflow.org}
}
@misc{vaswani2017attention,
	author = {Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
	year = 2017,
	title = {Attention Is All You Need},
	eprint = {1706.03762},
	archiveprefix = {arXiv},
	primaryclass = {cs.CL}
}
@misc{functorch,
	author = {Horace He, Richard Zou},
	year = 2021,
	title = {functorch: JAX-like composable function transforms for PyTorch},
	howpublished = {\url{https://github.com/pytorch/functorch}}
}
@inproceedings{bietti2019bias,
	author = {Bietti, Alberto and Mairal, Julien},
	year = 2019,
	title = {On the Inductive Bias of Neural Tangent Kernels},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 32,
	pages = {},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/c4ef9c39b300931b69a36fb3dbb8d60e-Paper.pdf},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@book{rasmussen2006gaussian,
	author = {Rasmussen, Carl Edward and Williams, Christopher KI and others},
	year = 2006,
	title = {Gaussian processes for machine learning},
	publisher = {Springer},
	volume = 1
}
@inproceedings{du2019graphntk,
	author = {Du, Simon S and Hou, Kangcheng and Salakhutdinov, Russ R and Poczos, Barnabas and Wang, Ruosong and Xu, Keyulu},
	year = 2019,
	title = {Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 32,
	pages = {},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/663fd3c5144fd10bd5ca6611a9a5b92d-Paper.pdf},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@article{abdar2021uq,
	author = {Moloud Abdar and Farhad Pourpanah and Sadiq Hussain and Dana Rezazadegan and Li Liu and Mohammad Ghavamzadeh and Paul Fieguth and Xiaochun Cao and Abbas Khosravi and U. Rajendra Acharya and Vladimir Makarenkov and Saeid Nahavandi},
	year = 2021,
	title = {A review of uncertainty quantification in deep learning: Techniques, applications and challenges},
	journal = {Information Fusion},
	volume = 76,
	pages = {243--297},
	doi = {https://doi.org/10.1016/j.inffus.2021.05.008},
	issn = {1566-2535},
	url = {https://www.sciencedirect.com/science/article/pii/S1566253521001081},
	keywords = {Artificial intelligence, Uncertainty quantification, Deep learning, Machine learning, Bayesian statistics, Ensemble learning},
	abstract = {Uncertainty quantification (UQ) methods play a pivotal role in reducing the impact of uncertainties during both optimization and decision making processes. They have been applied to solve a variety of real-world problems in science and engineering. Bayesian approximation and ensemble learning techniques are two widely-used types of uncertainty quantification (UQ) methods. In this regard, researchers have proposed different UQ methods and examined their performance in a variety of applications such as computer vision (e.g., self-driving cars and object detection), image processing (e.g., image restoration), medical image analysis (e.g., medical image classification and segmentation), natural language processing (e.g., text classification, social media texts and recidivism risk-scoring), bioinformatics, etc. This study reviews recent advances in UQ methods used in deep learning, investigates the application of these methods in reinforcement learning, and highlights fundamental research challenges and directions associated with UQ.}
}
@inproceedings{tancik2020fourierfeatures,
	author = {Tancik, Matthew and Srinivasan, Pratul and Mildenhall, Ben and Fridovich-Keil, Sara and Raghavan, Nithin and Singhal, Utkarsh and Ramamoorthi, Ravi and Barron, Jonathan and Ng, Ren},
	year = 2020,
	title = {Fourier Features Let Networks Learn High Frequency Functions in Low Dimensional Domains},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 33,
	pages = {7537--7547},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/55053683268957697aa39fba6f231c68-Paper.pdf},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@misc{incudini2022quantum,
	author = {Massimiliano Incudini and Michele Grossi and Antonio Mandarino and Sofia Vallecorsa and Alessandra Di Pierro and David Windridge},
	year = 2022,
	title = {The Quantum Path Kernel: a Generalized Quantum Neural Tangent Kernel for Deep Quantum Machine Learning},
	eprint = {2212.11826},
	archiveprefix = {arXiv},
	primaryclass = {quant-ph}
}
@article{hoover1982high,
	author = {Hoover, William G and Ladd, Anthony JC and Moran, Bill},
	year = 1982,
	title = {High-strain-rate plastic flow studied via nonequilibrium molecular dynamics},
	journal = {Physical Review Letters},
	publisher = {APS},
	volume = 48,
	number = 26,
	pages = 1818
}
@article{evans1983computer,
	author = {Evans, Denis J},
	year = 1983,
	title = {Computer ‘‘experiment’’for nonlinear thermodynamics of Couette flow},
	journal = {The Journal of Chemical Physics},
	publisher = {American Institute of Physics},
	volume = 78,
	number = 6,
	pages = {3297--3302}
}
@article{evans1990computer,
	author = {Evans, DJ and Morriss, GP},
	year = 1990,
	title = {Computer Phys. Rep. 1, 297 (1984). 25 DJ Evans and GP Morris},
	journal = {Statistical Mechanics of Nonequilibrium Liquids}
}
@inproceedings{chizat2020maxmargin,
	author = {Chizat, L\'ena\"ic  and Bach, Francis},
	year = 2020,
	title = {Implicit Bias of Gradient Descent for Wide Two-layer Neural Networks Trained with the Logistic Loss},
	month = {09--12 Jul},
	booktitle = {Proceedings of Thirty Third Conference on Learning Theory},
	publisher = {PMLR},
	series = {Proceedings of Machine Learning Research},
	volume = 125,
	pages = {1305--1338},
	url = {https://proceedings.mlr.press/v125/chizat20a.html},
	editor = {Abernethy, Jacob and Agarwal, Shivani},
	pdf = {http://proceedings.mlr.press/v125/chizat20a/chizat20a.pdf},
	abstract = {Neural networks trained to minimize the logistic (a.k.a. cross-entropy) loss with gradient-based methods are observed to perform well in many supervised classification tasks. Towards understanding this phenomenon, we analyze the training and generalization behavior of infinitely wide two-layer neural networks with homogeneous activations. We show that the limits of the gradient flow on exponentially tailed losses can be fully characterized as a max-margin classifier in a certain non-Hilbertian space of functions. In presence of hidden low-dimensional structures, the resulting margin is independent of the ambiant dimension, which leads to strong generalization bounds. In contrast, training only the output layer implicitly solves a kernel support vector machine, which a priori does not enjoy such an adaptivity. Our analysis of training is non-quantitative in terms of running time but we prove computational guarantees in simplified settings by showing equivalences with online mirror descent. Finally, numerical experiments suggest that our analysis describes well the practical behavior of two-layer neural networks with ReLU activation and confirm the statistical benefits of this implicit bias.}
}
@article{shah2021input,
	author = {Shah, Harshay and Jain, Prateek and Netrapalli, Praneeth},
	year = 2021,
	title = {Do input gradients highlight discriminative features?},
	journal = {Advances in Neural Information Processing Systems},
	volume = 34,
	pages = {2046--2059}
}
@article{snelson2005sparse,
	author = {Snelson, Edward and Ghahramani, Zoubin},
	year = 2005,
	title = {Sparse Gaussian processes using pseudo-inputs},
	journal = {Advances in neural information processing systems},
	volume = 18
}
@inproceedings{geifman2020similarity,
	author = {Geifman, Amnon and Yadav, Abhay and Kasten, Yoni and Galun, Meirav and Jacobs, David and Ronen, Basri},
	year = 2020,
	title = {On the {Similarity} between the {Laplace} and {Neural} {Tangent} {Kernels}},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	volume = 33,
	pages = {1451--1461},
	url = {https://proceedings.neurips.cc/paper/2020/hash/1006ff12c465532f8c574aeaa4461b16-Abstract.html},
	urldate = {2023-05-16},
	file = {Full Text PDF:/Users/mgeyer/Zotero/storage/TMZRRUYZ/Geifman et al. - 2020 - On the Similarity between the Laplace and Neural T.pdf:application/pdf}
}
@article{chen2020generalized,
	author = {Zixiang Chen and Yuan Cao and Quanquan Gu and Tong Zhang},
	year = 2020,
	title = {Mean-Field Analysis of Two-Layer Neural Networks: Non-Asymptotic Rates and Generalization Bounds},
	journal = {CoRR},
	volume = {abs/2002.04026},
	url = {https://arxiv.org/abs/2002.04026},
	eprinttype = {arXiv},
	eprint = {2002.04026},
	timestamp = {Wed, 13 Jan 2021 16:36:40 +0100},
	biburl = {https://dblp.org/rec/journals/corr/abs-2002-04026.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@misc{alemohammad2021recurrent,
	author = {Alemohammad, Sina and Wang, Zichao and Balestriero, Randall and Baraniuk, Richard},
	year = 2021,
	title = {The {Recurrent} {Neural} {Tangent} {Kernel}},
	month = jun,
	publisher = {arXiv},
	doi = {10.48550/arXiv.2006.10246},
	url = {http://arxiv.org/abs/2006.10246},
	urldate = {2023-05-16},
	note = {arXiv:2006.10246 [cs, stat]},
	abstract = {The study of deep neural networks (DNNs) in the infinite-width limit, via the so-called neural tangent kernel (NTK) approach, has provided new insights into the dynamics of learning, generalization, and the impact of initialization. One key DNN architecture remains to be kernelized, namely, the recurrent neural network (RNN). In this paper we introduce and study the Recurrent Neural Tangent Kernel (RNTK), which provides new insights into the behavior of overparametrized RNNs. A key property of the RNTK should greatly benefit practitioners is its ability to compare inputs of different length. To this end, we characterize how the RNTK weights different time steps to form its output under different initialization parameters and nonlinearity choices. A synthetic and 56 real-world data experiments demonstrate that the RNTK offers significant performance gains over other kernels, including standard NTKs, across a wide array of data sets.},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/mgeyer/Zotero/storage/YW47W8N7/Alemohammad et al. - 2021 - The Recurrent Neural Tangent Kernel.pdf:application/pdf;arXiv.org Snapshot:/Users/mgeyer/Zotero/storage/8HB6QBKD/2006.html:text/html}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{langley00,
	author = {P. Langley},
	year = 2000,
	title = {Crafting Papers on Machine Learning},
	booktitle = {Proceedings of the 17th International Conference on Machine Learning (ICML 2000)},
	publisher = {Morgan Kaufmann},
	address = {Stanford, CA},
	pages = {1207--1216},
	editor = {Pat Langley}
}
@techreport{mitchell80,
	author = {T. M. Mitchell},
	year = 1980,
	title = {The Need for Biases in Learning Generalizations},
	address = {New Brunswick, MA},
	institution = {Computer Science Department, Rutgers University}
}
@phdthesis{kearns89,
	author = {M. J. Kearns},
	year = 1989,
	title = {Computational Complexity of Machine Learning},
	school = {Department of Computer Science, Harvard University}
}
@book{MachineLearningI,
	year = 1983,
	title = {Machine Learning: An Artificial Intelligence Approach, Vol. I},
	publisher = {Tioga},
	address = {Palo Alto, CA},
	editor = {R. S. Michalski and J. G. Carbonell and T. M. Mitchell}
}
@book{DudaHart2nd,
	author = {R. O. Duda and P. E. Hart and D. G. Stork},
	year = 2000,
	title = {Pattern Classification},
	publisher = {John Wiley and Sons},
	edition = {2nd}
}
@misc{anonymous,
	author = {Author, N. N.},
	year = 2021,
	title = {Suppressed for Anonymity}
}
@incollection{Newell81,
	author = {A. Newell and P. S. Rosenbloom},
	year = 1981,
	title = {Mechanisms of Skill Acquisition and the Law of Practice},
	booktitle = {Cognitive Skills and Their Acquisition},
	publisher = {Lawrence Erlbaum Associates, Inc.},
	address = {Hillsdale, NJ},
	pages = {1--51},
	editor = {J. R. Anderson},
	chapter = 1
}
@article{Samuel59,
	author = {A. L. Samuel},
	year = 1959,
	title = {Some Studies in Machine Learning Using the Game of Checkers},
	journal = {IBM Journal of Research and Development},
	volume = 3,
	number = 3,
	pages = {211--229}
}
@article{wang2022pinns,
	author = {Sifan Wang and Xinling Yu and Paris Perdikaris},
	year = 2022,
	title = {When and why PINNs fail to train: A neural tangent kernel perspective},
	journal = {Journal of Computational Physics},
	volume = 449,
	pages = 110768,
	doi = {https://doi.org/10.1016/j.jcp.2021.110768},
	issn = {0021-9991},
	url = {https://www.sciencedirect.com/science/article/pii/S002199912100663X},
	keywords = {Physics-informed neural networks, Spectral bias, Multi-task learning, Gradient descent, Scientific machine learning},
	abstract = {Physics-informed neural networks (PINNs) have lately received great attention thanks to their flexibility in tackling a wide range of forward and inverse problems involving partial differential equations. However, despite their noticeable empirical success, little is known about how such constrained neural networks behave during their training via gradient descent. More importantly, even less is known about why such models sometimes fail to train at all. In this work, we aim to investigate these questions through the lens of the Neural Tangent Kernel (NTK); a kernel that captures the behavior of fully-connected neural networks in the infinite width limit during training via gradient descent. Specifically, we derive the NTK of PINNs and prove that, under appropriate conditions, it converges to a deterministic kernel that stays constant during training in the infinite-width limit. This allows us to analyze the training dynamics of PINNs through the lens of their limiting NTK and find a remarkable discrepancy in the convergence rate of the different loss components contributing to the total training error. To address this fundamental pathology, we propose a novel gradient descent algorithm that utilizes the eigenvalues of the NTK to adaptively calibrate the convergence rate of the total training error. Finally, we perform a series of numerical experiments to verify the correctness of our theory and the practical effectiveness of the proposed algorithms. The data and code accompanying this manuscript are publicly available at https://github.com/PredictiveIntelligenceLab/PINNsNTK.}
}
@inproceedings{DBLP:conf/nips/BubeckS21,
	author = {S{\'{e}}bastien Bubeck and Mark Sellke},
	year = 2021,
	title = {A Universal Law of Robustness via Isoperimetry},
	booktitle = {Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual},
	pages = {28811--28822},
	url = {https://proceedings.neurips.cc/paper/2021/hash/f197002b9a0853eca5e046d9ca4663d5-Abstract.html},
	editor = {Marc'Aurelio Ranzato and Alina Beygelzimer and Yann N. Dauphin and Percy Liang and Jennifer Wortman Vaughan},
	timestamp = {Tue, 03 May 2022 16:20:49 +0200},
	biburl = {https://dblp.org/rec/conf/nips/BubeckS21.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{DBLP:conf/iclr/LeeBNSPS18,
	author = {Jaehoon Lee and Yasaman Bahri and Roman Novak and Samuel S. Schoenholz and Jeffrey Pennington and Jascha Sohl{-}Dickstein},
	year = 2018,
	title = {Deep Neural Networks as Gaussian Processes},
	booktitle = {6th International Conference on Learning Representations, {ICLR} 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
	publisher = {OpenReview.net},
	url = {https://openreview.net/forum?id=B1EA-M-0Z},
	timestamp = {Mon, 17 Jan 2022 07:47:59 +0100},
	biburl = {https://dblp.org/rec/conf/iclr/LeeBNSPS18.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{neal1996priors,
	author = {Neal, Radford M and Neal, Radford M},
	year = 1996,
	title = {Priors for infinite networks},
	journal = {Bayesian learning for neural networks},
	publisher = {Springer},
	pages = {29--53}
}
@inproceedings{tramer2020adaptive,
	author = {Florian Tram{\`{e}}r and Nicholas Carlini and Wieland Brendel and Aleksander Madry},
	title = {On Adaptive Attacks to Adversarial Example Defenses},
	booktitle = {Advances in Neural Information Processing Systems 33 ({NeurIPS} 2020), virtual},
	editor = {Hugo Larochelle and Marc'Aurelio Ranzato and Raia Hadsell and Maria{-}Florina Balcan and Hsuan{-}Tien Lin}
}
@inproceedings{tramer2018ensemble,
	author = {Florian Tram{\`{e}}r and Alexey Kurakin and Nicolas Papernot and Ian J. Goodfellow and Dan Boneh and Patrick D. McDaniel},
	title = {Ensemble Adversarial Training: Attacks and Defenses},
	booktitle = {6th International Conference on Learning Representations, ({ICLR} 2018), Vancouver, BC, Canada}
}
% dynamical clustering paper
@inproceedings{qin2020,
	author = {Yao Qin and Nicholas Frosst and Sara Sabour and Colin Raffel and Garrison W. Cottrell and Geoffrey E. Hinton},
	title = {Detecting and Diagnosing Adversarial Images with Class-Conditional Capsule Reconstructions},
	booktitle = {8th International Conference on Learning Representations, ({ICLR} 2020), Addis Ababa, Ethiopia}
}
%%%%#######%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@inproceedings{roth19aodds,
	author = {Roth, Kevin and Kilcher, Yannic and Hofmann, Thomas},
	year = 2019,
	title = {The Odds are Odd: A Statistical Test for Detecting Adversarial Examples},
	booktitle = {Proceedings of the 36th International Conference on Machine Learning ({ICML})},
	volume = 97,
	pages = {5498--5507},
	editor = {Kamalika Chaudhuri and Ruslan Salakhutdinov},
	abstract = {We investigate conditions under which test statistics exist that can reliably detect examples, which have been adversarially manipulated in a white-box attack. These statistics can be easily computed and calibrated by randomly corrupting inputs. They exploit certain anomalies that adversarial attacks introduce, in particular if they follow the paradigm of choosing perturbations optimally under p-norm constraints. Access to the log-odds is the only requirement to defend models. We justify our approach empirically, but also provide conditions under which detectability via the suggested test statistics is guaranteed to be effective. In our experiments, we show that it is even possible to correct test time predictions for adversarial attacks with high accuracy.}
}
@article{hosseini2019odds,
	author = {Hossein Hosseini and Sreeram Kannan and Radha Poovendran},
	year = 2019,
	title = {Are Odds Really Odd? Bypassing Statistical Detection of Adversarial Examples},
	journal = {CoRR},
	volume = {abs/1907.12138},
	url = {http://arxiv.org/abs/1907.12138},
	archiveprefix = {arXiv},
	eprint = {1907.12138},
	timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
	biburl = {https://dblp.org/rec/journals/corr/abs-1907-12138.bib},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{kim2021torchattacks,
	author = {Hoki Kim},
	year = 2020,
	title = {Torchattacks : {A} Pytorch Repository for Adversarial Attacks},
	journal = {CoRR},
	volume = {abs/2010.01950}
}
@inproceedings{yu2019new,
	author = {Shengyuan Hu and Tao Yu and Chuan Guo and Wei{-}Lun Chao and Kilian Q. Weinberger},
	title = {A New Defense Against Adversarial Images: Turning a Weakness into a Strength},
	booktitle = {Advances in Neural Information Processing Systems 32 ({NeurIPS} 2019) Vancouver, BC, Canada},
	pages = {1633--1644},
	editor = {Hanna M. Wallach and Hugo Larochelle and Alina Beygelzimer and Florence d'Alch{\'{e}}{-}Buc and Emily B. Fox and Roman Garnett}
}
@inproceedings{aggsurprising,
	author = {Aggarwal, Charu C. and Hinneburg, Alexander and Keim, Daniel A.},
	year = 2001,
	title = {On the Surprising Behavior of Distance Metrics in High Dimensional Space},
	booktitle = {Database Theory --- ICDT 2001},
	publisher = {Springer Berlin Heidelberg},
	address = {Berlin, Heidelberg},
	pages = {420--434},
	isbn = {978-3-540-44503-6},
	editor = {Van den Bussche, Jan and Vianu, Victor}
}
@article{MNIST,
	author = {LeCun, Yann and Cortes, Corinna},
	year = 2010,
	title = {{MNIST} handwritten digit database},
	url = {http://yann.lecun.com/exdb/mnist/},
	added-at = {2010-06-28T21:16:30.000+0200},
	biburl = {https://www.bibsonomy.org/bibtex/2935bad99fa1f65e03c25b315aa3c1032/mhwombat},
	groups = {public},
	howpublished = {http://yann.lecun.com/exdb/mnist/},
	interhash = {21b9d0558bd66279df9452562df6e6f3},
	intrahash = {935bad99fa1f65e03c25b315aa3c1032},
	keywords = {MSc _checked character_recognition mnist network neural},
	lastchecked = {2016-01-14 14:24:11},
	timestamp = {2016-07-12T19:25:30.000+0200},
	username = {mhwombat}
}
@inproceedings{Imagenet-old,
	author = {Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
	year = 2009,
	title = {Imagenet: A large-scale hierarchical image database},
	booktitle = {2009 IEEE conference on computer vision and pattern recognition},
	pages = {248--255},
	organization = {IEEE}
}
@article{ILSVRC15,
	author = {Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
	year = 2015,
	title = {{ImageNet Large Scale Visual Recognition Challenge}},
	journal = {International Journal of Computer Vision (IJCV)},
	volume = 115,
	number = 3,
	pages = {211--252},
	doi = {10.1007/s11263-015-0816-y}
}
@article{shamir2021,
	author = {Shamir, Adi},
	year = 2021,
	title = {A new theory of adversarial examples in machine learning (a non-technical extended abstract)},
	journal = {preprint}
}
@inproceedings{gilmer2018adversarial,
	author = {Justin Gilmer and Luke Metz and Fartash Faghri and Samuel S. Schoenholz and Maithra Raghu and Martin Wattenberg and Ian J. Goodfellow},
	title = {Adversarial Spheres},
	booktitle = {6th International Conference on Learning Representations, ({ICLR} 2018), Vancouver, BC, Canada}
}
@inproceedings{Fawzi2018empirical,
	author = {Fawzi, Alhussein and Moosavi-Dezfooli, Seyed-Mohsen and Frossard, Pascal and Soatto, Stefano},
	year = 2018,
	title = {Empirical Study of the Topology and Geometry of Deep Networks},
	month = {June},
	booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}
}
@incollection{ledoux1999concentration,
	author = {Ledoux, Michel},
	year = 1999,
	title = {Concentration of measure and logarithmic Sobolev inequalities},
	booktitle = {Seminaire de probabilites XXXIII},
	publisher = {Springer},
	pages = {120--216}
}
@incollection{ledoux1996isoperimetry,
	author = {Ledoux, Michel},
	year = 1996,
	title = {Isoperimetry and Gaussian analysis},
	booktitle = {Lectures on probability theory and statistics},
	publisher = {Springer},
	pages = {165--294}
}
@article{wegner2021lecture,
	author = {Wegner, Sven-Ake},
	year = 2021,
	title = {Lecture notes on high-dimensional spaces},
	journal = {arXiv preprint arXiv:2101.05841}
}
@book{blum_hopcroft_kannan_2020,
	author = {Blum, Avrim and Hopcroft, John and Kannan, Ravindran},
	year = 2020,
	title = {Foundations of Data Science},
	publisher = {Cambridge University Press},
	doi = {10.1017/9781108755528},
	place = {Cambridge}
}
@book{morvan,
	author = {Morvan, Jean-Marie},
	year = 2008,
	title = {Generalized Curvatures},
	publisher = {Springer Publishing Company, Incorporated},
	abstract = {The intent of this book is to set the modern foundations of the theory of generalized curvature measures. This subject has a long history, beginning with J. Steiner (1850), H. Weyl (1939), H. Federer (1959), P. Wintgen (1982), and continues today with young and brilliant mathematicians. In the last decades, a renewal of interest in mathematics as well as computer science has arisen (finding new applications in computer graphics, medical imaging, computational geometry, visualization ). Following a historical and didactic approach, the book introduces the mathematical background of the subject, beginning with curves and surfaces, going on with convex subsets, smooth submanifolds, subsets of positive reach, polyhedra and triangulations, and ending with surface reconstruction. We focus on the theory of normal cycle, which allows to compute and approximate curvature measures of a large class of smooth or discrete objects of the Euclidean space. We give explicit computations when the object is a 2 or 3 dimensional polyhedron. This book can serve as a textbook to any mathematician or computer scientist, engineer or researcher who is interested in the theory of curvature measures.}
}
@conference{crecchi2019,
	author = {Francesco Crecchi and Davide Bacciu and Battista Biggio},
	year = 2019,
	title = {Detecting Adversarial Examples through Nonlinear Dimensionality Reduction},
	booktitle = {27th European Symposium on Artificial Neural Networks, Computational Intelligence and Machine Learning - ESANN {\textquoteright}19},
	pages = {483--488}
}
@inproceedings{alexnet,
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	year = 2012,
	title = {ImageNet Classification with Deep Convolutional Neural Networks},
	booktitle = {Advances in Neural Information Processing Systems},
	volume = 25,
	editor = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger}
}
@inproceedings{dongMIFGSM,
	author = {Yinpeng Dong and Fangzhou Liao and Tianyu Pang and Hang Su and Jun Zhu and Xiaolin Hu and Jianguo Li},
	title = {Boosting Adversarial Attacks With Momentum},
	booktitle = {2018 {IEEE} Conference on Computer Vision and Pattern Recognition, ({CVPR}) 2018, Salt Lake City, UT, USA, June 18-22, 2018},
	pages = {9185--9193}
}
@article{shamir2021dimpled,
	author = {Shamir, Adi and Melamed, Odelia and BenShmuel, Oriel},
	year = 2021,
	title = {The dimpled manifold model of adversarial examples in machine learning},
	journal = {arXiv preprint arXiv:2106.10151}
}
@inproceedings{stutz2019disentangling,
	author = {Stutz, David and Hein, Matthias and Schiele, Bernt},
	year = 2019,
	title = {Disentangling adversarial robustness and generalization},
	booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
	pages = {6976--6987}
}
@article{khoury2018geometry,
	author = {Khoury, Marc and Hadfield-Menell, Dylan},
	year = 2018,
	title = {On the geometry of adversarial examples},
	journal = {arXiv preprint arXiv:1811.00525}
}
@article{ganz2022perceptually,
	author = {Ganz, Roy and Kawar, Bahjat and Elad, Michael},
	year = 2022,
	title = {Do Perceptually Aligned Gradients Imply Adversarial Robustness?},
	journal = {arXiv preprint arXiv:2207.11378}
}
@article{baehrens2010explain,
	author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\"u}ller, Klaus-Robert},
	year = 2010,
	title = {How to explain individual classification decisions},
	journal = {The Journal of Machine Learning Research},
	publisher = {JMLR. org},
	volume = 11,
	pages = {1803--1831}
}
@article{adebayo2018sanity,
	author = {Adebayo, Julius and Gilmer, Justin and Muelly, Michael and Goodfellow, Ian and Hardt, Moritz and Kim, Been},
	year = 2018,
	title = {Sanity checks for saliency maps},
	journal = {Advances in neural information processing systems},
	volume = 31
}
@incollection{kindermans2019reliability,
	author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"u}tt, Kristof T and D{\"a}hne, Sven and Erhan, Dumitru and Kim, Been},
	year = 2019,
	title = {The (un) reliability of saliency methods},
	booktitle = {Explainable AI: Interpreting, Explaining and Visualizing Deep Learning},
	publisher = {Springer},
	pages = {267--280}
}
@article{tjeng2017evaluating,
	author = {Tjeng, Vincent and Xiao, Kai and Tedrake, Russ},
	year = 2017,
	title = {Evaluating robustness of neural networks with mixed integer programming},
	journal = {arXiv preprint arXiv:1711.07356}
}
@article{singh2018fast,
	author = {Singh, Gagandeep and Gehr, Timon and Mirman, Matthew and P{\"u}schel, Markus and Vechev, Martin},
	year = 2018,
	title = {Fast and effective robustness certification},
	journal = {Advances in neural information processing systems},
	volume = 31
}
@inproceedings{xie2017aggregated,
	author = {Xie, Saining and Girshick, Ross and Doll{\'a}r, Piotr and Tu, Zhuowen and He, Kaiming},
	year = 2017,
	title = {Aggregated residual transformations for deep neural networks},
	booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
	pages = {1492--1500}
}
@article{akhtar2018threat,
	author = {Akhtar, Naveed and Mian, Ajmal},
	year = 2018,
	title = {Threat of adversarial attacks on deep learning in computer vision: A survey},
	journal = {Ieee Access},
	publisher = {IEEE},
	volume = 6,
	pages = {14410--14430}
}
@article{tramer2019adversarial,
	author = {Tramer, Florian and Boneh, Dan},
	year = 2019,
	title = {Adversarial training and robustness for multiple perturbations},
	journal = {Advances in Neural Information Processing Systems},
	volume = 32
}
@article{jo2017measuring,
	author = {Jo, Jason and Bengio, Yoshua},
	year = 2017,
	title = {Measuring the tendency of cnns to learn surface statistical regularities},
	journal = {arXiv preprint arXiv:1711.11561}
}
@article{geirhos2018imagenet,
	author = {Geirhos, Robert and Rubisch, Patricia and Michaelis, Claudio and Bethge, Matthias and Wichmann, Felix A and Brendel, Wieland},
	year = 2018,
	title = {ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness},
	journal = {arXiv preprint arXiv:1811.12231}
}
@article{kaur2019perceptually,
	author = {Kaur, Simran and Cohen, Jeremy and Lipton, Zachary C},
	year = 2019,
	title = {Are perceptually-aligned gradients a general property of robust classifiers?},
	journal = {arXiv preprint arXiv:1910.08640}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
@misc{vardi2022gradient,
	author = {Gal Vardi and Gilad Yehudai and Ohad Shamir},
	year = 2022,
	title = {Gradient Methods Provably Converge to Non-Robust Networks},
	eprint = {2202.04347},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@inproceedings{lecuyer2019certified,
	author = {Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
	year = 2019,
	title = {Certified robustness to adversarial examples with differential privacy},
	booktitle = {2019 IEEE Symposium on Security and Privacy (SP)},
	pages = {656--672},
	organization = {IEEE}
}
@article{li2019certified,
	author = {Li, Bai and Chen, Changyou and Wang, Wenlin and Carin, Lawrence},
	year = 2019,
	title = {Certified adversarial robustness with additive noise},
	journal = {Advances in neural information processing systems},
	volume = 32
}
@article{blum2020random,
	author = {Blum, Avrim and Dick, Travis and Manoj, Naren and Zhang, Hongyang},
	year = 2020,
	title = {Random smoothing might be unable to certify $L^{\infty}$ robustness for high-dimensional images},
	journal = {The Journal of Machine Learning Research},
	publisher = {JMLRORG},
	volume = 21,
	number = 1,
	pages = {8726--8746}
}
@inproceedings{kumar2020curse,
	author = {Kumar, Aounon and Levine, Alexander and Goldstein, Tom and Feizi, Soheil},
	year = 2020,
	title = {Curse of dimensionality on randomized smoothing for certifiable robustness},
	booktitle = {International Conference on Machine Learning},
	pages = {5458--5467},
	organization = {PMLR}
}
@inproceedings{yang2020randomized,
	author = {Yang, Greg and Duan, Tony and Hu, J Edward and Salman, Hadi and Razenshteyn, Ilya and Li, Jerry},
	year = 2020,
	title = {Randomized smoothing of all shapes and sizes},
	booktitle = {International Conference on Machine Learning},
	pages = {10693--10705},
	organization = {PMLR}
}
@inproceedings{cohen2019certified,
	author = {Cohen, Jeremy and Rosenfeld, Elan and Kolter, Zico},
	year = 2019,
	title = {Certified adversarial robustness via randomized smoothing},
	booktitle = {international conference on machine learning},
	pages = {1310--1320},
	organization = {PMLR}
}
@misc{blau2023classifier,
	author = {Tsachi Blau and Roy Ganz and Chaim Baskin and Michael Elad and Alex Bronstein},
	year = 2023,
	title = {Classifier Robustness Enhancement Via Test-Time Transformation},
	eprint = {2303.15409},
	archiveprefix = {arXiv},
	primaryclass = {cs.CV}
}
@inproceedings{nguyen-minh-luu-2022-textual,
	author = {Nguyen Minh, Dang  and Luu, Anh Tuan},
	year = 2022,
	title = {Textual Manifold-based Defense Against Natural Language Adversarial Examples},
	month = dec,
	booktitle = {Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing},
	publisher = {Association for Computational Linguistics},
	address = {Abu Dhabi, United Arab Emirates},
	pages = {6612--6625},
	url = {https://aclanthology.org/2022.emnlp-main.443},
	abstract = {Despite the recent success of large pretrained language models in NLP, they are susceptible to adversarial examples. Concurrently, several studies on adversarial images have observed an intriguing property: the adversarial images tend to leave the low-dimensional natural data manifold. In this study, we find a similar phenomenon occurs in the contextualized embedding space of natural sentences induced by pretrained language models in which textual adversarial examples tend to have their embeddings diverge off the manifold of natural sentence embeddings. Based on this finding, we propose Textual Manifold-based Defense (TMD), a defense mechanism that learns the embedding space manifold of the underlying language model and projects novel inputs back to the approximated structure before classification. Through extensive experiments, we find that our method consistently and significantly outperforms previous defenses under various attack settings while remaining unaffected to the clean accuracy. To the best of our knowledge, this is the first kind of manifold-based defense adapted to the NLP domain.}
}
@inproceedings{Osada_2023_WACV,
	author = {Osada, Genki and Takahashi, Tsubasa and Ahsan, Budrul and Nishide, Takashi},
	year = 2023,
	title = {Out-of-Distribution Detection With Reconstruction Error and Typicality-Based Penalty},
	month = {January},
	booktitle = {Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
	pages = {5551--5563}
}
@misc{melamed2023adversarial,
	author = {Odelia Melamed and Gilad Yehudai and Gal Vardi},
	year = 2023,
	title = {Adversarial Examples Exist in Two-Layer ReLU Networks for Low Dimensional Data Manifolds},
	eprint = {2303.00783},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@misc{magai2022topology,
	author = {German Magai and Anton Ayzenberg},
	year = 2022,
	title = {Topology and geometry of data manifold in deep learning},
	eprint = {2204.08624},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@techreport{aparne2022pca,
	author = {Aparne, Gupta and Banburski, Andrzej and Poggio, Tomaso},
	year = 2022,
	title = {PCA as a defense against some adversaries},
	institution = {Center for Brains, Minds and Machines (CBMM)}
}
@inproceedings{lu2022randommasking,
	author = {Lu, Zhiping and Hu, Hongchao and Huo, Shumin and Li, Shuyi},
	year = 2022,
	title = {MR2D: Multiple Random Masking Reconstruction Adversarial Detector},
	booktitle = {2022 10th International Conference on Information Systems and Computing Technology (ISCTech)},
	volume = {},
	number = {},
	pages = {61--67},
	doi = {10.1109/ISCTech58360.2022.00016}
}
@article{jin2022roby,
	author = {Haibo Jin and Jinyin Chen and Haibin Zheng and Zhen Wang and Jun Xiao and Shanqing Yu and Zhaoyan Ming},
	year = 2022,
	title = {ROBY: Evaluating the adversarial robustness of a deep model by its decision boundaries},
	journal = {Information Sciences},
	volume = 587,
	pages = {97--122},
	doi = {https://doi.org/10.1016/j.ins.2021.12.021},
	issn = {0020-0255},
	url = {https://www.sciencedirect.com/science/article/pii/S0020025521012421},
	keywords = {Robustness evaluation, Deep learning, Deep neural network, Decision boundaries},
	abstract = {With the successful applications of DNNs in many real-world tasks, model’s robustness has raised public concern. Recently the robustness of deep models is often evaluated by purposely generated adversarial samples, which is time-consuming and usually dependent on the specific attacks and model structures. Addressing the problem, we propose a generic evaluation metric ROBY, a novel attack-independent robustness measurement based on the model’s feature distribution. Without prior knowledge of adversarial samples, ROBY uses inter-class and intra-class statistics to capture the features in the latent space. Models with stronger robustness always have larger distances between classes and smaller distances in the same class. Comprehensive experiments have been conducted on ten state-of-the-art deep models and different datasets to verify ROBY’s effectiveness and efficiency. Compared with other evaluation metrics, ROBY better matches the robustness golden standard attack success rate (ASR), with significantly less computation cost. To the best of our knowledge, ROBY is the first light-weighted attack-independent robustness evaluation metric general to a wide range of deep models. The code of it can be downloaded at https://github.com/Allen-piexl/ROBY.}
}
@inproceedings{carmon2019unlabeled,
	author = {Carmon, Yair and Raghunathan, Aditi and Schmidt, Ludwig and Duchi, John C and Liang, Percy S},
	year = 2019,
	title = {Unlabeled Data Improves Adversarial Robustness},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 32,
	pages = {},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/32e0bd1497aa43e02a42f47d9d6515ad-Paper.pdf},
	editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett}
}
@inproceedings{taori2020shifts,
	author = {Taori, Rohan and Dave, Achal and Shankar, Vaishaal and Carlini, Nicholas and Recht, Benjamin and Schmidt, Ludwig},
	year = 2020,
	title = {Measuring Robustness to Natural Distribution Shifts in Image Classification},
	booktitle = {Advances in Neural Information Processing Systems},
	publisher = {Curran Associates, Inc.},
	volume = 33,
	pages = {18583--18599},
	url = {https://proceedings.neurips.cc/paper_files/paper/2020/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf},
	editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin}
}
@inproceedings{Wang2020Improving,
	author = {Yisen Wang and Difan Zou and Jinfeng Yi and James Bailey and Xingjun Ma and Quanquan Gu},
	year = 2020,
	title = {Improving Adversarial Robustness Requires Revisiting Misclassified Examples},
	booktitle = {International Conference on Learning Representations},
	url = {https://openreview.net/forum?id=rklOg6EFwS}
}
@inproceedings{he2018decision,
	author = {Warren He and Bo Li and Dawn Song},
	year = 2018,
	title = {Decision Boundary Analysis of Adversarial Examples},
	booktitle = {International Conference on Learning Representations},
	url = {https://openreview.net/forum?id=BkpiPMbA-}
}
@misc{xu2023exploring,
	author = {Yuancheng Xu and Yanchao Sun and Micah Goldblum and Tom Goldstein and Furong Huang},
	year = 2023,
	title = {Exploring and Exploiting Decision Boundary Dynamics for Adversarial Robustness},
	eprint = {2302.03015},
	archiveprefix = {arXiv},
	primaryclass = {cs.LG}
}
@article{chen2023aware,
	author = {Chen, Chen and Zhang, Jingfeng and Xu, Xilie and Lyu, Lingjuan and Chen, Chaochao and Hu, Tianlei and Chen, Gang},
	year = 2023,
	title = {Decision Boundary-Aware Data Augmentation for Adversarial Training},
	journal = {IEEE Transactions on Dependable and Secure Computing},
	volume = 20,
	number = 3,
	pages = {1882--1894},
	doi = {10.1109/TDSC.2022.3165889}
}
