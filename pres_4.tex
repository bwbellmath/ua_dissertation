\section{An Exact Kernel Equivalence for Finite Classification Models} % Main chapter title
\label{Chapter4} % For referencing the chapter elsewhere, use \ref{Chapter1} 

% This paper proposes the first exact path kernel representation for
% general gradient trained classifiers. The primary derivation and proof
% was written by Brian Bell and the supporting impoementation and work
% were mostly conducted by Michael Geyer. The central focus of the paper
% is on the derivation and demonstration that this method works in
% practice. The interest that gave rise for this approach comes from the fact that
% kernel methods and more specifically bilinear map based models
% decompose their predictions into a contribution from each of their
% training data. the Neural Tangent Kernel (NTK) is an interesting tool,
% but predicated on too many approximation assumptions. This exact
% formulation allows a much more solid foundation for analyzing neural
% networks and suggests the possibility that predictions can be
% decomposed using this framework. The implementation and application of
% this method to real machine-learning models and tasks demonstrates
% that it is not only a theoretical framework, it is practical! This
% paper was accepted at the Topology Algebra and Geometry (TAG) workshop
% at ICML 2023 (an archival publication).  

% \section{Introduction}

% This study investigates the relationship between kernel methods and finite parametric models. To date, interpreting the predictions of complex models, like neural networks, has proven to be challenging. Prior work has shown that the inference-time predictions of a neural network can be exactly written as a sum of independent predictions computed with respect to each training point. We formally show that classification models trained with cross-entropy loss can be exactly formulated as a kernel machine. It is our hope that these new theoretical results will open new research directions in the interpretation of neural network behavior.




% % \begin{tikzpicture}[scale=0.8]
% % % main step angles
% % \def\angA{72}
% % \def\angB{35}
% % \def\angC{15}
% % \def\angD{-20}

% % \def\nang{62}
% % \def\dang{45}
% % \def\sang{15}
% % \def\pang{30}

% % % \def\mang{29}
% % % \def\sang{39}
% % % \def\tang{50}
% % % \def\bang{60}
% % % \def\dang{34}

% %   \def\xs{1.0}
% %   \def\xa{4}
% %   \def\xe{1.0}
% %   \def\xd{0.75}
% %   \def\xn{0.5}
% %   \def\xi{1.4}
% %   % incoming edge
% %   \coordinate  (s1) at (0,0);
% %   \coordinate  (si1) at ($ (s1) + (\angA:1) $);
% %   \coordinate  (s2) at ($ (si1) + (\angA:\xs) $);
% %   \coordinate  (s3) at ($ (s2) + (\angB:\xa) $);
% %   \coordinate  (si3) at ($(s3) + (\angC:\xs) $);
% %   \coordinate  (s4) at ($(si3) + (\angC:\xs) $);

% %   %\node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
% %   \path (si1) -- (s2) node [midway, sloped] (elip) {\ldots};
% %   \path (s3) -- (si3) node [midway, sloped] (elip) {\ldots};
% %   \node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
% %   \node[fill=black,circle,inner sep=1.9] (s02) at (s2) {};
% %   \node[fill=black,circle,inner sep=1.9] (s03) at (s3) {};
% %   \node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};
% %   %\node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};

% % \draw[name path=pb, black, line width=0.4mm, draw opacity=0.7] (s2) -- (s3);
% % \draw[black, line width=0.4mm, draw opacity=0.7] (s1) -- (si1);
% % \path[name path=fb, black, line width=0.4mm, draw opacity=0.7] (s1) -- (s2);
% % \draw[name path=gb, black, line width=0.4mm, draw opacity=0.7] (si3) -- (s4);

% % \coordinate (p1) at (s2);%($ (s2)+(\ang:\xi) $);   % origin
% % % \coordinate (p1n) at ($ (p1)+(\nang:\xe) $);       % ntk test

% % % \coordinate (p1d) at ($ (p1)+(\angB+\sang:\xe) $); % dpk test

% % % \coordinate (p1e) at ($ (p1)+(\angB+\sang:\xe) $); % epk test
% % % \coordinate (p1t) at ($ (p1)+(\angD:\xe) $); % ntk train
% % % \coordinate (p1b) at ($ (p1)+(\angB:\xe) $); % dpk epk train

% % % \coordinate (p2) at  ($ (p1b)+(\angB:\xi) $);   % origin
% % % \coordinate (p2n) at ($ (p2)+(\nang:\xe) $);       % ntk test
% % % \coordinate (p2d) at ($ (p2)+(\angB+\sang:\xe) $); % dpk test
% % % \coordinate (p2e) at ($ (p2)+(\angB+\pang:\xe) $); % epk test
% % % \coordinate (p2t) at ($ (p2)+(\angD:\xe) $); % ntk train
% % % \coordinate (p2b) at ($ (p2)+(\angB:\xe) $); % dpk epk train

% % % \coordinate (p3) at  (s4); %($ (s4)+(\angB:\xi) $);   % origin
% % %   \draw[line width=10pt, ->, red] (0,0) -- (3,0);
% % %   \draw[line width=10pt, ->, blue] (0,0.5) -- (3,0.5);
  
% % %     \begin{scope}
% % %     \clip (-1,0) rectangle (3,1);
% % %     \draw[line width=10pt, ->] (0,0) -- (3,0);
% % %   \end{scope}
% % % \coordinate (p3n) at ($ (p3)+(\nang:\xe) $);       % ntk test
% % % \coordinate (p3d) at ($ (p3)+(\nang:\xe) $); % dpk test
% % % \coordinate (p3e) at ($ (p3)+(\nang:\xe) $); % epk test
% % % \coordinate (p3t) at ($ (p3)+(\angD:\xe) $); % ntk train
% % % \coordinate (p3b) at ($ (p3)+(\angD:\xe) $); % dpk epk train

% % % \coordinate (p2) at ($ (p1a)+(\angB:0.6) $);
% % % \coordinate (p2a) at ($ (p2)+(\angB:\xe) $);
% % % \coordinate (p2b) at ($ (p2)+(\sang:\xe) $); 
% % % \coordinate (p3) at ($ (p2a)+(\angB:\xi) $);
% % % \coordinate (p3a) at ($ (p3)+(\angB:\xe) $);
% % % \coordinate (p3b) at ($ (p3)+(\tang:\xe) $);
% % % \coordinate (n0) at ($ (s1)+(\nang:\xn) $);
% % % \coordinate (n1) at ($ (p1)+(\nang:\xn) $);
% % % \coordinate (n2) at ($ (p2)+(\nang:\xn) $);
% % % \coordinate (n3) at ($ (p3)+(\nang:\xn) $);
% % % \coordinate (d0) at ($ (s1)+(\dang:\xd) $);
% % % \coordinate (d1) at ($ (p1)+(\dang:\xd) $);
% % % \coordinate (d2) at ($ (p2)+(\dang:\xd) $);
% % % \coordinate (d3) at ($ (p3)+(\dang:\xd) $);
    
% % % \draw[vector, ->, ntk] (s1) -- (n0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % % \draw[vector, ->, dpk] (s1) -- (d0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % % \draw[vector, opacity=0.5, ->, ntk] (p1) -- (p1n) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk test};

% % % \draw[vector, opacity=0.5,  ->, dpk] (p1) -- (p1d) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%dpk test};

% % % \begin{scope}
% % % \clip (p1) -- (p1d) -- ($(p1d) + (0,1) $) -- ($ (p1) + (-1,1) $);
% % % \draw[vector, ->, epk] (p1) -- (p1e) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%epk test};
% % % \end{scope}
% % % \draw[vector, ->, ntk] (p1) -- (p1t) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk train};
% % % \draw[vector, ->, epk] (p1) -- (p1b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%(de)pk test};

% % % \draw[vector, ->, ntk] (p2) -- (p2n) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk test};
% % % \draw[vector, ->, dpk] (p2) -- (p2d) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%dpk test};
% % % \draw[vector, ->, epk] (p2) -- (p2e) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%epk test};
% % % \draw[vector, ->, ntk] (p2) -- (p2t) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk train};
% % % \draw[vector, ->, epk] (p2) -- (p2b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%(de)pk test};



% % % \draw[vector, ->, ntk] (p3) -- (p3n) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk test};
% % % \draw[vector, ->, dpk] (p3) -- (p3d) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%dpk test};
% % % \draw[vector, ->, epk] (p3) -- (p3e) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%epk test};
% % % \draw[vector, ->, ntk] (p3) -- (p3t) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%ntk train};
% % % \draw[vector, ->, epk] (p3) -- (p3b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.2] {};%(de)pk test};

% % \draw [name path=pa, thick , ->]
% %   (p1) .. controls ($ (s2) + (37:2.1) $)  .. ($ (s3) + (125:0.6) $);
% %   \draw [name path=fa, thick , ->]
% %   (s1) .. controls ($ (s1) + (80:1.1) $)  .. ($ (s2) + (162:0.50) $);
% %   \draw [name path=ga, thick , ->]
% %   (s3) .. controls ($ (s3) + (20:1) $)  .. ($ (s4) + (105:0.4) $);
% % \tikzfillbetween[of=pa and pb]{orange, opacity=0.2};
% % \tikzfillbetween[of=fa and fb]{orange, opacity=0.2}; 
% % \tikzfillbetween[of=ga and gb]{orange, opacity=0.2};

% % % \draw[vector, ->, epk] (p1) -- (p1b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % % \draw[vector, ->, epk] (p1) -- (p1a) node[scale=1,below left=1mm and 0mm, draw opacity=0.5] {$\vu{y}$};
% % % \draw[vector, ->, epk] (p2) -- (p2b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % % \draw[vector, ->, epk] (p2) -- (p2a) node[scale=1,below left=1mm and 0mm, draw opacity=0.5] {$\vu{y}$};
% % % \draw[vector, ->, epk] (p3) -- (p3b) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% % % \draw[vector, ->, epk] (p3) -- (p3a) node[scale=1,below left=1mm and 0mm, draw opacity=0.5] {$\vu{y}$};

% % \end{tikzpicture}

\begin{frame}
\frametitle{Model Optimization Step}  
\begin{figure}[!ht]
\centering
\begin{tikzpicture}[scale=1.2]
\def\bang{72}
\def\ang{35}
\def\lang{15}
\def\mang{57}
\def\sang{69}
\def\kang{90}
\def\tang{87}
\def\dang{34}
\def\xs{1.0}
\def\xa{4}
\def\xe{1.0}
\def\xd{0.75}
\def\xn{0.5}
\def\xi{0.4}
% incoming edge
\coordinate  (s1) at (0,0);
\coordinate  (si1) at ($ (s1) + (\bang:1) $);
\coordinate  (s2) at ($ (si1) + (\bang:\xs) $);
\coordinate  (s3) at ($ (s2) + (\ang:\xa) $);
\coordinate  (si3) at ($(s3) + (\lang:\xs) $);
\coordinate  (s4) at ($(si3) + (\lang:\xs) $);

%\node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
\path (si1) -- (s2) node [midway, sloped] (elip) {\ldots};
\path (s3) -- (si3) node [midway, sloped] (elip) {\ldots};
\node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
\node [below right=0mm and 1mm, rotate=\kang-90] (ss1) at (s01) {$w_1(t=0)$};
\node[fill=black,circle,inner sep=1.9] (s02) at (s2) {};
\node [below right=0mm and 1mm, rotate=\kang-90] (ss2) at (s02) {$w_s(t=0)$};
\node[fill=black,circle,inner sep=1.9] (s03) at (s3) {};
\node [below right=-1mm and 1mm, rotate=\kang-90] (ss3) at (s3) {$w_s(t=1)=w_{s+1}(t=0)$}; 
\node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};
%\node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};
\node [below right=-2mm and 1mm, rotate=\kang-90] (ss4) at (s04) {$w_S(t=0)$};
\draw[black, line width=0.4mm, draw opacity=0.3] (s2) -- (s3);
\draw[black, line width=0.4mm, draw opacity=0.3] (s1) -- (si1);
\draw[black, line width=0.4mm, draw opacity=0.3] (si3) -- (s4);

\coordinate (p1) at (s2);%($ (s2)+(\ang:\xi) $);
\coordinate (p1a) at ($ (p1)+(\ang:\xe) $);
\coordinate (p1b) at ($ (p1)+(\mang:\xe) $);
\coordinate (p2) at ($ (p1a)+(\ang:0.6) $);
\coordinate (p2a) at ($ (p2)+(\ang:\xe) $);
\coordinate (p2b) at ($ (p2)+(\sang:\xe) $); 
\coordinate (p2c) at ($ (p2)+(\mang:\xe) $); 
\coordinate (p3) at ($ (p2a)+(\ang:\xi) $);
\coordinate (p3a) at ($ (p3)+(\ang:\xe) $);
\coordinate (p3b) at ($ (p3)+(\tang:\xe) $);
\coordinate (p3c) at ($ (p3)+(\mang:\xe) $);
\coordinate (n0) at ($ (s1)+(\bang:\xn) $);
\coordinate (n1) at ($ (p1)+(\bang:\xn) $);
\coordinate (n2) at ($ (p2)+(\bang:\xn) $);
\coordinate (n3) at ($ (p3)+(\bang:\xn) $);
\coordinate (d0) at ($ (s1)+(\dang:\xd) $);
\coordinate (d1) at ($ (p1)+(\dang:\xd) $);
\coordinate (d2) at ($ (p2)+(\dang:\xd) $);
\coordinate (d3) at ($ (p3)+(\dang:\xd) $);

\draw[name path=pb, black, line width=0.4mm, draw opacity=0.7] (s2) -- (s3);
\draw[black, line width=0.4mm, draw opacity=0.7] (s1) -- (si1);
\path[name path=fb, black, line width=0.4mm, draw opacity=0.7] (s1) -- (s2);
\draw[name path=gb, black, line width=0.4mm, draw opacity=0.7] (si3) -- (s4);  
  \draw [name path=pa, thick , ->, opacity=0.0]
  (p1) .. controls ($ (s2) + (37:2.1) $)  .. ($ (s3) + (125:0.6) $);
  \draw [name path=fa, thick , ->, opacity=0.0]
  (s1) .. controls ($ (s1) + (80:1.1) $)  .. ($ (s2) + (162:0.50) $);
  \draw [name path=ga, thick , ->, opacity=0.0]
  (s3) .. controls ($ (s3) + (20:1) $)  .. ($ (s4) + (105:0.4) $);
\tikzfillbetween[of=pa and pb]{orange, opacity=0.2};
\tikzfillbetween[of=fa and fb]{orange, opacity=0.2}; 
\tikzfillbetween[of=ga and gb]{orange, opacity=0.2};
    
% \draw[vector, ->, ntk] (s1) -- (n0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% \draw[vector, ->, dpk] (s1) -- (d0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
\draw[vector, ->, dpk] (p1) -- (p1b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0)}(x)$};
\begin{scope}
\clip (p1) -- (p1b) -- ($(p1b) + (90+\mang:0.1) $) -- ($ (p1) + (90+\mang:0.1) $);

\draw[vector, ->, epkt] (p1) -- (p1b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0)}(x)$};

\end{scope}
\draw[vector, ->, epk] (p1) -- (p1a) node[scale=1,below right=1mm and -2mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[vector, ->, epkt] (p2) -- (p2b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0.4)}(x)$};
\draw[vector, ->, dpk] (p2) -- (p2c) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {};
\draw[vector, ->, epk] (p2) -- (p2a) node[scale=1,below right=3mm and -4mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[vector, ->, epkt] (p3) -- (p3b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0.7)}(x)$};
\draw[vector, ->, dpk] (p3) -- (p3c) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {};
\draw[vector, ->, epk] (p3) -- (p3a) node[scale=1,below right=4mm and -6mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[->,line width=0.2mm, xcol, diffpk] (p2c) -- (p2b) node[scale=1,below right=1mm and -2mm, draw opacity=0.5] {};
\draw[->,line width=0.2mm, xcol, diffpk] (p3c) -- (p3b) node[scale=1,below right=1mm and -2mm, draw opacity=0.5] {};
  

  %\draw[vector,<->,unitcol]
  %  (v3) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v1) --
  %  (v2) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};
    
    %   \draw[vector,<->,unitcol]
    % (v6) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v4) --
    % (v5) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};
    %       \draw[vector,<->,unitcol]
    % (v9) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v7) --
    % (v8) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};

% active step
% % outgoing edge
%   \def\ul{0.52}
%   \def\R{2.6}
%   \def\ang{28}
%   \coordinate (O) at (0,0);
%   \coordinate (R) at (\ang:\R);
%   \coordinate (X) at ({\R*cos(\ang)},0);
%   \coordinate (Y) at (0,{\R*sin(\ang)});
%   \node[fill=black,circle,inner sep=1.9] (O') at (O) {};
%   \node[fill=black,circle,inner sep=1.9] (R') at (R) {};
%   \node[above right=-2] at (R') {$(x,y)$};
%   \draw[<->,line width=0.9] %very thick
%     ({1.2*\R*cos(\ang)},0) -- (O) -- (0,{1.3*\R*sin(\ang)});
%   \draw[projcol,dashed] (X) -- (R);
%   \draw[black, thick] (O') -- (R');
%   \draw[projcol,dashed] (Y) -- (R);
%   %\draw[vector] (O) -- (R') node[midway,left=5,above right=0] {$\vb{r}$};
%   \draw[vector,<->,unitcol]
%     (\ul,0) node[scale=1,left=2,below left=0] {$\vu{x}$} -- (O) --
%     (0,\ul) node[scale=1,below=2,below left=0] {$\vu{y}$};
%   \draw pic[->,thick,"$\theta$",draw=black,angle radius=26,angle eccentricity=1.3]
%     {angle = X--O--R};
%   \draw[thick] (X)++(0,0.1) --++ (0,-0.2) node[scale=0.9,below=-1] {$x = r\cos\theta$};
%   \draw[thick] (Y)++(0.1,0) --++ (-0.2,0) node[scale=0.9,left] {$y = r\sin\theta$};
\end{tikzpicture}
\caption{Comparison of test gradients used by Discrete Path Kernel (DPK) from prior work (Blue) and the Exact Path Kernel (EPK) proposed in this work (green) versus total training vectors (black) used for kernel formulations along a discrete training path. }
\label{fig:vecs}
\end{figure}
\end{frame}

\begin{frame}
\frametitle{Kernel Approximation Error}

% \begin{figure}[!ht]
% % \begin{tikzpicture}
% % \node
% % \end{tikzpicture}
% \end{figure}
\begin{figure}[!ht]
        \centering
        % \begin{minipage}{0.5\textwidth}
        \includegraphics[width=0.8\linewidth]{c4_figures/estimation_comparison.pdf}
        % \end{minipage}

        \caption{Measurement of gradient alignment on test points across the training path. The EPK is used as a frame of reference. The y-axis is exactly the difference between the EPK and other representations.} %For example $EPK-DPK = \langle \phi_{s,t}(X), \phi_{s,t}(x) - \phi_{s,0}(x) \rangle$ (See Definition 3.4). Shaded regions indicate total accumulated error. Note: this is measuring an angle of error in weight space; therefore, equivalent positive and negative error will not result in zero error.}
        \label{fig:error}
\end{figure}
\end{frame}

% There has recently been a surge of interest in the connection between neural networks and kernel methods~\cite{bietti2019bias, du2019graphntk, tancik2020fourierfeatures, abdar2021uq, geifman2020similarity, chen2020generalized, alemohammad2021recurrent}. Much of this work has been motivated by the the neural tangent kernel (NTK), which describes the training dynamics of neural networks in the infinite limit of network width~\cite{jacot2018neural}.
% We argue that many intriguing behaviors arise in the \emph{finite} parameter regime~\cite{DBLP:conf/nips/BubeckS21}. 
% All prior works, to the best of our knowledge, appeal to discrete approximations of the kernel corresponding to a neural network. 
% Specifically, prior approaches are derived under the assumption that training step size is small enough to guarantee close approximation of a gradient flow
% %. These approximations have been speculated to open up parametric models trained with gradient descent, including artificial neural networks (ANNs), to many theoretical tools available to kernel methods
% ~\cite{ghojogh2021, shawe2004kernel, zhao2005extracting}.

% In this work, we show that the simplifying assumptions used in prior works (i.e. infinite network width and infinitesimal gradient descent steps) are not necessary. Our \textbf{Exact Path Kernel (EPK)} provides the first, exact method to study the behavior of finite-sized neural networks used for classification.
% Previous results are limited in application ~\cite{incudini2022quantum} due to dependence of the kernel on test data unless strong conditions are imposed on the training process as by ~\cite{chen2021equivalence}. We show, however, that the training step sizes used in practice do not closely follow this gradient flow, introducing significant error into all prior approaches (Figure~\ref{fig:error}).
% %use of such tools is highly dependent on the assumption that ANN training is a faithful discrete approximation of the smooth path kernel at practical step sizes. The accuracy of this assumption depends on difficult measurements of convergence and error.

% Our experimental results build on prior studies attempting to evaluate empirical properties of the kernels corresponding to finite neural networks ~\cite{DBLP:conf/iclr/LeeBNSPS18, chen2021equivalence}. While the properties of infinite neural networks are fairly well understood~\cite{neal1996priors}, we find that the kernels learned by finite neural networks have non-intuitive properties that may explain the failures of modern neural networks on important tasks such as robust classification and calibration on out-of-distribution data.

  \begin{frame}
    \frametitle{Contributions}
 This paper makes the following significant theoretical and experimental contributions:
 \begin{enumerate}
     \item We prove that finite-sized neural networks trained with finite-sized gradient descent steps and cross-entropy loss can be exactly represented as kernel machines using the EPK. Our derivation incorporates a previously-proposed path kernel, but extends this method to account for practical training procedures~\cite{domingos2020every, chen2021equivalence}.
  
     \item We demonstrate that it is computationally tractable to estimate the kernel underlying a neural network classifier, including for small convolutional computer vision models.
     % \item We estimate a kernel corresponding to both a toy-sized neural network and a convolutional classifier on MNIST up to machine precision.
     \item We compute Gram matrices using the EPK and use them to illuminate prior theory of neural networks and their understanding of uncertainty. 
     \item We employ Gaussian processes to compute the covariance of a neural network's logits and show that this reiterates previously observed shortcomings of neural network generalization.
 \end{enumerate}
\end{frame}
% \section{Related Work}
% % Interpreting and understanding 

% \begin{frame}
%   \frametitle{Related Work : Neural Tangent Kernel}
% % % The neural tangent kernel (NTK) ~\cite{he2020bayesian} has received significant attention recently in light of a growing body of work relating parametric gradient models in infinite width with Gaussian processes (GPs) and also the NTK. 
%   \begin{itemize}
%   \item The neural tangent kernel (NTK) is rooted in the
%   concept that all information necessary to represent a parametric
%   model is stored in the Hilbert space occupied by the model's weight
%   gradients up to a constant factor. 
  
% \item This is very well supported in infinite width ~\citep{jacot2018neural}. 

%   \item It has been shown that neural networks are equivalent to
%     support vector machines, drawing a connection to maximum margin
%     classifiers ~\citep{chen2021equivalence, chizat2020maxmargin}.
%     \item  Shah et al. demonstrate that this maximum margin classifier exists in Wasserstien space; however, they also show that model gradients may not contain the required information to represent this ~\citep{shah2021input}.
% % % However, discrete approximation of an infinite-width kernel is very limiting.
%     \end{itemize}
%   \end{frame}
  
% This formulation derives a kernel for finite-parametric models however it relies on a continuous integration over a gradient flow, while real-world models are trained by discrete Forward Euler steps through a gradient field.
% The exploration of discrete approximations has been a key focus in addressing the challenges associated with neural networks. 
  \begin{frame}
    \frametitle{Related Work : Discrete Path Kernel}
    \begin{itemize}
      \item \citet{domingos2020} attempts to establish a correspondence between kernel machines and parametric models trained by gradient descent in the case of a continuous training path (i.e. the limit as gradient descent step size $\varepsilon \to 0$)
% One notable approach is the formulation of the continuous path
% kernel, which aims to derive a kernel for finite-parametric models
      \item We will refer to this continuously integrated path
        representation as the Discrete Path Kernel (DPK).
\item ~\citet{chen2021equivalence} refined this formulation to
  establish an equivalence, however this equivalence requires a
  restrictive assumptions. 
\item A major limitation of this formulation is its reliance on a
  continuous integration over a gradient flow, which differs from the
  discrete forward Euler steps employed in real-world model training.
  \item The unbounded error of this approximation limits the
    value of the continuous path kernel in practical scenarios ~\citep{incudini2022quantum}.
% Moreover, the formulation of the sample weights and bias term in the DPK depends on its test points. Chen et al. propose that this can be addressed, in part, by imposing restrictions on the loss function used for training, but did not entirely disentangle the kernel formulation from sample importance weights on training points 
\end{itemize}
\end{frame}
% We address the limitations of \citet{domingos2020} and \citet{chen2021equivalence} in Subsection %~\remref{rem5}
% ~\ref{subsec:disc}. By default, their approach produces a system which can be viewed as an ensemble of kernel machines, but without a single aggregated kernel which can be analyzed directly. ~\citet{chen2021equivalence} propose that the resulting sum over kernel machines can be formulated as a kernel machine so long as the sign of the gradient of the loss stays constant through training; however, we show that this is not necessarily a sufficient restriction. Instead, their formulation leads to one of several non-symmetric functions which can serve as a surrogate to replicate a given models behavior, but without retaining properties of a kernel.

% % I don't think we need this paragraph
% % Previous studies have explored the use of various discrete kernel approximations. Tancik et al. employed Fourier features to approximate the kernel function, resulting in improved performance on coordinate-based MLP networks ~\cite{tancik2020fourierfeatures}.
% % The broader justification for this thread of research surrounds the potential value of having a single kernel representation for a neural network.
% % Various discrete kernel approximations have been used with interesting affects in order to adapt neural networks to achieve desirable kernel properties including stationarity and uncertainty quantification ~\cite{wang2022pinns, tancik2020fourierfeatures}. 


% % Similarly, Wang et al. (2022) utilized discrete kernel approximations to adapt neural networks, specifically focusing on achieving desirable properties such as stationarity and uncertainty quantification~\cite{wang2022pinns}. These approaches highlight the versatility and potential of discrete kernel approximations in enhancing the performance and interpretability of neural networks.

% % ~\cite{shah2021input}

% % ~\cite{incudini2022quantum} % CERN (unpublished?) paper discussing discrete path kernel and quantum context. They address limitations of domingos and chen, bring up non-uniqueness, but do not provide any real examples or proofs. Limitations : no discrete qualifier, "effective path kernel" is unfounded and a little odd. 

% % Something sets us up for the limitations and justifies our discrete path kernel exact representation to dig into these. Also set up studying spatial properties with neural tangent kernels?

% % ~\cite{domingos2020} % poses path kernel, limitation: depends on X, approximate. 
% % ~\cite{chen2021equivalence} % refines path kernel and does a lot of discussion, mentions discrete path kernel, limitations : constant signed loss function, does not figure out precise discrete formulation
% % ~\cite{chizat2020maxmargin} % shows NNs form max margin classifiers wrt wasserstein
% % ~\cite{shah2021input} % talks about limitations of gradients to explain what's going on due to chizat max margin equivalence [4]

% \section{Theory}
\subsection{Theory}

\begin{frame}
  \frametitle{Theory}
  \textbf{Goal} : Show that artificial neural networks are equivalent
  to kernel machines. \\

\begin{definition}
A \emph{kernel} is a function of two variables which is symmetric and positive semi-definite. 
\end{definition}

\vspace{.4cm}

\textbf{Note:}  We define this equivalence in terms of the output of the parametric model $f_w(x)$ and our kernel method in the sense that they form identical maps from input to output. In the specific case of neural network classification models, we consider the mapping $f_w(x)$ to include all layers of the neural network up to and including the log-softmax activation function. 

\end{frame}

\begin{frame}
  \frametitle{Theory : Kernel Method Definition}
\begin{definition}
Given a Hilbert space $X$, a test point $x \in X$, and a training set $X_T = \{x_1,x_2,...x_n\} \subset X$ indexed by $I$, a \emph{Kernel Machine} is a model characterized by 
\begin{align}
    \text{K}(x) = b + \sum_{i\in I} a_i k(x,x_i)
\end{align}
where the $a_i \in \mathbb{R}$ do not depend on $x$, $b \in \mathbb{R}$ is a constant, and $k$ is a kernel. ~\cite{rasmussen2006gaussian}

By Mercer's theorem ~\cite{ghojogh2021} a kernel can be produced by composing an inner product on a Hilbert space with a mapping $\phi$ from the space of data into the chosen Hilbert space.
We use this property to construct a kernel machine of the following form.
\begin{align}
    \text{K}(x) = b + \sum_{i\in I} a_i \langle \phi(x), \phi(x_i) \rangle
\end{align}
\end{definition}
\end{frame}


%Where $\phi$ is a function mapping input data into the weight space via gradients. Our $\phi$ will additionally differentiate between test and training points to resolve a discontinuity that arises under discrete training. 

\begin{frame}
  \frametitle{Derivation}
% \subsection{Exact Path Kernels}

We first derive a kernel which is an exact representation of the change in model output over one training step, and then compose our final representation by summing along the finitely many steps.

\begin{figure}[!ht]
\centering
\begin{tikzpicture}[scale=0.8]
\def\bang{72}
\def\ang{35}
\def\lang{15}
\def\mang{57}
\def\sang{69}
\def\kang{90}
\def\tang{87}
\def\dang{34}
\def\xs{1.0}
\def\xa{4}
\def\xe{1.0}
\def\xd{0.75}
\def\xn{0.5}
\def\xi{0.4}
% incoming edge
\coordinate  (s1) at (0,0);
\coordinate  (si1) at ($ (s1) + (\bang:1) $);
\coordinate  (s2) at ($ (si1) + (\bang:\xs) $);
\coordinate  (s3) at ($ (s2) + (\ang:\xa) $);
\coordinate  (si3) at ($(s3) + (\lang:\xs) $);
\coordinate  (s4) at ($(si3) + (\lang:\xs) $);

%\node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
\path (si1) -- (s2) node [midway, sloped] (elip) {\ldots};
\path (s3) -- (si3) node [midway, sloped] (elip) {\ldots};
\node[fill=black,circle,inner sep=1.9] (s01) at (s1) {};
\node [below right=0mm and 1mm, rotate=\kang-90] (ss1) at (s01) {$w_1(t=0)$};
\node[fill=black,circle,inner sep=1.9] (s02) at (s2) {};
\node [below right=0mm and 1mm, rotate=\kang-90] (ss2) at (s02) {$w_s(t=0)$};
\node[fill=black,circle,inner sep=1.9] (s03) at (s3) {};
\node [below right=-1mm and 1mm, rotate=\kang-90] (ss3) at (s3) {$w_s(t=1)=w_{s+1}(t=0)$}; 
\node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};
%\node[fill=black,circle,inner sep=1.9] (s04) at (s4) {};
\node [below right=-2mm and 1mm, rotate=\kang-90] (ss4) at (s04) {$w_S(t=0)$};
\draw[black, line width=0.4mm, draw opacity=0.3] (s2) -- (s3);
\draw[black, line width=0.4mm, draw opacity=0.3] (s1) -- (si1);
\draw[black, line width=0.4mm, draw opacity=0.3] (si3) -- (s4);

\coordinate (p1) at (s2);%($ (s2)+(\ang:\xi) $);
\coordinate (p1a) at ($ (p1)+(\ang:\xe) $);
\coordinate (p1b) at ($ (p1)+(\mang:\xe) $);
\coordinate (p2) at ($ (p1a)+(\ang:0.6) $);
\coordinate (p2a) at ($ (p2)+(\ang:\xe) $);
\coordinate (p2b) at ($ (p2)+(\sang:\xe) $); 
\coordinate (p2c) at ($ (p2)+(\mang:\xe) $); 
\coordinate (p3) at ($ (p2a)+(\ang:\xi) $);
\coordinate (p3a) at ($ (p3)+(\ang:\xe) $);
\coordinate (p3b) at ($ (p3)+(\tang:\xe) $);
\coordinate (p3c) at ($ (p3)+(\mang:\xe) $);
\coordinate (n0) at ($ (s1)+(\bang:\xn) $);
\coordinate (n1) at ($ (p1)+(\bang:\xn) $);
\coordinate (n2) at ($ (p2)+(\bang:\xn) $);
\coordinate (n3) at ($ (p3)+(\bang:\xn) $);
\coordinate (d0) at ($ (s1)+(\dang:\xd) $);
\coordinate (d1) at ($ (p1)+(\dang:\xd) $);
\coordinate (d2) at ($ (p2)+(\dang:\xd) $);
\coordinate (d3) at ($ (p3)+(\dang:\xd) $);

\draw[name path=pb, black, line width=0.4mm, draw opacity=0.7] (s2) -- (s3);
\draw[black, line width=0.4mm, draw opacity=0.7] (s1) -- (si1);
\path[name path=fb, black, line width=0.4mm, draw opacity=0.7] (s1) -- (s2);
\draw[name path=gb, black, line width=0.4mm, draw opacity=0.7] (si3) -- (s4);  
  \draw [name path=pa, thick , ->, opacity=0.0]
  (p1) .. controls ($ (s2) + (37:2.1) $)  .. ($ (s3) + (125:0.6) $);
  \draw [name path=fa, thick , ->, opacity=0.0]
  (s1) .. controls ($ (s1) + (80:1.1) $)  .. ($ (s2) + (162:0.50) $);
  \draw [name path=ga, thick , ->, opacity=0.0]
  (s3) .. controls ($ (s3) + (20:1) $)  .. ($ (s4) + (105:0.4) $);
\tikzfillbetween[of=pa and pb]{orange, opacity=0.2};
\tikzfillbetween[of=fa and fb]{orange, opacity=0.2}; 
\tikzfillbetween[of=ga and gb]{orange, opacity=0.2};
    
% \draw[vector, ->, ntk] (s1) -- (n0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
% \draw[vector, ->, dpk] (s1) -- (d0) node[scale=1,above left=-3mm and 3mm, draw opacity=0.5] {$\vu{x}$};
\draw[vector, ->, dpk] (p1) -- (p1b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0)}(x)$};
\begin{scope}
\clip (p1) -- (p1b) -- ($(p1b) + (90+\mang:0.1) $) -- ($ (p1) + (90+\mang:0.1) $);

\draw[vector, ->, epkt] (p1) -- (p1b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0)}(x)$};

\end{scope}
\draw[vector, ->, epk] (p1) -- (p1a) node[scale=1,below right=1mm and -2mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[vector, ->, epkt] (p2) -- (p2b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0.4)}(x)$};
\draw[vector, ->, dpk] (p2) -- (p2c) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {};
\draw[vector, ->, epk] (p2) -- (p2a) node[scale=1,below right=3mm and -4mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[vector, ->, epkt] (p3) -- (p3b) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {$\nabla f_{w_s(t=0.7)}(x)$};
\draw[vector, ->, dpk] (p3) -- (p3c) node[scale=1,above left=-5mm and 2mm, draw opacity=0.5] {};
\draw[vector, ->, epk] (p3) -- (p3a) node[scale=1,below right=4mm and -6mm, draw opacity=0.5, rotate=\kang-90] {$\nabla f_{w_s(t=0)}(X)$};
\draw[->,line width=0.2mm, xcol, diffpk] (p2c) -- (p2b) node[scale=1,below right=1mm and -2mm, draw opacity=0.5] {};
\draw[->,line width=0.2mm, xcol, diffpk] (p3c) -- (p3b) node[scale=1,below right=1mm and -2mm, draw opacity=0.5] {};
  

  %\draw[vector,<->,unitcol]
  %  (v3) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v1) --
  %  (v2) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};
    
    %   \draw[vector,<->,unitcol]
    % (v6) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v4) --
    % (v5) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};
    %       \draw[vector,<->,unitcol]
    % (v9) node[scale=1,above left=-3mm and 3mm] {$\vu{x}$} -- (v7) --
    % (v8) node[scale=1,below=2,below left=1mm and 0mm] {$\vu{y}$};

% active step
% % outgoing edge
%   \def\ul{0.52}
%   \def\R{2.6}
%   \def\ang{28}
%   \coordinate (O) at (0,0);
%   \coordinate (R) at (\ang:\R);
%   \coordinate (X) at ({\R*cos(\ang)},0);
%   \coordinate (Y) at (0,{\R*sin(\ang)});
%   \node[fill=black,circle,inner sep=1.9] (O') at (O) {};
%   \node[fill=black,circle,inner sep=1.9] (R') at (R) {};
%   \node[above right=-2] at (R') {$(x,y)$};
%   \draw[<->,line width=0.9] %very thick
%     ({1.2*\R*cos(\ang)},0) -- (O) -- (0,{1.3*\R*sin(\ang)});
%   \draw[projcol,dashed] (X) -- (R);
%   \draw[black, thick] (O') -- (R');
%   \draw[projcol,dashed] (Y) -- (R);
%   %\draw[vector] (O) -- (R') node[midway,left=5,above right=0] {$\vb{r}$};
%   \draw[vector,<->,unitcol]
%     (\ul,0) node[scale=1,left=2,below left=0] {$\vu{x}$} -- (O) --
%     (0,\ul) node[scale=1,below=2,below left=0] {$\vu{y}$};
%   \draw pic[->,thick,"$\theta$",draw=black,angle radius=26,angle eccentricity=1.3]
%     {angle = X--O--R};
%   \draw[thick] (X)++(0,0.1) --++ (0,-0.2) node[scale=0.9,below=-1] {$x = r\cos\theta$};
%   \draw[thick] (Y)++(0.1,0) --++ (-0.2,0) node[scale=0.9,left] {$y = r\sin\theta$};
\end{tikzpicture}
\end{figure}
\end{frame}
% Models trained by gradient descent can be characterized by a discrete set of intermediate states in the space of their parameters.
% These discrete states are often considered to be an estimation of the gradient flow, however in practical settings where $\epsilon \not \rightarrow 0$ these discrete states differ from the true gradient flow.
% Our primary theoretical contribution is an algorithm which accounts for this difference by observing the true path the model followed during training.
% Here we consider the training dynamics of practical gradient descent steps by integrating a discrete path for weights whose states differ from the gradient flow induced by the training set.
% % As such, the model's training dynamics do not exactly follow the the gradient flow defined by the model's loss function (i.e. the $\epsilon \rightarrow 0$ limit in forward Euler). 
% %These states are commonly computed by gradient descent.

% \textbf{Gradient Along Training Path vs Gradient Field:}
% \begin{frame}
%   \frametitle{Derivation}
%  In order to compute the EPK, gradients on training data must serve two purposes. 
%  First, they are the reference points for comparison (via inner product) with test points. 
%  Second, they determine the path of the model in weight space. \\
% % % % For a continuous path kernel which follows a gradient flow, gradients on the training data exactly match (determine) the path of the parameters through the gradient field.
% % % This would allow us to simply evaluate the gradient of the training data directly.
% % % This means that that for every point, we can simply evaluate the gradient of the training data directly.
% % % Unfortunately, the path followed in practice is not the gradient flow.
% % In practice, the path followed during gradient descent does not match the gradient field exactly. 
% % Instead, the gradient used to move the state of the model forward during training is only computed for finitely many discrete weight states of the model.
% % In order to produce a path kernel,
%  We must \textit{continuously} compare the model's gradient at test points with \textit{fixed} training gradients along each discrete training step $s$ whose weights we we interpolate linearly by $w_s(t) = w_s - t(w_s - w_{s+1})$. We will do this by integrating across the gradient field induced by test points, but holding each training gradient fixed along the entire discrete step taken. This creates an asymmetry, where test gradients are being measured continuously but the training gradients are being measured discretely (see Figure~\ref{fig:vecs}).
% % % One problem encountered when constructing a path kernel for discrete steps is the which requires addressing is the different ways which this learned kernel treats its training points compared to all other inputs.
% % % The path taken during training is dependent on the combination of model, training data, learning algorithm and loss function.
% % % Because the final path includes dependence on training data, the final kernel that represents this path will also depend on the training data.
% \end{frame}
% \begin{frame}
%   \frametitle{Derivation : Assumptions}

% We will redefine our data using an indicator to separate training points from all other points in the input space.
%  \begin{definition}
%  \label{fpm}
%  Let $X$ be two copies of a Hilbert space $H$ with indices $0$ and $1$ so that $X = H \times \{0,1\}$. We will write $x \in H \times \{0,1\}$ so that $x = (x_H, x_I)$ (For brevity, we will omit writing $_H$ and assume each of the following functions defined on $H$ will use $x_H$ and $x_I$ will be a hidden indicator).
%  Let $ f_{w}$ be a differentiable function on $H$ parameterized by $w \in \mathbb{R}^d$. Let $X_T = \{(x_i, 1)\}_{i=1}^M$ be a finite subset of $X$ of size $M$ with corresponding observations $Y_T = \{y_{x_i}\}_{i=1}^M$ with initial parameters $w_0$ so that there is a constant $b \in \mathbb{R}$ such that for all $x$, $ f_{w_0}(x) = b$. Let $L$ be a differentiable loss function of two values which maps $(f(x), y_x)$ into the positive real numbers. Starting with $f_{w_0}$, let $\{w_s\}$ be the sequence of points attained by $N$ forward Euler steps of fixed size $\varepsilon$ so that $w_{s+1} = w_{s} - \varepsilon \nabla L(f(X_T), Y_T)$. Let $x \in H \times \{0\}$ be arbitrary and within the domain of $f_w$ for every $w$. Then $f_{w_s(t)}$ is a \emph{finite parametric gradient model (FPGM)}. 
%  \end{definition}
% \end{frame}
\begin{frame}
  \frametitle{Exact Path Kernel : Definition}
 \begin{definition}
 \label{epk}

 Let $f_{w_s(t)}$ be an FPGM with all corresponding assumptions. Then, for a given training step $s$, the \emph{exact path kernel} (EPK) can be written  
 \begin{equation}
  K_{\text{EPK}}(x, x', s) = \int_0^1\langle \phi_{s,t}(x), \phi_{s,t}(x')\rangle dt
  \label{eq2}
 \end{equation}
 where
 \begin{align}
% % a_{i, s} &= -\varepsilon  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial f_i} \in \mathbb{R} \\
 \phi_{s, t}(x) &=  \nabla_w f_{w_s(t,x)} (x)\\
 w_s(t) &= w_s - t(w_s - w_{s+1})\\
 w_s(t,x) &= \begin{cases} w_s(0), & \text{if } x_I = 1\\ w_s(t), & \text{if } x_I = 0 \end{cases}
 % b &= f_{w_0}(x) 
 \end{align}
 \textbf{Note:} $\phi$ is deciding whether to select a continuously or
 discrete gradient based on whether the data is from the training or
 testing space.

 %This is due to the inherent
% asymmetry that is apparent from the derivation of this kernel (see
% Appendix section ~\ref{proof:eker}). This choice avoids potential discontinuity in the kernel output when a test set happens to contain training points. 
 \end{definition}
\end{frame}
\begin{frame}
  \frametitle{Derivation : Exact Path Kernel}
  \begin{lemma}%{ker}
  The exact path kernel (EPK) is a kernel.
  \end{lemma}


\textbf{Proof.} We must show that the associated kernel matrix $K_{\text{EPK}} \in \mathbb{R}^{n\times n}$ defined for an arbitrary subset of data $\{x_i\}_{i=1}^M \subset X$ as $K_{\text{EPK},i,j} = \int_0^1\langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt$ is both symmetric and positive semi-definite.

Since the inner product on a Hilbert space $\langle \cdot, \cdot \rangle$ is symmetric and since the same mapping $\varphi$ is used on the left and right, $K_{\text{EPK}}$ is \textbf{symmetric}. 

To see that $K_{\text{EPK}}$ is \textbf{Positive Semi-Definite}, let
$\alpha = (\alpha_1, \alpha_2, \dots, \alpha_n)^\top \in \mathbb{R}^n$
be any vector. We need to show that $\alpha^\top K_{\text{EPK}} \alpha
\geq 0$.

\end{frame}
\begin{frame}
  \frametitle{Derivation : Exact Path Kernel}

\begin{align}
\alpha^\top K_{\text{EPK}} \alpha &= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \int_0^1 \langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt \\
&= \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \int_0^1 \langle
                                                                                                                                                  \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
&= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
&= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n  \langle \alpha_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, \alpha_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
\end{align}
\end{frame}

\begin{frame}
  \frametitle{Derivation : Exact Path Kernel}
  \begin{align}
&= \int_0^1    \langle \sum_{i=1}^n \alpha_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, \sum_{j=1}^n \alpha_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
& \text{Re-ordering the sums so that their indices match, we have}\\
&= \int_0^1 \left\lVert \sum_{i=1}^n \alpha_i \nabla_{w}\hat{y}_{w_s(t,x_i)}\right\rVert^2 dt \\
    &\geq 0 \\
    \qed
\end{align}

 Note that this reordering does not depend on the continuity of our mapping function $\phi_{s,t}(x_i)$.
\end{frame}

% \begin{proof}
% We must show that the associated kernel matrix $K_{\text{DPK}} \in \mathbb{R}^{n\times n}$ defined for an arbitrary subset of data $\{x_i\}_{i=1}^M \subset X$ as $K_{\text{DPK},i,j} = \int_0^1\langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt$ is both symmetric and positive semi-definite.

% Since the inner product on a Hilbert space $\langle \cdot, \cdot \rangle$ is symmetric and since the same mapping $\varphi$ is used on the left and right, $K_{\text{DPK}}$ is \textbf{symmetric}. 

% To see that $K_{\text{DPK}}$ is \textbf{Positive Semi-Definite}, let $f = (f_1, f_2, \dots, f_n)^\top \in \mathbb{R}^n$ be any vector. We need to show that $f^\top K_{\text{DPK}} f \geq 0$. We have

% \begin{align*}
% f^\top K_{\text{DPK}} f &= \sum_{i=1}^n \sum_{j=1}^n f_i f_j \int_0^1 \langle \phi_{s,t}(x_i), \phi_{s,t}(x_j)\rangle dt \\
% &= \sum_{i=1}^n \sum_{j=1}^n f_i f_j \int_0^1 \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n f_i f_j \langle \nabla_{w}\hat{y}_{w_s(t,x_i)}, \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1 \sum_{i=1}^n \sum_{j=1}^n  \langle f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, f_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% &= \int_0^1    \langle \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}, \sum_{j=1}^n f_j \nabla_{w}\hat{y}_{w_s(t,x_j)}\rangle dt \\
% & \text{Re-ordering the sums so that their indices match, we have}\\
% &= \int_0^1 \left\lVert \sum_{i=1}^n f_i \nabla_{w}\hat{y}_{w_s(t,x_i)}\right\rVert^2 dt \\
% &\geq 0,
% \end{align*}

% We note that this reordering does not depend on the continuity of our mapping function $\phi_{s,t}(x_i)$. 
% \end{proof}
\begin{frame}
  \frametitle{Derivation : Exact Kernel Ensemble Representation}
\begin{theorem}[Exact Kernel Ensemble Representation]%{theorem}{eker}
\label{thm:eker}
A model $f_{w_N}$ trained using discrete steps matching the conditions of the exact path kernel has the following exact representation as an ensemble of $N$ kernel machines:
\begin{equation}
f_{w_N} = \text{KE}(x) :=  \sum_{s = 1}^N \sum_{i = 1}^{M} a_{i,s} K_{\text{EPK}}(x, x', s) + b
\label{ensemble}
\end{equation}
where
\begin{align}
a_{i, s} &= -\varepsilon  \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)} \\
% \phi_{s, t}(x) &=  \nabla_w f_{w_s(t,x)} (x)\\
% w_s(t,x) &= \begin{cases} w_s, & \text{if } x_I = 1\\ w_s(t), & \text{if } x_I = 0 \end{cases}
b &= f_{w_0}(x)
\end{align}
% which is to say that every such model is a sheaf in a reproducing
% kernel Banach space (RKBS). 
\end{theorem}
\end{frame}
\begin{frame}
  \frametitle{Derivation : Exact Kernel Ensemble Representation Proof}
\textbf{Proof.} 
\label{proof:eker}
Let $f_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed step size $\varepsilon$ on a training dataset $X$ with labels $ Y$, with initial parameters $w_0$ so that there is a constant $b$ such that for every $x$, $f_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x \in X$ be arbitrary and within the domain of $f_w$ for every $w$. For the final trained state of this model $f_{w_N}$, let $y = f_{w_N}(x)$. \\

For one step of training, we consider $y_s  = f_{w_s(0)}(x)$ and
$y_{s+1} = f_{w_{s+1}}(x)$. We wish to account for the change $y_{s+1}
- y_s$ in terms of a gradient flow, so we must compute
$\dfrac{\partial y}{dt}$ for a continuously varying parameter
$t$. Since $f$ is trained using forward Euler with a step size of
$\varepsilon > 0$, this derivative is determined by a step of fixed
size of the weights $w_s$ to $w_{s+1}$.
\end{frame}
\begin{frame}
  \frametitle{Derivation : Exact Kernel Ensemble Representation Proof}

We parameterize this step in terms of the weights:

\begin{align}
    \dfrac{d w_s(t)}{dt} &= (w_{s+1} - w_s)\\   
    \int \dfrac{d w_s(t)}{dt} dt &= \int (w_{s+1} - w_s)dt\\
    w_s(t) &= w_s + t(w_{s+1} - w_s)\\
\end{align}
Since $f$ is being trained using forward Euler, across the entire training set $X$ we can write:
\begin{align}
    \dfrac{d w_s(t)}{dt} &= -\varepsilon \nabla_w L(f_{w_s(0)}(X), y_i) = -\varepsilon \sum_{j = 1}^{d} \sum_{i=1}^M  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial w_j} \label{eq10}
\end{align}
\end{frame}
\begin{frame}
  \frametitle{Derivation : Exact Kernel Ensemble Representation Proof}

Applying chain rule and the above substitution, we can write
\begin{align}
    \dfrac{d \hat y}{dt} = \dfrac{d f_{w_s(t)}}{dt} &= \sum_{j = 1}^{d} \dfrac{d f}{\partial w_j} \dfrac{\partial w_j}{dt}\\
&= \sum_{j = 1}^{d} \dfrac{d f_{w_s(t)}(x)}{\partial w_j} \left(-\varepsilon \dfrac{\partial L(f_{w_s(0)}(X_T),  Y_T)}{\partial w_j}\right)\\
&= \sum_{j = 1}^{d} \dfrac{d f_{w_s(t)}(x)}{\partial w_j} \left(-\varepsilon \sum_{i = 1}^{M}\dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}\dfrac{\partial  f_{w_s(0)}(x_i)}{\partial w_j}\right)\\
&= -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)} \sum_{j = 1}^{d} \dfrac{d f_{w_s(t)}(x)}{\partial w_j}  \dfrac{d f_{w_s(0)}(x_i)}{\partial w_j}\\
&= -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)} \nabla_w f_{w_s(t)}(x) \cdot \nabla_w f_{w_s(0)}(x_i)\label{eq11}
\end{align}
\end{frame}
\begin{frame}
  \frametitle{Derivation : Exact Kernel Ensemble Representation Proof}

Using the fundamental theorem of calculus, we can compute the change in the model's output over step $s$
\begin{align}
    y_{s+1} - y_s &= \int_0^1 -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}  \nabla_w f_{w_s(t)}(x) \cdot \nabla_w f_{w_s(0)}(x_i)dt\\
 &=  -\varepsilon \sum_{i = 1}^{M} \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}  \left(\int_0^1\nabla_w f_{w_s(t)}(x)dt\right) \cdot \nabla_w f_{w_s(0)}(x_i)
\end{align}
\end{frame}
\begin{frame}
  \frametitle{Derivation : Exact Kernel Ensemble Representation Proof}

For all $N$ training steps, we have
\begin{align*}
y_N &= b + \sum_{s=1}^N y_{s+1} - y_s\\
y_N &= b + \sum_{s = 1}^N -\varepsilon \sum_{i = 1}^{M} \dfrac{d
      L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}
      \left(\int_0^1\nabla_w f_{w_s(t)}(x)dt\right) \cdot \nabla_w
      f_{w_s(0)}(x_i)\\   \label{eqint}
% &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial f_i}  \left(\int_0^1\nabla_w f_{w_s(t)}(x)dt\right) \cdot \nabla_w f_{w_s(0)}(x_i)\\
% &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial f_i}  \int_0^1\left\langle \nabla_w f_{w_s(t)}(x), \nabla_w f_{w_s(0)}(x_i) \right\rangle dt\\ 
&= b + \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{d L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}  \int_0^1\left\langle \nabla_w f_{w_s(t,x)}(x), \nabla_w f_{w_s(t,x_i)}(x_i) \right\rangle dt\\ 
&= b + \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i, s}  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt
\end{align*}
Since an integral of a symmetric positive semi-definite function is
still symmetric and positive-definite, each step is thus represented
by a kernel machine.  $\qed$

\end{frame}

% \begin{frame}
%   \frametitle{Derivation : Exact Kernel Ensemble Representation}

% Having established this representation, we can introduce $P_S(t)$, the
% training path which is composed by placing each of the $S$ training
% steps end-to-end. We can rewrite ~\ref{eqint} by combining $\sum_{s =
%   1}^S$ and $\int_0^1 dt$ into a single integral $\int_{P_S}$:

% \begin{align}
% y_N &= b +  -\varepsilon \sum_{i = 1}^{M} \int_{P_S} \dfrac{d
%       L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_i)}
%       \left(\nabla_w f_{w_s(t)}(x)\right) \cdot \nabla_w
%       f_{w_s(0)}(x_i)   
% \end{align}
% \end{frame}
% Rewriting this way, we can re-evaluate another assumption made during
% our statement of this theorem, the bias term $b$. We have forced $b$
% to be a constant in order to give our representation a chance of
% reducing to the form of a kernel machine in a later theorem
% ~\ref{thm:ekr}. Let us relax this and replace $b$ with
% $f_{w_0(0)}(x)$. We can see that this new representation no longer
% requires assumptions about $f_{w_0(0)}(x)$ which gives us a much more
% general representation which includes most ANNs in practice, 
% \begin{align}
% f_{w_F(0)}(x) = f_{w_0(0)}(x) +  -\varepsilon \sum_{i=1}^N \sum_s^S \int_{P_S} \dfrac{d
%       L(f_{w_s(0)}(x_i),  y_i)}{d f_{w_s(0)}(x_0)}
%       \langle\nabla_w f_{w_s(t)}(x), \nabla_w
%       f_{w_s(0)}(x_i)\rangle dt \label{eqint}.
% \end{align}

% % TODO ... make sure is clean?
% % \begin{sproof}
% % Assuming the theorem hypothesis, we'll measure the change in model output as we interpolate across each training step $s$ by measuring the change in model state along a linear parametrization $w_s(t) = w_s - t(w_s - w_{s+1})$. We will let $d$ denote the number of parameters of $f_w$. For brevity, we define $L(x_i, y_i)= l(f_{w_s(0)}(x_i),  y_i)$ where $l$ is the loss function used to train the model.
% % \begin{align}
% %     \dfrac{d \hat y}{dt} &= \sum_{j = 1}^{d} \dfrac{d \hat y}{\partial w_j} \dfrac{d w_j}{dt}\\
% % &= \sum_{j = 1}^{d} \dfrac{d f_{w_s(t)}(x)}{\partial w_j} \left(-\varepsilon \sum_{i = 1}^{M}\dfrac{\partial L(x_i, y_i)}{\partial f_{w_s(0)}(x_i)}\dfrac{\partial f_{w_s(0)}(x_i)}{\partial w_j}\right) \label{eq11}
% % \end{align}
% % We use fundamental theorem of calculus to integrate this equation from step $s$ to  step $s+1$ and then add up across all steps. See Appendix~\ref{proof:eker} for the full proof.
% % \end{sproof}

% \textbf{Remark ~\remlabel{rem:init}} Note that in this formulation, $b$ depends on the test point $x$.
% In order to ensure information is not being leaked from the kernel into this bias term the model $f$ must have constant output for all input. 
% When relaxing this property, to allow for models that have a non-constant starting output, but still requiring $b$ to remain constant, we note that this representation ceases to be exact for all $x$.
% The resulting approximate representation has logit error bounded by its initial bias which can be chosen as $b = \text{mean}(f_{w_0(0)}(X_T))$.
% Starting bias can be minimized by starting with small parameter values which will be out-weighed by contributions from training.
% In practice, we sidestep this issue by initializing all weights in the final layer to $0$, resulting in $b=\text{log}(\text{softmax}(0))$, thus removing $b$'s dependence on $x$.

% \textbf{Remark ~\remlabel{rem:exact}} 
% The exactness of this proof hinges on the \emph{separate} measurement of how the model's parameters change.
% The gradients on training data, which are fixed from one step to the next, measure how the parameters are changing.
% This is opposed to the gradients on test data, which are \textit{not} fixed and vary with time.
% These measure a continuous gradient field for a given point.
% We are using interpolation as a way to measure the difference between the step-wise linear training path and the continuous loss gradient field. 

% % \newcommand{\pluseq}{\mathrel{+}=}
% % \begin{algorithm*}[h]
% %     \caption{Exact Path Kernel: Given a training set $(X, Y)$ with $M$ data points, a testing point $x$ and $N$ weight states $\{w_0, w_1 ... w_N\}$, the kernel machine corresponding to the exact path kernel can be calculated for a model with $W$ weights and $K$ outputs. We estimate the integral across test points by calculating the Riemann sum with sufficient steps ($T$) to achieve machine precision. For loss functions that do not have constant gradient values throughout training, this algorithm produces an ensemble of kernel machines.}
% %     \label{alg:exact}
% % \begin{algorithmic}
% %     % \STATE {\bfseries Input:} $(X_T, Y_T)$, $x$, $w_s \in S$
% %     \STATE $b = f(w_0, x)$
% %     \FOR{$s=0$ \textbf{to} $N$}
% %     \STATE $J^{X} = \nabla_{w} f_{w_s(0)}(X)$ \hfill \COMMENT{Jacobian of training point outputs w.r.t model weights $[M \times K \times W]$ }
% %     \FOR{t \textbf{from} 0 \textbf{to} 1 \textbf{with step} 1/T}
% %     \STATE $w_s(t) = w_s + t(w_{s+1} - w_s)$
% %     \STATE $J^{x} \pluseq \dfrac{1}{T} \nabla_{w} f_{w_s(t)}(x)$ \hfill \COMMENT{Jacobian of testing point output w.r.t model weights averaged across $T$ steps $ [K \times W]$}
% %     \ENDFOR
% %     \STATE $G_{ijk} =  \sum_w J_{ijw}^{X} J_{kw}^x$ \hfill \COMMENT{Inner product on the weight space, this is the kernel value $[M \times K \times K]$}
% %     \STATE $L' = \nabla_{f}L(f_{w_s(0)}(X), Y)$ \hfill \COMMENT{Jacobian of loss w.r.t model output of training points $[M \times K]$}
% %     \STATE $P^s_{ik} = \sum_j L'_{ij} G_{ijk}$ \hfill \COMMENT{Inner product of kernel value scaled by loss gradients $[M \times K$]}
% %     \ENDFOR
% %     \STATE $\mathcal{P}_{sik} = \{P^0, P^1, ..., P^N\}$ \hfill \COMMENT{Stack values across all training steps $[N \times M \times K]$}
% %     \STATE $\hat p = -\varepsilon \dfrac{1}{M} \sum_s \sum_i \mathcal{P}_{sik} + b$ \hfill \COMMENT{Sum across training steps and average across training points for final prediction $[K]$}
% % \end{algorithmic}
% % % \caption{caption}
% % \end{algorithm*}

\begin{frame}
  \frametitle{Derivation : Exact Kernel Machine Representation}
\begin{theorem}[Exact Kernel Machine Reduction]
\label{thm:ekr}
Let $\nabla L(f(w_{s}(x), y)$ be constant  across steps $s$, $(a_{i,s}) = (a_{i,0})$. Let the kernel across all $N$ steps be defined as $K_{\text{NEPK}}(x,x') = \sum_{s = 1}^N a_{i,0} K_{\text{EPK}}(x, x', s)$ Then the exact kernel ensemble representation for $f_{w_N}$ can be reduced exactly to the kernel machine representation:
\begin{equation}
f_{w_N}(x) = \text{KM}(x) := b + \sum_{i = 1}^{M} a_{i,0} K_{\text{NEPK}}(x,x')
\label{exact}
\end{equation}
which is to say that all such models, which are sheaves in the RKBS of
functions integrated along discrete optimization paths in fact reduce
to a sheaf in the reproducing kernel Hilbert space (RKHS). 
\end{theorem}
\end{frame}

% See Appendix~\ref{proof:ekmr} for full proof. By combining theorems ~\ref{thm:eker} and ~\ref{thm:ekr}, we can construct an exact kernel machine representation for any arbitrary parameterized model trained by gradient descent which satisfies the additional property of having constant loss across training steps (e.g. any ANN using catagorical cross-entropy loss (CCE) for classification). This representation will produce exactly identical output to the model across  the model's entire domain. This establishes exact kernel-neural equivalence for classification ANNs. Furthermore, Theorem ~\ref{thm:eker} establishes an exact kernel ensemble representation without limitation to models using loss functions with constant derivatives across steps. It remains an open problem to determine other conditions under which this ensemble may be reduced to a single kernel representation.  

% % \begin{tikzpicture}
% % \node[\text{[insert graphic of forward euler with vectors]}
% % \end{tikzpicture}


% % \begin{theorem}
% % Let $ y_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed size $\varepsilon$ on a finite subset $X_T = \{x_i\}_{i=1}^M$ of a Hilbert space $X$ of size $M$ with labels $Y_T = \{y_i\}_{i=1}^M$, with initial parameters $w_0$ so that there is a constant $b \in \mathbb{R}$ such that $\forall x$, $ y_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x$ be an arbitrary point in the domain of $\hat y_w$ for every $w$. Then $\hat y_{w_N}$ (the final trained state of the model) has the following exact representation: 
% % \begin{equation}
% % \hat y_{w_N}(x) = b + \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i,s} \int_0^1\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\rangle dt
% % \end{equation}
% % where
% % \begin{align}
% % a_{i, s} &= -\varepsilon  \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i} \in \mathbb{R} \\
% % \phi_{s,t}(x) &=  \nabla_w \hat y_{w_s(t,x)} (x)\\
% % w_s(t,x) &= \begin{cases} w_s, x \in X_T\\ w_s(t), x \notin X_T \end{cases}
% % \end{align}
% % Which is to say that all such models are Kernel Methods. 
% % \end{theorem}

% % \begin{proof}
% % Let $\hat y_{w}$ be a differentiable function parameterized by parameters $w$ which is trained via $N$ forward Euler steps of fixed step size $\varepsilon$ on a training dataset $X$ with labels $ Y$, with initial parameters $w_0$ so that there is a constant $b$ such that $\forall x$, $\hat y_{w_0}(x) = b$, and weights at each step ${w_s : 0 \leq s \leq N}$. Let $x$ be an arbitrary point in the domain of $\hat y_w$ for every $w$. For the final trained state of this model $\hat y_{w_N}$, let $y = \hat y_{w_N}(x)$. 

% % For one step of training, we consider $y_s  = \hat y_{w_s(0)}(x)$ and $y_{s+1} = \hat y_{w_{s+1}}(x)$. We wish to account for the change $y_{s+1} - y_s$ in terms of a gradient flow, so we must compute $\dfrac{\partial \hat y}{dt}$ for a continuously varying parameter $t$. Since $f$ is trained using forward Euler with a step size of $\varepsilon > 0$, this derivative is determined by a step of fixed size of the weights $w_s$ to $w_{s+1}$. We will parameterize this step in terms of the weights:

% % \begin{align}
% %     \dfrac{\partial w_s(t)}{dt} &= (w_{s+1} - w_s)\\   
% %     \int \dfrac{\partial w_s(t)}{dt} dt &= \int (w_{s+1} - w_s)dt\\
% %     w_s(t) &= w_s + t(w_{s+1} - w_s)\\
% % \end{align}
% % Since $f$ is being trained using forward Euler, we can write:
% % \begin{align}
% %     \dfrac{\partial w_s(t)}{dt} &= -\varepsilon \nabla_w L(\hat y_{w_s(0)}(x_i), y_i) = -\varepsilon \sum_{j = 1}^{d} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial w_j} \label{eq10}
% % \end{align}
% % Applying chain rule and the above substitution, we can write
% % \begin{align}
% %     \dfrac{\partial \hat y}{dt} = \dfrac{d \hat y_{w_s(t)}}{dt} &= \sum_{j = 1}^{d} \dfrac{\partial \hat y}{\partial w_j} \dfrac{\partial w_j}{dt}\\
% % &= \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j} \left(-\varepsilon \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial w_j}\right)\\
% % &= \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j} \left(-\varepsilon \sum_{i = 1}^{M}\dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}\dfrac{\partial \hat y_{w_s(0)}(x_i)}{\partial w_j}\right)\\
% % &= -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i} \sum_{j = 1}^{d} \dfrac{\partial \hat y_{w_s(t)}(x_i)}{\partial w_j}  \dfrac{\partial \hat y_{w_s(0)}(x_i)}{\partial w_j}\\
% % &= -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(_i),  y_i)}{\partial \hat y_i}  \nabla_w \hat y_{w_s(t)}(x) \cdot \nabla_w \hat y_{w_s(0)}(x_i)\\
% % \end{align}
% % Using the fundamental theorem of calculus, we can compute the change in the model's output over step $s$
% % \begin{align}
% %     y_{s+1} - y_s &= \int_0^1 -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \nabla_w \hat y_{w_s(t)}(x) \cdot \nabla_w \hat y_{w_s(0)}(x_i)dt\\
% %  &=  -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s(0)}(x_i)\\
% % \end{align}
% % For all $N$ training steps, we have
% % \begin{align}
% % y_N &= b + \sum_{s=1}^N y_{s+1} - y_s\\
% % y_N &= \sum_{s = 1}^N -\varepsilon \sum_{i = 1}^{M} \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s(0)}(x_i)\\
% % % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \left(\int_0^1\nabla_w \hat y_{w_s(t)}(x)dt\right) \cdot \nabla_w \hat y_{w_s(0)}(x_i)\\
% % % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t)}(x), \nabla_w \hat y_{w_s(0)}(x_i) \right\rangle dt\\ 
% % &= \sum_{i = 1}^{M}\sum_{s = 1}^N -\varepsilon  \dfrac{\partial L(\hat y_{w_s(0)}(x_i),  y_i)}{\partial \hat y_i}  \int_0^1\left\langle \nabla_w \hat y_{w_s(t,x)}(x), \nabla_w \hat y_{w_s(t,x_i)}(x_i) \right\rangle dt\\ 
% % &= \sum_{i = 1}^{M}\sum_{s = 1}^N a_{i, s}  \int_0^1 \left\langle \phi_{s,t}(x), \phi_{s,t}(x_i)\right\rangle dt
% % \end{align}
% % Since an integral of a symmetric positive semi-definite function is still symmetric and positive-definie and likewise for discrete sums, this represention is a kernel method. 

% % \end{proof}
% \subsection{Discussion}
% %\textbf{Remark \remlabel{rem0}} 


% %\textbf{Remark \remlabel{rem1}} 
% \begin{frame}
%   \frametitle{Derivation : Remarks}
%   $\phi_{s,t}(x)$ depends on both $s$ and $t$, which is non-standard but valid, however an important consequence of this mapping is that the output of this representation is not guaranteed to be continuous. This discontinuity is exactly measuring the error between the model along the exact path compared with the gradient flow for each step. \\

%  We can write another function $k'$ which is continuous but not symmetric, yet still produces an exact representation:
%  \begin{align}
%  k'(x, x') = \langle \nabla_w f_{w_s(t)}(x), \nabla_w f_{w_s(0)}(x')\rangle
%  \end{align}
%  The resulting function is a valid kernel if and only if for every $s$ and every $x$, 
%  \begin{align}
%  \label{eq:cond}
%      \int_0^1 \nabla_w f_{w_s(t)}(x)dt = \nabla_w f_{w_s(0)}(x)
%  \end{align}
% \end{frame}
% % %\textbf{Remark \remlabel{rem3}} 

% \begin{frame}
%   \frametitle{Derivation : Remarks}
% Since $f$ is being trained using forward Euler, we can write:
% \begin{align}
%     \dfrac{\partial w_s(t)}{dt} &= -\varepsilon \nabla_w L(f_{w_s(0)}(x_i), y_i) \label{dstep}% = -\varepsilon \sum_{j = 1}^{d} \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial w_j} \label{rem3}
% \end{align}
% Our parameterization of this step depends on the step size $\varepsilon$ and as $\varepsilon \to 0$, we have 
% \begin{align}
%     \int_0^1 \nabla_w f_{w_{s}(t)}(x)dt \approx \nabla_w f_{w_s(0)}(x)
% \end{align}
% In particular, given a model $f$ that admits a Lipshitz constant $K$
% this approximation has error bounded by $\varepsilon K$ and a proof of
% this convergence is direct. 
% \end{frame}

% % This demonstrates that the asymmetry of this function is exactly measuring the disagreement between the discrete steps taken during training with the gradient field. 
% % This function is one of several subjects for further study, particularly in the context of Gaussian processes whereby the asymmetric Gram matrix corresponding with this function can stand in for a covariance matrix. It may be that the not-symmetric analogue of the covariance in this case has physical meaning relative to uncertainty.

% \begin{frame}
%   \frametitle{Derivation : Remarks}
%   We can see that by changing equation ~\ref{dstep} we can produce an
%   exact representation under a variety of modifications:
%   \begin{itemize}
%   \item Any first-order optimization scheme.
% \item Any subset of training data (by keeping track of the exact
%   data used to compute gradients for each step)
% \item Adversarial training (by keeping copies of the adversarial
%   images used for each step)
% \end{itemize}
% \end{frame}
% % \subsection{Independence from Optimization Scheme}
% % We can see that by changing equation ~\ref{dstep} we can produce an exact representation for any first order discrete optimization scheme that can be written in terms of model gradients aggregated across subsets of training data. This could include backward Euler, leapfrog, and any variation of adaptive step sizes. This includes stochastic gradient descent, and other forms of subsampling (for which the training sums need only be taken over each sample). One caveat is adversarial training, whereby the $a_i$ are now sampling a measure over the continuum of adversarial images. We can write this exactly, however computation will require approximation across the measure. Modification of this kernel for higher order optimization schemes remains an open problem.

% % %\textbf{Remark \remlabel{rem2}} 

  
% \begin{frame}
%   \frametitle{Ensemble Reduction}
% %\textbf{Remark \remlabel{rem4}} 
% In order to reduce the ensemble representation of Equation ~\eqref{ensemble} to the kernel representation of Equation ~\eqref{exact}, we require that the sum over steps still retain the properties of the kernel (symmetry and positive semi-definiteness). In particular we require that for every subset of the training data ${x_i}$ and arbitrary ${\alpha_i}$ and ${\alpha_j}$, we have
% \begin{align}
%     \sum_{i=1}^n\sum_{j=1}^n \sum_{l=1}^M \sum_{s=1}^N \alpha_i \alpha_j a_{l, s}\int_{0}^1 K_{\text{EPK}}(x_i,x_j) dt \geq 0
% \end{align}
% A sufficient condition for this reduction is that the gradient of the
% loss function does not change throughout training. This is the case
% for categorical cross-entropy where labels are in $\{0,1\}$. In fact,
% in this specific context the gradient of the loss function does not
% depend on $f(x)$, and are fully determined by the ground truth label,
% making the gradient of the cross-entropy loss a constant value
% throughout training. 
% \end{frame}
%Showing the positive-definiteness of more general loss functions (e.g. mean squared error loss) will likely require additional regularity conditions on the training path, and is left as future work.
% % There are other conditions which may be imposed in order to guarantee this reduction, and it remains to be studied whether this is unconditionally true for certain training paths.
% %  We demonstrate that constant sign of the loss gradient is a sufficient but not necessary condition for positive-semi-definiteness of the kernel.

\begin{frame}
  \frametitle{Note on Prior Work}
% \subsection{Prior Work}
% \label{subsec:disc}
%\textbf{Remark \remlabel{rem5}} 
Constant sign loss functions have been previously studied by Chen et al. ~\cite{chen2021equivalence}, however the kernel that they derive for a finite-width case is of the form
\begin{align}
    K(x,x_i) =  \int_0^T |\nabla_f L(f_t(x_i), y_i)| \langle \nabla_w f_t(x), \nabla_w f_t(x_i) \rangle dt
\end{align}
The summation across these terms satisfies the positive semi-definite requirement of a kernel, however the weight $|\nabla L(f_t(x_i), y_i)|$ depends on $x_i$ which is one of the two inputs. This makes the resulting function $K(x,x_i)$ asymmetric and therefore not a kernel.
% In this formulation, weight $|\nabla L(f_t(x_i), y_i)|$ depends on $x_i$ which is one of the two inputs. This makes the resulting function $k$ asymmetric and therefore not a kernel.
\end{frame}

% %\textbf{Remark \remlabel{rem6}} 
% \subsection{Uniqueness}
% Uniqueness of this kernel is not guaranteed. 
% The mapping from paths in gradient space to kernels is in fact a function, meaning that each finite continuous path has a unique exact kernel representation of the form described above. 
% However, this function is not necessarily onto the set of all possible kernels. 
% This is evident from the existence of kernels for which representation by a finite parametric function is impossible.
% Nor is this function necessarily one-to-one since there is a continuous manifold of equivalent parameter configurations for neural networks.
% For a given training path, we can pick another path of equivalent configurations whose gradients will be separated by some constant $\delta > 0$.
% The resulting kernel evaluation along this alternate path will be exactly equivalent to the first, despite being a unique path. 
% We also note that the linear path $l_2$ interpolation is not the only valid path between two discrete points in weight space.
% Following the changes in model weights along a path defined by Manhattan Distance is equally valid and will produce a kernel machine with equivalent outputs.
% It remains an open problem to compute paths from two different starting points which both satisfy the constant bias condition from Definition~\eqref{epk} which both converge to the same final parameter configuration and define different kernels.

\begin{frame}
\frametitle{Experimental Results}
    % \begin{figure}
    %     \centering
    %     \begin{minipage}{0.45\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/sample_dataset.pdf}
    %     \end{minipage}
    %     \begin{minipage}{0.45\textwidth}
    %         \centering
    %         \includegraphics[width=0.95\linewidth]{figures/model_kernel_predictions.pdf}
    %     \end{minipage}
    %     \caption{On the left is a 2d dataset of points sampled from Gaussians with different means. Specifically, class A is normally distributed with $\mu = \left[1, 4\right]$ and $\sigma^2 = 1$ while class B is $\mu = \left[4, 1\right]$ and $\sigma^2 = 1$. 2000 data points were sampled for each class. These values were chosen arbitrarily to provide separation with a limited amount of overlap. On the right is the prediction similarity between the kernel and the original model. This demonstrates that our kernel formulation accurately represents the trained network.}
    %     \label{fig:sample_data}
    % \end{figure}
    Our first experiments test the kernel formulation on a dataset 
    which can be visualized in 2d. These experiments serve as a sanity check
    and provide an interpretable representation of what the kernel is learning.
    \begin{figure}[!ht]
        \centering

        \includegraphics[width=0.40\textwidth]{c4_figures/image.png}
        \caption{Class 1 EPK Kernel Prediction (Y) versus neural network prediction (X) for 100 test points, demonstrating extremely close agreement.}
        \label{fig:toymatch}
    \end{figure}
\end{frame}

\begin{frame}
\frametitle{Experimental Results}
\begin{figure}[!h]
    \centering
    % \vspace{-5mm}
\includegraphics[width=0.45\linewidth]{c4_figures/mnist_model_kernel_compare_1_step.pdf}
    \vspace{-9mm}
\includegraphics[width=0.45\linewidth]{c4_figures/mnist_model_kernel_compare_10_steps.pdf}
    \vspace{-9mm}
\includegraphics[width=0.45\linewidth]{c4_figures/mnist_model_kernel_compare_200_steps.pdf}
  \end{figure}
  \end{frame}

% \subsection{Evaluating The Kernel} \label{subsec:evaluate}
% A small test data set within 100 dimensions is created by generating 1000 random samples with means $(1,4,0,...)$, $(4,1,0,...)$ and $(5,5,0,...)$ and standard deviation $1.0$. These points are labeled according to the mean of the Gaussian used to generate them, providing 1000 points each from 3 classes. A fully connected ReLU network with 1 hidden layer is trained using categorical cross-entropy (CCE) and gradient descent with gradients aggregated across the entire training set for each step. We then compute the EPK for this network, approximating the integral from Equation~\ref{eq2} with 100 steps which replicates the output from the ReLU network within machine precision. The EPK (Kernel) outputs are compared with neural network predictions in Fig.~\ref{fig:toymatch} for class 1. Having established this kernel, and its corresponding kernel machine, one natural extension is to allow the kernel weights $a_i$ to be retrained. We perform this updating of the krenel weights using a SVM and present its predictions for each of three classes in Fig.~\ref{fig:svm}.
    


%     % \begin{figure}[h]
%     %     \centering
%     %     \begin{minipage}{0.45\textwidth}
%     %         \centering
%     %         % \begin{wrapfigure}{l}
%     %         \includegraphics[width=0.95\linewidth]{figures/in_distribution_uncertan.pdf}
%     %         % \caption{In Distribution}
%     %         % \end{wrapfigure}
%     %         % \captionof{figure}{Figure 1 is a figure}
%     %     \end{minipage}
%     %         \begin{minipage}{0.45\textwidth}
%     %             \centering
%     %             \includegraphics[width=0.95\linewidth]{figures/ood_positive_2.pdf}
%     %             % \captionof{figure}{Figure 1 is a figure}
%     %         \end{minipage}
%     % %   \caption{Another figure caption.}
%     % % \end{figure}

%     % % \begin{figure}
%     %     \centering
%     %     \begin{minipage}{0.45\textwidth}
%     %         \centering
%     %         \includegraphics[width=0.95\linewidth]{figures/in_distribution.pdf}
%     %         \captionsetup{labelformat=empty}
%     %         \captionof{figure}{In-Distribution}
%     %         \addtocounter{figure}{-1}
%     %         \end{minipage}
%     %         \begin{minipage}{0.45\textwidth}
%     %             \centering
%     %             \includegraphics[width=0.95\linewidth]{figures/ood.pdf}
%     %         \captionsetup{labelformat=empty}
%     %         \captionof{figure}{Out-Of-Distribution}
%     %         \addtocounter{figure}{-1}
%     %         \end{minipage}
%     %     \caption{Example of the kernel values on in-distribution and out-of-distribution (OOD) data. Left column shows samples which are in-distribution for our dataset. Right column row shows OOD samples.}
%     %     \label{fig:kernel}
%     % \end{figure}

%     % We are able to see the agreement between the neural network and its kernel representation in  figure ~\ref{fig:agree}. In figure ~\ref{fig:near} we see that the function learned by the kernel does not directly mimic euclidean distance in the image space. Samples which are nearby in kernel space are not necessarily nearby in pixel space. The similarity metric learned is a direct explanation of how the neural network is making decisions.

%     %     \begin{figure}
%     %     \centering
%     %     \begin{minipage}{0.2\textwidth}
%     %         \centering
%     %         \includegraphics[width=0.95\linewidth]{figures/samples/original_0.png}
%     %         % \captionof{figure}{Figure 1 is a figure}
%     %     \end{minipage}
%     %     \begin{minipage}{0.2\textwidth}
%     %         \centering
%     %         \includegraphics[width=0.95\linewidth]{figures/samples/8409_0.png}
%     %         % \captionof{figure}{Figure 1 is a figure}
%     %     \end{minipage}
%     %     \begin{minipage}{0.2\textwidth}
%     %         \centering
%     %         \includegraphics[width=0.95\linewidth]{figures/samples/euclid_12516_0.png}
%     %         % \captionof{figure}{Figure 1 is a figure}
%     %     \end{minipage} \\
%     %     \vspace{.2cm}
%     % %   \caption{Another figure caption.}
%     %     \centering
%     %     \begin{minipage}{0.2\textwidth}
%     %         \centering
%     %         \includegraphics[width=0.95\linewidth]{figures/samples/original_1.png}
%     %         \captionsetup{labelformat=empty}
%     %         \captionof{figure}{Original Image}            % \captionof{figure}{Figure 1 is a figure}
%     %         \addtocounter{figure}{-1}
%     %     \end{minipage}
%     %     \begin{minipage}{0.2\textwidth}
%     %         \centering
%     %         \includegraphics[width=0.95\linewidth]{figures/samples/322_0.png}
%     %         \captionsetup{labelformat=empty}
%     %         \captionof{figure}{Kernel Distance}            % \captionof{figure}{Figure 1 is a figure}
%     %         \addtocounter{figure}{-1}
%     %     \end{minipage}
%     %     \begin{minipage}{0.2\textwidth}
%     %         \centering
%     %         \includegraphics[width=0.95\linewidth]{figures/samples/euclid_3233_1.png}
%     %         % Pixel Distance
%     %         \captionsetup{labelformat=empty}
%     %         \captionof{figure}{Pixel Distance}
%     %         \addtocounter{figure}{-1}
%     %     \end{minipage}
%     %   \caption{Comparison between the nearest samples in kernel space and pixel space. From left to right in each column: Test set point, nearest sample in kernel space, nearest sample in pixel space using euclidean distance.}
%     %   \label{fig:near}
%     % \end{figure}
    
%     % \begin{figure}
%     % \end{figure}

% % \begin{figure*}[h]
% % \centering
% % \includegraphics[width=8cm]{c4_figures/image.png}
% % \caption{This plot shows output of the ANN versus output of the corresponding kernel representation for a set of test images from the MNIST dataset. We note the very strong agreement between the two outputs.}  
% % \label{fig:agree}
% % \end{figure*}


% \begin{frame}
%  \frametitle{Kernel Properties}
% \begin{figure}[!ht]
%     \centering
% \includegraphics[width=0.3\textwidth]{c4_figures/svm1.png}\includegraphics[width=0.3\textwidth]{c4_figures/svm2.png}\includegraphics[width=0.3\textwidth]{c4_figures/svm3.png}
%     \caption{Updated predictions with kernel $a_i$ updated via gradient descent with training data overlaid for classes 1 (left), 2 (middle), and 3 (right). The high prediction confidence in regions far from training points demonstrates that the learned kernel is non-stationary.}
%     \label{fig:svm}
%   \end{figure}
% \end{frame}

% \subsection{Kernel Analysis}
% Having established the efficacy of this kernel for model representation, the next step is to analyze this kernel to understand how it may inform us about the properties of the corresponding model. In practice, it becomes immediately apparent that this kernel lacks typical properties preferred when humans select kernels. Fig.~\ref{fig:svm} show that the weights of this kernel are non-stationary on our toy problem, with very stable model predictions far away from training data. Next, we use this kernel to estimate uncertainty. Consistent with many other research works on Gaussian processes for classification ~\cite{rasmussen2006gaussian} we use a GP to regress to logits. We then use Monte-Carlo to estimate posteriors with respect to probabilities (post-soft-max) for each prediction across a grid spanning the training points of our toy problem. The result is shown on the right-hand column of Fig.~\ref{fig:cov}. We can see that the kernel values are more confident (lower standard deviation) and more stable (higher kernel values) the farther they get from the training data in most directions. 

% % show kernel values or mean prediction with variances from GP
\begin{frame}
\frametitle{Kernel Properties}

    \begin{figure}[h]
        \centering
\includegraphics[width=0.250\textwidth,
angle=90,origin=c]{c4_figures/kers_square.png}

\vspace{-3cm}

\includegraphics[width=0.250\textwidth,
angle=90,origin=c]{c4_figures/vars_square.png}
      \end{figure}
      \end{frame}

% In order to further understand how these strange kernel properties come about, we exercise another advantage of a kernel by analyzing the points that are contributing to the kernel value for a variety of test points. 
% In Fig.~\ref{fig:points} we examine the kernel values for each of the training points during evaluation of three points chosen as the mean of the generating distribution for each class. 
% The most striking property of these kernel point values is the fact that they are not proportional to the euclidean distance from the test point.
% This appears to indicate a set of basis vectors relative to each test point learned by the model based on the training data which are used to spatially transform the data in preparation for classification. This may relate to the correspondence between neural networks and maximum margin classifiers discussed in related work (~\cite{chizat2020maxmargin} ~\cite{shah2021input}). 
% % In aggregate, the data are imposing a spatial transform on the test point and this transform is represented in the kernel weights by a smooth variation in the weights orthogonal to the basis function of this transform. 
% % Our toy problem primarily varies in only 2 dimensions so these basis functions correspond with only normal vectors in 2 dimensions. 
% Another more subtle property is that some individual data points, mostly close to decision boundaries are slightly over-weighted compared to the other points in their class. 
% This latter property points to the fact that during the latter period of training, once the network has already achieved high accuracy, only the few points which continue to receive incorrect predictions, i.e. caught on the wrong side of a decision boundary, will continue contributing to the training gradient and therefore to the kernel value.

\begin{frame}
\frametitle{Experimental Results}

    \begin{figure}[ht]
        \centering
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_00_sample_00.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_01_sample_00.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_02_sample_00.pdf}\\
        
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_00_sample_01.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_01_sample_01.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_02_sample_01.pdf}\\

        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_00_sample_02.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_01_sample_02.pdf}
        \includegraphics[width=0.32\textwidth]{c4_figures/test_kernel_example_class_02_sample_02.pdf}
        % \includegraphics[width=0.40\textwidth]{c4_figures/test_ntk_example_4.pdf}
        % \includegraphics[width=0.40\textwidth]{c4_figures/test_epk_example_3.pdf}
        % \includegraphics[width=0.40\textwidth]{c4_figures/test_ntk_example_3.pdf}
        % \caption{Plots showing kernel values for each training point relative to a test point. Because our kernel is replicating the output of a network, there are three kernel values per sample on a three class problem. This plot shows kernel values for all three classes across three different test points selected as the mean of the generating distribution. Figures on the diagonal show kernel values of the predicted class. Background shading is the neural network decision boundary.}
        % \label{fig:points}
    \end{figure}
\end{frame}

% % some more words

% % \newpage

% \subsection{Extending To Image Data}
%     We perform experiments on MNIST to demonstrate the applicability to image data. 
%     This kernel representation was generated for convolutional ReLU Network with the categorical cross-entropy loss function, using Pytorch ~\cite{pytorch2019}. 
%     The model was trained using forward Euler (gradient descent) using gradients generated as a sum over all training data for each step. 
%     The state of the model was saved for every training step. In order to compute the per-training-point gradients needed for the kernel representation, the per-input jacobians are computed at execution time in the representation by loading the model for each training step $i$, computing the jacobians for each training input to compute $\nabla_w f_{w_s(0)}(x_i)$, and then repeating this procedure for 200 $t$ values between 0 and 1 in order to approximate $\int_0^1 f_{w_s(t)}(x)$. For MNIST, the resulting prediction is very sensitive to the accuracy of this integral approximation, as shown in Fig.~\ref{fig:mnist}. The top plot shows approximation of the above integral with only one step, which corresponds to the DPK from previous work (~\cite{chen2021equivalence}, ~\cite{domingos2020}, ~\cite{incudini2022quantum}) and as we can see, careful approximation of this integral is necessary to achieve an accurate match between the model and kernel. 

% %I will leave it to michael to add any out-of-sample plots and plots related to $a_{i,s}$ weights and such. 
% \section{Conclusion and Outlook} % confusion and onset
% The implications of a practical and finite kernel representation for the study of neural networks are profound and yet importantly limited by the networks that they are built from. For most gradient trained models, there is a disconnect between the input space (e.g. images) and the parameter space of a network. Parameters are intrinsically difficult to interpret and much work has been spent building approximate mappings that convert model understanding back into the input space in order to interpret features, sample importance, and other details ~\cite{simonyan2013deep, lundberg2017unified, Selvaraju_2019}. The EPK is composed of a direct mapping from the input space into parameter space. This mapping allows for a much deeper understanding of gradient trained models because the internal state of the method has an exact representation mapped from the input space. As we have shown in Fig.~\ref{fig:points}, kernel values derived from gradient methods tell an odd story. We have observed a kernel that picks inputs near decision boundaries to emphasize and derives a spatial transform whose basis vectors depend neither uniformly nor continuously on training points. Although kernel values are linked to sample importance, we have shown that most contributions to the kernel's prediction for a given point are measuring an overall change in the network's internal representation. This supports the notion that most of what a network is doing is fitting a spatial transform based on a wide aggregation of data, and only doing a trivial calculation to the data once this spatial transform has been determined ~\cite{chizat2020maxmargin}. 
% As stated in previous work ~\cite{domingos2020}, this representation has strong implications about the structure of gradient trained models and how they can understand the problems that they solve. Since the kernel weights in this representation are fixed derivatives with respect to the loss function $L$, $a_{i, s} = -\varepsilon  \dfrac{\partial L(f_{w_s(0)}(x_i),  y_i)}{\partial f_i}$, nearly all of the information used by the network is represented by the kernel mapping function and inner product. Inner products are not just measures of distance, they also measure angle. In fact, figure \ref{fig:grad} shows that for a typical training example, the $L_2$ norm of the weights changes monotonically by only 20-30\% during training. This means that the "learning" of a gradient trained model is dominated by change in angle, which is predicted for kernel methods in high dimensions ~\cite{hardle2004nonparametric}.

\begin{frame}
\frametitle{Gradient Alignment During Training}
\begin{figure}[h]
\centering
\includegraphics[width=.95\textwidth]{c4_figures/stab-n-201mnist-C32-100-100-10-0.001-0.0001-eval-mod_int_acc-trn.png}
% \caption{This plot shows a linear interpolation $w(t) = w_0 + t(w_{1} - w_0)$ of model parameters $w$ for a convolutional neural network $f_w$ from their starting random state $w_0$ to their ending trained state $w_1$. The hatched purple line shows the dot product of the sum of the  gradient over the training data $X$, $\langle \nabla_w f_{w(t)}(X), (w_1 - w_0)/|w_1 - w_0| \rangle$. The other lines indicate accuracy (blue), total loss (red decreasing), and L2 Regularization (green increasing)}  
% \label{fig:grad}
\end{figure}
\end{frame}
% % Perhaps the most significant advantage for gradient trained models of an exact kernel representation is that the combination of kernel and kernel weights provides a spatial representation of the model's understanding relative to the training data. In previous work (~\cite{gillette2022data} ~\cite{yousefzadeh2021deep} it has been shown that image classification can be represented by projection onto the convex hull of training data. This projection is computationally infeasible, but it provides a geometric gold-standard classifier in the native image space. Recent work ~\cite{chizat2020maxmargin} indicates that neural networks are in fact max margin classifiers in a metric space defined by their approximation of the wasserstein metric. Since kernel methods provide a spatial representation of their prediction which can be directly compared to the spatial classifier in image space, it can be used to analyze properties of the spatial transform that converts the computationally intractable convex hull in image space, to the computationally tractable approximated Wasserstein metric space. We can see the consequence of this in Fig.~\ref{fig:points}. 

% For kernel methods, our result also represents a new direction. Despite their firm mathematical foundations, kernel methods have lost ground since the early 2000s because the features implicitly learned by deep neural networks yield better accuracy than any known hand-crafted kernels for complex high-dimensional problems ~\cite{NIPS2005_663772ea}. 
% We're hopeful about the scalability of learned kernels based on recent results in scaling kernel methods  ~\cite{snelson2005sparse}. 
% Exact kernel equivalence could allow the use of neural networks to implicitly construct a kernel. 
% This could allow kernel based classifiers to approach the performance of neural networks on complex data. 
% Kernels built in this way may be used with Gaussian processes to allow meaningful direct uncertainty measurement. 
% This would allow for much more significant analysis for out-of-distribution samples including adversarial attacks ~\cite{szegedy2013intriguing, ilyas2019adversarial}. 
% There is significant work to be done in improving the properties of the kernels learned by neural networks for these tools to be used in practice.
% We are confident that this direct connection between practical neural networks and kernels is a strong first step towards achieving this goal.
% % Although this does not provide a "free lunch" on its own, it may open the door for composite kernels based on neural network representation which can retain performance while gaining desirable properties like stationarity ~\cite{tancik2020fourierfeatures}. 

% % <<< I would recommend ending here. The next paragraph is way too technical and seems more like ending in a wimper than a bang >>>

% % Another implication from this representation is the increased importance of models following their gradient flow during training. Since this derived kernel is either discontinuous or asymmetric depending on the neural network's training trajectory, developing training restrictions which satisfy equation~\ref{eq:cond} may produce more useful kernels and have implications about the accuracy and generality of neural network models. This will provide a new motivation for such research separate from just the question of efficiency of training. Approaches in this direction may be found in control theory (~\cite{lin2020gradient}) and the neural ODE approach (~\cite{bilovs2021neural}, ~\cite{neuralode2018}).
% %Also in this vein is the precise formulation of the divergence error from the discrete training path to the smooth gradient flow. Such a formulation would shed light on the dynamics of how such representations converge in performance under various step refinements.

% % TODO final conclusion paragraph

% % \section{Acknowledgements}

% % This research was funded by Los Alamos National Lab LDRD-DR XX9C UQ4ML (help with how to acknowledge this LDRD funding juston?) Thanks to Yen Ting Lin, Philip Hoskins, Keenan Eikenberry, and Craig Thompson for feedback on early iterations of this paper. 

% %\bibliography{bibfile}
% %\bibliographystyle{icml2023}




